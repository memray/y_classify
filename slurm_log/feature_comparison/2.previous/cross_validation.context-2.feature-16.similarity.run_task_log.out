/ihome/pbrusilosky/rum20/packages/anaconda3/bin/python -m dialogue.classify.task_runner -experiment_mode cross_validation -selected_feature_set_id 16 -add_similarity_feature -selected_context_id 2
No. of param settings = 1
[('experiment_mode', 'cross_validation'), ('deep_model', False), ('selected_context_id', 2), ('selected_feature_set_id', 16), ('similarity_feature', True), ('k_feature_to_keep', -1), ('k_component_for_pca', -1)]
Loading extracted features of dstc2
Loading extracted features of dstc3
Loading extracted features of family
Loading extracted features of ghome
01/29/2018 20:21:38 [INFO] configuration: experiment_mode  :   cross_validation
01/29/2018 20:21:38 [INFO] configuration: deep_model  :   False
01/29/2018 20:21:38 [INFO] configuration: selected_context_id  :   2
01/29/2018 20:21:38 [INFO] configuration: selected_feature_set_id  :   16
01/29/2018 20:21:38 [INFO] configuration: similarity_feature  :   True
01/29/2018 20:21:38 [INFO] configuration: k_feature_to_keep  :   -1
01/29/2018 20:21:38 [INFO] configuration: k_component_for_pca  :   -1
01/29/2018 20:21:38 [INFO] configuration: seed  :   154316847
01/29/2018 20:21:38 [INFO] configuration: root_path  :   /ihome/pbrusilosky/rum20/y_classify
01/29/2018 20:21:38 [INFO] configuration: task_name  :   utterance_type
01/29/2018 20:21:38 [INFO] configuration: timemark  :   20180129-202138
01/29/2018 20:21:38 [INFO] configuration: context_set  :   last
01/29/2018 20:21:38 [INFO] configuration: utterance_names  :   ['last_user_utterance', 'last_system_utterance', 'current_user_utterance', 'next_system_utterance', 'next_user_utterance']
01/29/2018 20:21:38 [INFO] configuration: utterance_range  :   ['current_user_utterance', 'last_system_utterance', 'current_user_utterance']
01/29/2018 20:21:38 [INFO] configuration: feature_set  :   16-[6+1.2.3.4]
01/29/2018 20:21:38 [INFO] configuration: feature_set_number  :   ['9', '1', '2', '3', '4', '5', '6', '7']
01/29/2018 20:21:38 [INFO] configuration: experiment_name  :   20180129-202138.cross_validation.context=last.feature=16-[6+1.2.3.4].similarity=true
01/29/2018 20:21:38 [INFO] configuration: experiment_path  :   /ihome/pbrusilosky/rum20/y_classify/output/20180129-202138.cross_validation.context=last.feature=16-[6+1.2.3.4].similarity=true
01/29/2018 20:21:38 [INFO] configuration: log_path  :   /ihome/pbrusilosky/rum20/y_classify/output/20180129-202138.cross_validation.context=last.feature=16-[6+1.2.3.4].similarity=true/output.log
01/29/2018 20:21:38 [INFO] configuration: valid_type  :   {'R', 'F', 'A', 'C'}
01/29/2018 20:21:38 [INFO] configuration: data_name  :   
01/29/2018 20:21:38 [INFO] configuration: data_names  :   ['dstc2', 'dstc3', 'family', 'ghome']
01/29/2018 20:21:38 [INFO] configuration: raw_feature_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.raw_feature.pkl
01/29/2018 20:21:38 [INFO] configuration: extracted_feature_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.extracted_feature.pkl
01/29/2018 20:21:38 [INFO] configuration: pipeline_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.pipeline.pkl
01/29/2018 20:21:38 [INFO] configuration: metrics  :   ['accuracy', 'precision', 'recall', 'f1_score', 'training_time', 'test_time']
01/29/2018 20:21:38 [INFO] configuration: do_cross_validation  :   True
01/29/2018 20:21:38 [INFO] configuration: #division  :   5
01/29/2018 20:21:38 [INFO] configuration: #cross_validation  :   10
01/29/2018 20:21:38 [INFO] configuration: cv_index_cache_path  :   
01/29/2018 20:21:38 [INFO] configuration: action_words  :   {'remove', 'ani', 'share', 'volum', 'clear', 'price', 'centre', 'show', 'findcare', 'number', 'cheap', 'list', 'food', 'temperatur', 'next', 'room', 'song', 'moder', 'watch', 'help', 'volume', 'findcar', 'light', 'items', 'item', 'telephone', 'telephon', 'temperature', 'expens', 'stop', 'add', 'reminders', 'music', 'moderate', 'expensive', 'play', 'area', 'snooz', 'weather', 'time', 'skip', 'tell', 'turn', 'snooze', 'video', 'delete', 'start', 'any', 'delet', 'remind', 'south', 'discard', 'cast', 'centr', 'phone', 'matter', 'alarm', 'part', 'address', 'north', 'shuffl', 'else', 'timer', 'shuffle', 'remov', 'els', 'member', 'reminder', 'post', 'reminds'}
01/29/2018 20:21:38 [INFO] configuration: corenlp_jars  :   ('/Users/memray/Project/stanford/stanford-corenlp-full-3.8.0/*', '/Users/memray/Project/stanford/stanford-corenlp-full-3.8.0/stanford-english-kbp-corenlp-2017-06-09-models.jar')
01/29/2018 20:21:38 [INFO] configuration: lda_topic_number  :   50
01/29/2018 20:21:38 [INFO] configuration: lda_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.topic=50.lda.pkl
01/29/2018 20:21:38 [INFO] configuration: gensim_corpus_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.corpus.pkl
01/29/2018 20:21:38 [INFO] configuration: gensim_dict_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.dict
01/29/2018 20:21:38 [INFO] configuration: w2v_path  :   /home/memray/Data/glove/GoogleNews-vectors-negative300.bin
01/29/2018 20:21:38 [INFO] configuration: w2v_vector_length  :   300
01/29/2018 20:21:38 [INFO] configuration: d2v_vector_length  :   300
01/29/2018 20:21:38 [INFO] configuration: d2v_window_size  :   5
01/29/2018 20:21:38 [INFO] configuration: d2v_min_count  :   2
01/29/2018 20:21:38 [INFO] configuration: d2v_model_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.doc2vec.dim=300.window=5.min_count=2.model
01/29/2018 20:21:38 [INFO] configuration: d2v_vector_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.doc2vec.dim=300.window=5.min_count=2.vector
01/29/2018 20:21:38 [INFO] configuration: num_word_keep  :   {'dstc2': 300, 'dstc3': 300, 'family': 1000, 'ghome': 1000}
01/29/2018 20:21:38 [INFO] configuration: batch_size  :   128
01/29/2018 20:21:38 [INFO] configuration: max_epoch  :   50
01/29/2018 20:21:38 [INFO] configuration: early_stop_tolerance  :   2
01/29/2018 20:21:38 [INFO] configuration: concat_sents  :   False
01/29/2018 20:21:38 [INFO] configuration: cnn_setting  :   {'model': 'multichannel', 'early_stopping': True, 'word_dim': 300, 'filters': [3, 4, 5], 'filter_num': [100, 100, 100], 'class_size': 4, 'batch_size': 128, 'learning_rate': 0.001, 'norm_limit': 10, 'dropout_prob': 0.0, 'sentence_num': 3}
01/29/2018 20:21:38 [INFO] configuration: skipthought_setting  :   {'skipthought_model_path': '/Users/memray/Data/skip-thought', 'skipthought_data_path': '/ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.skip-thought.biskip.vector', 'fixed_emb': True, 'sentence_num': 3, 'hidden_size': 2400, 'class_size': 4, 'learning_rate': 0.0001, 'norm_limit': 3, 'dropout_prob': 0.5}
01/29/2018 20:21:38 [INFO] configuration: lstm_setting  :   {'model': 'non-static', 'hidden_size': 32, 'embedding_size': 300, 'num_layers': 1, 'bidirectional': False, 'learning_rate': 0.001, 'class_size': 4, 'norm_limit': 2, 'clip_grad_norm': 2, 'dropout_prob': 0.1}
01/29/2018 20:21:43 [INFO] exp_shallowmodel: ******************** dstc2 - Round 0 
01/29/2018 20:21:43 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:21:43 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:21:43 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:21:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:21:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:21:43 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:21:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:23:26 [INFO] exp_shallowmodel: train time: 103.073s
01/29/2018 20:23:26 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:23:26 [INFO] exp_shallowmodel: accuracy:   0.681
01/29/2018 20:23:26 [INFO] exp_shallowmodel: f1_score:   0.489
01/29/2018 20:23:26 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:23:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.68      0.68      0.68       164
          F       0.76      0.82      0.79       268
          R       0.52      0.46      0.49       125

avg / total       0.66      0.68      0.67       571

01/29/2018 20:23:26 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:23:26 [INFO] exp_shallowmodel: 
[[  0   0   8   6]
 [  1 111  28  24]
 [  1  23 220  24]
 [  3  30  34  58]]
01/29/2018 20:23:28 [INFO] exp_shallowmodel: ******************** dstc2 - Round 1 
01/29/2018 20:23:28 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:23:28 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:23:28 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:23:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:23:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:23:28 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:23:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:24:56 [INFO] exp_shallowmodel: train time: 87.787s
01/29/2018 20:24:56 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:24:56 [INFO] exp_shallowmodel: accuracy:   0.634
01/29/2018 20:24:56 [INFO] exp_shallowmodel: f1_score:   0.479
01/29/2018 20:24:56 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:24:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.62      0.67      0.65       164
          F       0.72      0.74      0.73       268
          R       0.46      0.42      0.44       125

avg / total       0.62      0.63      0.63       571

01/29/2018 20:24:56 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:24:56 [INFO] exp_shallowmodel: 
[[  1   2   7   4]
 [  1 110  29  24]
 [  2  35 199  32]
 [  2  30  41  52]]
01/29/2018 20:24:58 [INFO] exp_shallowmodel: ******************** dstc2 - Round 2 
01/29/2018 20:24:58 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:24:58 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:24:58 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:24:58 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:24:58 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:24:58 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:24:58 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:26:20 [INFO] exp_shallowmodel: train time: 81.914s
01/29/2018 20:26:20 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:26:20 [INFO] exp_shallowmodel: accuracy:   0.643
01/29/2018 20:26:20 [INFO] exp_shallowmodel: f1_score:   0.483
01/29/2018 20:26:20 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:26:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.07      0.10        14
          C       0.58      0.62      0.60       164
          F       0.76      0.78      0.77       268
          R       0.50      0.45      0.47       125

avg / total       0.63      0.64      0.64       571

01/29/2018 20:26:20 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:26:20 [INFO] exp_shallowmodel: 
[[  1   2   5   6]
 [  2 102  31  29]
 [  2  37 208  21]
 [  2  36  31  56]]
01/29/2018 20:26:22 [INFO] exp_shallowmodel: ******************** dstc2 - Round 3 
01/29/2018 20:26:22 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:26:22 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:26:22 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:26:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:26:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:26:22 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:26:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:27:40 [INFO] exp_shallowmodel: train time: 78.066s
01/29/2018 20:27:40 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:27:40 [INFO] exp_shallowmodel: accuracy:   0.622
01/29/2018 20:27:40 [INFO] exp_shallowmodel: f1_score:   0.470
01/29/2018 20:27:40 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:27:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.07      0.12        14
          C       0.58      0.61      0.59       164
          F       0.71      0.76      0.74       268
          R       0.45      0.40      0.43       125

avg / total       0.61      0.62      0.61       571

01/29/2018 20:27:40 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:27:40 [INFO] exp_shallowmodel: 
[[  1   1   9   3]
 [  0 100  34  30]
 [  1  36 204  27]
 [  0  36  39  50]]
01/29/2018 20:27:42 [INFO] exp_shallowmodel: ******************** dstc2 - Round 4 
01/29/2018 20:27:42 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:27:42 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:27:42 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:27:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:27:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:27:42 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:27:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:29:03 [INFO] exp_shallowmodel: train time: 81.409s
01/29/2018 20:29:03 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:29:03 [INFO] exp_shallowmodel: accuracy:   0.634
01/29/2018 20:29:03 [INFO] exp_shallowmodel: f1_score:   0.458
01/29/2018 20:29:03 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:29:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.66      0.64       164
          F       0.70      0.74      0.72       268
          R       0.51      0.45      0.48       125

avg / total       0.62      0.63      0.62       571

01/29/2018 20:29:03 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:29:03 [INFO] exp_shallowmodel: 
[[  0   1   8   5]
 [  1 108  38  17]
 [  1  37 198  32]
 [  0  30  39  56]]
01/29/2018 20:29:05 [INFO] exp_shallowmodel: ******************** dstc2 - Round 5 
01/29/2018 20:29:05 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:29:05 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:29:05 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:29:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:29:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:29:05 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:29:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:30:28 [INFO] exp_shallowmodel: train time: 83.421s
01/29/2018 20:30:28 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:30:28 [INFO] exp_shallowmodel: accuracy:   0.580
01/29/2018 20:30:28 [INFO] exp_shallowmodel: f1_score:   0.408
01/29/2018 20:30:28 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:30:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.54      0.54      0.54       164
          F       0.69      0.73      0.71       268
          R       0.39      0.37      0.38       125

avg / total       0.57      0.58      0.57       571

01/29/2018 20:30:28 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:30:28 [INFO] exp_shallowmodel: 
[[  0   1   9   4]
 [  1  89  37  37]
 [  2  39 196  31]
 [  3  35  41  46]]
01/29/2018 20:30:30 [INFO] exp_shallowmodel: ******************** dstc2 - Round 6 
01/29/2018 20:30:30 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:30:30 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:30:30 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:30:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:30:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:30:30 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:30:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:31:59 [INFO] exp_shallowmodel: train time: 88.310s
01/29/2018 20:31:59 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:31:59 [INFO] exp_shallowmodel: accuracy:   0.639
01/29/2018 20:31:59 [INFO] exp_shallowmodel: f1_score:   0.460
01/29/2018 20:31:59 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:31:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.64      0.62       164
          F       0.73      0.76      0.74       268
          R       0.49      0.46      0.47       125

avg / total       0.63      0.64      0.63       571

01/29/2018 20:31:59 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:31:59 [INFO] exp_shallowmodel: 
[[  0   3   7   4]
 [  0 105  29  30]
 [  4  35 203  26]
 [  1  29  38  57]]
01/29/2018 20:32:01 [INFO] exp_shallowmodel: ******************** dstc2 - Round 7 
01/29/2018 20:32:01 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:32:01 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:32:01 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:32:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:32:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:32:01 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:32:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:33:19 [INFO] exp_shallowmodel: train time: 78.018s
01/29/2018 20:33:19 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:33:19 [INFO] exp_shallowmodel: accuracy:   0.618
01/29/2018 20:33:19 [INFO] exp_shallowmodel: f1_score:   0.462
01/29/2018 20:33:19 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:33:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.07      0.11        14
          C       0.59      0.61      0.60       164
          F       0.73      0.76      0.74       268
          R       0.42      0.38      0.40       125

avg / total       0.61      0.62      0.61       571

01/29/2018 20:33:19 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:33:19 [INFO] exp_shallowmodel: 
[[  1   3   6   4]
 [  0 100  30  34]
 [  3  32 204  29]
 [  1  35  41  48]]
01/29/2018 20:33:20 [INFO] exp_shallowmodel: ******************** dstc2 - Round 8 
01/29/2018 20:33:20 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:33:20 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:33:20 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:33:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:33:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:33:20 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:33:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:34:44 [INFO] exp_shallowmodel: train time: 83.094s
01/29/2018 20:34:44 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:34:44 [INFO] exp_shallowmodel: accuracy:   0.634
01/29/2018 20:34:44 [INFO] exp_shallowmodel: f1_score:   0.494
01/29/2018 20:34:44 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:34:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.14      0.17        14
          C       0.61      0.68      0.64       164
          F       0.74      0.74      0.74       268
          R       0.46      0.39      0.42       125

avg / total       0.63      0.63      0.63       571

01/29/2018 20:34:44 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:34:44 [INFO] exp_shallowmodel: 
[[  2   0   9   3]
 [  1 112  24  27]
 [  4  37 199  28]
 [  2  36  38  49]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 20:34:46 [INFO] exp_shallowmodel: ******************** dstc2 - Round 9 
01/29/2018 20:34:46 [INFO] exp_shallowmodel: #(data) = 4568
01/29/2018 20:34:46 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:34:46 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:34:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:34:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:34:46 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:34:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:36:09 [INFO] exp_shallowmodel: train time: 83.557s
01/29/2018 20:36:09 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:36:09 [INFO] exp_shallowmodel: accuracy:   0.601
01/29/2018 20:36:09 [INFO] exp_shallowmodel: f1_score:   0.425
01/29/2018 20:36:09 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:36:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.54      0.60      0.57       169
          F       0.70      0.75      0.72       271
          R       0.44      0.37      0.40       130

avg / total       0.58      0.60      0.59       586

01/29/2018 20:36:09 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:36:09 [INFO] exp_shallowmodel: 
[[  0   1  11   4]
 [  0 102  35  32]
 [  2  43 202  24]
 [  1  42  39  48]]
01/29/2018 20:36:11 [INFO] exp_shallowmodel: ******************** dstc2 - Round 10 
01/29/2018 20:36:11 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:36:11 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:36:11 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:36:11 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:36:11 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:36:11 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:36:11 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:37:35 [INFO] exp_shallowmodel: train time: 83.612s
01/29/2018 20:37:35 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:37:35 [INFO] exp_shallowmodel: accuracy:   0.601
01/29/2018 20:37:35 [INFO] exp_shallowmodel: f1_score:   0.425
01/29/2018 20:37:35 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:37:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.56      0.61      0.58       164
          F       0.69      0.74      0.72       268
          R       0.46      0.35      0.40       125

avg / total       0.59      0.60      0.59       571

01/29/2018 20:37:35 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:37:35 [INFO] exp_shallowmodel: 
[[  0   3   7   4]
 [  2 100  39  23]
 [  6  38 199  25]
 [  1  37  43  44]]
01/29/2018 20:37:37 [INFO] exp_shallowmodel: ******************** dstc2 - Round 11 
01/29/2018 20:37:37 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:37:37 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:37:37 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:37:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:37:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:37:37 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:37:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:39:02 [INFO] exp_shallowmodel: train time: 85.108s
01/29/2018 20:39:02 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:39:02 [INFO] exp_shallowmodel: accuracy:   0.615
01/29/2018 20:39:02 [INFO] exp_shallowmodel: f1_score:   0.459
01/29/2018 20:39:02 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:39:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.60      0.60      0.60       164
          F       0.72      0.76      0.74       268
          R       0.40      0.38      0.39       125

avg / total       0.60      0.61      0.61       571

01/29/2018 20:39:02 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:39:02 [INFO] exp_shallowmodel: 
[[  1   1   8   4]
 [  0  99  30  35]
 [  4  28 203  33]
 [  1  37  39  48]]
01/29/2018 20:39:04 [INFO] exp_shallowmodel: ******************** dstc2 - Round 12 
01/29/2018 20:39:04 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:39:04 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:39:04 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:39:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:39:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:39:04 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:39:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:40:35 [INFO] exp_shallowmodel: train time: 90.900s
01/29/2018 20:40:35 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:40:35 [INFO] exp_shallowmodel: accuracy:   0.608
01/29/2018 20:40:35 [INFO] exp_shallowmodel: f1_score:   0.454
01/29/2018 20:40:35 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:40:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.58      0.65      0.61       164
          F       0.68      0.74      0.71       268
          R       0.45      0.34      0.38       125

avg / total       0.59      0.61      0.60       571

01/29/2018 20:40:35 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:40:35 [INFO] exp_shallowmodel: 
[[  1   1   9   3]
 [  1 107  34  22]
 [  1  43 197  27]
 [  1  34  48  42]]
01/29/2018 20:40:36 [INFO] exp_shallowmodel: ******************** dstc2 - Round 13 
01/29/2018 20:40:36 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:40:36 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:40:36 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:40:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:40:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:40:36 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:40:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:42:19 [INFO] exp_shallowmodel: train time: 102.816s
01/29/2018 20:42:19 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:42:19 [INFO] exp_shallowmodel: accuracy:   0.641
01/29/2018 20:42:19 [INFO] exp_shallowmodel: f1_score:   0.461
01/29/2018 20:42:19 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:42:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.63      0.67      0.65       164
          F       0.75      0.75      0.75       268
          R       0.45      0.44      0.44       125

avg / total       0.63      0.64      0.64       571

01/29/2018 20:42:19 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:42:19 [INFO] exp_shallowmodel: 
[[  0   0   4  10]
 [  0 110  28  26]
 [  3  32 201  32]
 [  2  32  36  55]]
01/29/2018 20:42:21 [INFO] exp_shallowmodel: ******************** dstc2 - Round 14 
01/29/2018 20:42:21 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:42:21 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:42:21 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:42:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:42:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:42:21 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:42:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:43:53 [INFO] exp_shallowmodel: train time: 91.881s
01/29/2018 20:43:53 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:43:53 [INFO] exp_shallowmodel: accuracy:   0.629
01/29/2018 20:43:53 [INFO] exp_shallowmodel: f1_score:   0.500
01/29/2018 20:43:53 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:43:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.21        14
          C       0.58      0.62      0.60       164
          F       0.75      0.75      0.75       268
          R       0.45      0.43      0.44       125

avg / total       0.62      0.63      0.62       571

01/29/2018 20:43:53 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:43:53 [INFO] exp_shallowmodel: 
[[  2   2   7   3]
 [  0 102  31  31]
 [  1  34 201  32]
 [  2  39  30  54]]
01/29/2018 20:43:55 [INFO] exp_shallowmodel: ******************** dstc2 - Round 15 
01/29/2018 20:43:55 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:43:55 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:43:55 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:43:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:43:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:43:55 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:43:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:45:17 [INFO] exp_shallowmodel: train time: 82.307s
01/29/2018 20:45:17 [INFO] exp_shallowmodel: test time:  0.009s
01/29/2018 20:45:17 [INFO] exp_shallowmodel: accuracy:   0.634
01/29/2018 20:45:17 [INFO] exp_shallowmodel: f1_score:   0.454
01/29/2018 20:45:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:45:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.67      0.62       164
          F       0.75      0.74      0.74       268
          R       0.48      0.43      0.46       125

avg / total       0.62      0.63      0.63       571

01/29/2018 20:45:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:45:17 [INFO] exp_shallowmodel: 
[[  0   2   5   7]
 [  0 110  30  24]
 [  0  43 198  27]
 [  1  38  32  54]]
01/29/2018 20:45:19 [INFO] exp_shallowmodel: ******************** dstc2 - Round 16 
01/29/2018 20:45:19 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:45:19 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:45:19 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:45:19 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:45:19 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:45:19 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:45:19 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:46:41 [INFO] exp_shallowmodel: train time: 82.161s
01/29/2018 20:46:41 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:46:41 [INFO] exp_shallowmodel: accuracy:   0.651
01/29/2018 20:46:41 [INFO] exp_shallowmodel: f1_score:   0.462
01/29/2018 20:46:41 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:46:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.64      0.72      0.68       164
          F       0.76      0.77      0.76       268
          R       0.44      0.38      0.41       125

avg / total       0.63      0.65      0.64       571

01/29/2018 20:46:41 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:46:41 [INFO] exp_shallowmodel: 
[[  0   1   8   5]
 [  0 118  19  27]
 [  3  31 207  27]
 [  3  35  40  47]]
01/29/2018 20:46:43 [INFO] exp_shallowmodel: ******************** dstc2 - Round 17 
01/29/2018 20:46:43 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:46:43 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:46:43 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:46:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:46:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:46:43 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:46:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:48:06 [INFO] exp_shallowmodel: train time: 82.655s
01/29/2018 20:48:06 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:48:06 [INFO] exp_shallowmodel: accuracy:   0.637
01/29/2018 20:48:06 [INFO] exp_shallowmodel: f1_score:   0.480
01/29/2018 20:48:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:48:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.07      0.09        14
          C       0.64      0.60      0.62       164
          F       0.72      0.77      0.75       268
          R       0.47      0.46      0.46       125

avg / total       0.63      0.64      0.63       571

01/29/2018 20:48:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:48:06 [INFO] exp_shallowmodel: 
[[  1   1  10   2]
 [  0  99  32  33]
 [  3  29 207  29]
 [  5  26  37  57]]
01/29/2018 20:48:08 [INFO] exp_shallowmodel: ******************** dstc2 - Round 18 
01/29/2018 20:48:08 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:48:08 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:48:08 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:48:08 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:48:08 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:48:08 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:48:08 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:49:27 [INFO] exp_shallowmodel: train time: 78.647s
01/29/2018 20:49:27 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:49:27 [INFO] exp_shallowmodel: accuracy:   0.620
01/29/2018 20:49:27 [INFO] exp_shallowmodel: f1_score:   0.489
01/29/2018 20:49:27 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:49:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.14      0.22        14
          C       0.55      0.59      0.57       164
          F       0.74      0.77      0.75       268
          R       0.43      0.38      0.41       125

avg / total       0.61      0.62      0.61       571

01/29/2018 20:49:27 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:49:27 [INFO] exp_shallowmodel: 
[[  2   1   7   4]
 [  0  97  34  33]
 [  0  35 207  26]
 [  2  42  33  48]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 20:49:28 [INFO] exp_shallowmodel: ******************** dstc2 - Round 19 
01/29/2018 20:49:28 [INFO] exp_shallowmodel: #(data) = 4568
01/29/2018 20:49:28 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:49:28 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:49:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:49:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:49:28 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:49:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:50:46 [INFO] exp_shallowmodel: train time: 77.302s
01/29/2018 20:50:46 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:50:46 [INFO] exp_shallowmodel: accuracy:   0.618
01/29/2018 20:50:46 [INFO] exp_shallowmodel: f1_score:   0.465
01/29/2018 20:50:46 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:50:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.06      0.09        16
          C       0.61      0.64      0.62       169
          F       0.68      0.75      0.71       271
          R       0.49      0.39      0.43       130

avg / total       0.60      0.62      0.61       586

01/29/2018 20:50:46 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:50:46 [INFO] exp_shallowmodel: 
[[  1   2   8   5]
 [  0 108  38  23]
 [  3  40 202  26]
 [  2  27  50  51]]
01/29/2018 20:50:48 [INFO] exp_shallowmodel: ******************** dstc2 - Round 20 
01/29/2018 20:50:48 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:50:48 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:50:48 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:50:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:50:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:50:48 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:50:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:52:16 [INFO] exp_shallowmodel: train time: 88.545s
01/29/2018 20:52:16 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:52:16 [INFO] exp_shallowmodel: accuracy:   0.622
01/29/2018 20:52:16 [INFO] exp_shallowmodel: f1_score:   0.500
01/29/2018 20:52:16 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:52:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.21        14
          C       0.59      0.63      0.61       164
          F       0.70      0.72      0.71       268
          R       0.49      0.45      0.47       125

avg / total       0.62      0.62      0.62       571

01/29/2018 20:52:16 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:52:16 [INFO] exp_shallowmodel: 
[[  2   1   8   3]
 [  0 103  38  23]
 [  1  41 194  32]
 [  2  30  37  56]]
01/29/2018 20:52:18 [INFO] exp_shallowmodel: ******************** dstc2 - Round 21 
01/29/2018 20:52:18 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:52:18 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:52:18 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:52:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:52:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:52:18 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:52:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:53:45 [INFO] exp_shallowmodel: train time: 86.367s
01/29/2018 20:53:45 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:53:45 [INFO] exp_shallowmodel: accuracy:   0.613
01/29/2018 20:53:45 [INFO] exp_shallowmodel: f1_score:   0.458
01/29/2018 20:53:45 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:53:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.56      0.57      0.57       164
          F       0.72      0.77      0.74       268
          R       0.44      0.39      0.41       125

avg / total       0.60      0.61      0.60       571

01/29/2018 20:53:45 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:53:45 [INFO] exp_shallowmodel: 
[[  1   2   9   2]
 [  1  94  36  33]
 [  0  34 206  28]
 [  2  38  36  49]]
01/29/2018 20:53:47 [INFO] exp_shallowmodel: ******************** dstc2 - Round 22 
01/29/2018 20:53:47 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:53:47 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:53:47 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:53:47 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:53:47 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:53:47 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:53:47 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:55:07 [INFO] exp_shallowmodel: train time: 80.436s
01/29/2018 20:55:07 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:55:07 [INFO] exp_shallowmodel: accuracy:   0.643
01/29/2018 20:55:07 [INFO] exp_shallowmodel: f1_score:   0.505
01/29/2018 20:55:07 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:55:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.14      0.18        14
          C       0.61      0.66      0.64       164
          F       0.73      0.76      0.74       268
          R       0.50      0.42      0.46       125

avg / total       0.63      0.64      0.64       571

01/29/2018 20:55:07 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:55:07 [INFO] exp_shallowmodel: 
[[  2   2   3   7]
 [  1 108  30  25]
 [  4  39 204  21]
 [  1  27  44  53]]
01/29/2018 20:55:09 [INFO] exp_shallowmodel: ******************** dstc2 - Round 23 
01/29/2018 20:55:09 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:55:09 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:55:09 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:55:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:55:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:55:09 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:55:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:56:28 [INFO] exp_shallowmodel: train time: 79.471s
01/29/2018 20:56:28 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:56:28 [INFO] exp_shallowmodel: accuracy:   0.639
01/29/2018 20:56:28 [INFO] exp_shallowmodel: f1_score:   0.451
01/29/2018 20:56:28 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:56:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.68      0.64       164
          F       0.74      0.77      0.76       268
          R       0.45      0.38      0.41       125

avg / total       0.62      0.64      0.63       571

01/29/2018 20:56:28 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:56:28 [INFO] exp_shallowmodel: 
[[  0   1   7   6]
 [  0 111  28  25]
 [  1  34 207  26]
 [  1  39  38  47]]
01/29/2018 20:56:30 [INFO] exp_shallowmodel: ******************** dstc2 - Round 24 
01/29/2018 20:56:30 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:56:30 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:56:30 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:56:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:56:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:56:30 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:56:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:58:06 [INFO] exp_shallowmodel: train time: 96.133s
01/29/2018 20:58:06 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:58:06 [INFO] exp_shallowmodel: accuracy:   0.599
01/29/2018 20:58:06 [INFO] exp_shallowmodel: f1_score:   0.423
01/29/2018 20:58:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:58:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.55      0.62      0.59       164
          F       0.72      0.73      0.72       268
          R       0.41      0.36      0.38       125

avg / total       0.59      0.60      0.59       571

01/29/2018 20:58:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:58:06 [INFO] exp_shallowmodel: 
[[  0   2   6   6]
 [  1 102  32  29]
 [  3  39 195  31]
 [  1  41  38  45]]
01/29/2018 20:58:08 [INFO] exp_shallowmodel: ******************** dstc2 - Round 25 
01/29/2018 20:58:08 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:58:08 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:58:08 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:58:08 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:58:08 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:58:08 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:58:08 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 20:59:32 [INFO] exp_shallowmodel: train time: 83.855s
01/29/2018 20:59:32 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 20:59:32 [INFO] exp_shallowmodel: accuracy:   0.625
01/29/2018 20:59:32 [INFO] exp_shallowmodel: f1_score:   0.444
01/29/2018 20:59:32 [INFO] exp_shallowmodel: classification report:
01/29/2018 20:59:32 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.64      0.65      0.64       164
          F       0.71      0.76      0.74       268
          R       0.41      0.38      0.40       125

avg / total       0.61      0.63      0.62       571

01/29/2018 20:59:32 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 20:59:32 [INFO] exp_shallowmodel: 
[[  0   2   9   3]
 [  0 106  29  29]
 [  3  26 203  36]
 [  2  32  43  48]]
01/29/2018 20:59:34 [INFO] exp_shallowmodel: ******************** dstc2 - Round 26 
01/29/2018 20:59:34 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 20:59:34 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 20:59:34 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 20:59:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 20:59:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 20:59:34 [INFO] exp_shallowmodel: Training: 
01/29/2018 20:59:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:01:03 [INFO] exp_shallowmodel: train time: 88.922s
01/29/2018 21:01:03 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:01:03 [INFO] exp_shallowmodel: accuracy:   0.613
01/29/2018 21:01:03 [INFO] exp_shallowmodel: f1_score:   0.455
01/29/2018 21:01:03 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:01:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.07      0.12        14
          C       0.55      0.61      0.58       164
          F       0.74      0.77      0.76       268
          R       0.39      0.34      0.36       125

avg / total       0.60      0.61      0.60       571

01/29/2018 21:01:03 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:01:03 [INFO] exp_shallowmodel: 
[[  1   1   7   5]
 [  0 100  26  38]
 [  0  37 207  24]
 [  1  43  39  42]]
01/29/2018 21:01:05 [INFO] exp_shallowmodel: ******************** dstc2 - Round 27 
01/29/2018 21:01:05 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:01:05 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:01:05 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:01:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:01:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:01:05 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:01:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:02:42 [INFO] exp_shallowmodel: train time: 97.033s
01/29/2018 21:02:42 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:02:42 [INFO] exp_shallowmodel: accuracy:   0.643
01/29/2018 21:02:42 [INFO] exp_shallowmodel: f1_score:   0.460
01/29/2018 21:02:42 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:02:42 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.66      0.64       164
          F       0.74      0.77      0.75       268
          R       0.49      0.42      0.45       125

avg / total       0.63      0.64      0.63       571

01/29/2018 21:02:42 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:02:42 [INFO] exp_shallowmodel: 
[[  0   4   7   3]
 [  0 109  30  25]
 [  4  32 206  26]
 [  3  33  37  52]]
01/29/2018 21:02:44 [INFO] exp_shallowmodel: ******************** dstc2 - Round 28 
01/29/2018 21:02:44 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:02:44 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:02:44 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:02:44 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:02:44 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:02:44 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:02:44 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:04:12 [INFO] exp_shallowmodel: train time: 88.428s
01/29/2018 21:04:12 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:04:12 [INFO] exp_shallowmodel: accuracy:   0.643
01/29/2018 21:04:12 [INFO] exp_shallowmodel: f1_score:   0.454
01/29/2018 21:04:12 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:04:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.65      0.61       164
          F       0.77      0.78      0.77       268
          R       0.46      0.40      0.43       125

avg / total       0.63      0.64      0.63       571

01/29/2018 21:04:12 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:04:12 [INFO] exp_shallowmodel: 
[[  0   2   9   3]
 [  0 107  27  30]
 [  3  30 210  25]
 [  1  46  28  50]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 21:04:14 [INFO] exp_shallowmodel: ******************** dstc2 - Round 29 
01/29/2018 21:04:14 [INFO] exp_shallowmodel: #(data) = 4568
01/29/2018 21:04:14 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:04:14 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:04:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:04:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:04:14 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:04:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:05:34 [INFO] exp_shallowmodel: train time: 79.936s
01/29/2018 21:05:34 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:05:34 [INFO] exp_shallowmodel: accuracy:   0.613
01/29/2018 21:05:34 [INFO] exp_shallowmodel: f1_score:   0.466
01/29/2018 21:05:34 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:05:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.06      0.10        16
          C       0.62      0.66      0.64       169
          F       0.69      0.71      0.70       271
          R       0.44      0.42      0.43       130

avg / total       0.60      0.61      0.61       586

01/29/2018 21:05:34 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:05:34 [INFO] exp_shallowmodel: 
[[  1   1   9   5]
 [  0 111  34  24]
 [  3  35 193  40]
 [  0  32  44  54]]
01/29/2018 21:05:36 [INFO] exp_shallowmodel: ******************** dstc2 - Round 30 
01/29/2018 21:05:36 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:05:36 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:05:36 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:05:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:05:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:05:36 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:05:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:06:59 [INFO] exp_shallowmodel: train time: 82.702s
01/29/2018 21:06:59 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:06:59 [INFO] exp_shallowmodel: accuracy:   0.630
01/29/2018 21:06:59 [INFO] exp_shallowmodel: f1_score:   0.469
01/29/2018 21:06:59 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:06:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.07      0.09        14
          C       0.60      0.72      0.66       164
          F       0.75      0.73      0.74       268
          R       0.42      0.36      0.39       125

avg / total       0.62      0.63      0.62       571

01/29/2018 21:06:59 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:06:59 [INFO] exp_shallowmodel: 
[[  1   1   6   6]
 [  1 118  21  24]
 [  2  39 196  31]
 [  5  38  37  45]]
01/29/2018 21:07:01 [INFO] exp_shallowmodel: ******************** dstc2 - Round 31 
01/29/2018 21:07:01 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:07:01 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:07:01 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:07:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:07:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:07:01 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:07:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:08:30 [INFO] exp_shallowmodel: train time: 89.182s
01/29/2018 21:08:30 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:08:30 [INFO] exp_shallowmodel: accuracy:   0.608
01/29/2018 21:08:30 [INFO] exp_shallowmodel: f1_score:   0.426
01/29/2018 21:08:30 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:08:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.64      0.61       164
          F       0.71      0.75      0.73       268
          R       0.40      0.34      0.37       125

avg / total       0.59      0.61      0.60       571

01/29/2018 21:08:30 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:08:30 [INFO] exp_shallowmodel: 
[[  0   2   6   6]
 [  0 105  29  30]
 [  3  38 200  27]
 [  2  36  45  42]]
01/29/2018 21:08:32 [INFO] exp_shallowmodel: ******************** dstc2 - Round 32 
01/29/2018 21:08:32 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:08:32 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:08:32 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:08:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:08:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:08:32 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:08:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:09:57 [INFO] exp_shallowmodel: train time: 84.613s
01/29/2018 21:09:57 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:09:57 [INFO] exp_shallowmodel: accuracy:   0.637
01/29/2018 21:09:57 [INFO] exp_shallowmodel: f1_score:   0.452
01/29/2018 21:09:57 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:09:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.60      0.60       164
          F       0.74      0.79      0.76       268
          R       0.46      0.43      0.45       125

avg / total       0.62      0.64      0.63       571

01/29/2018 21:09:57 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:09:57 [INFO] exp_shallowmodel: 
[[  0   4   7   3]
 [  0  98  29  37]
 [  2  31 212  23]
 [  1  30  40  54]]
01/29/2018 21:09:59 [INFO] exp_shallowmodel: ******************** dstc2 - Round 33 
01/29/2018 21:09:59 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:09:59 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:09:59 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:09:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:09:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:09:59 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:09:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:11:19 [INFO] exp_shallowmodel: train time: 80.351s
01/29/2018 21:11:19 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:11:19 [INFO] exp_shallowmodel: accuracy:   0.655
01/29/2018 21:11:19 [INFO] exp_shallowmodel: f1_score:   0.462
01/29/2018 21:11:19 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:11:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.63      0.62       164
          F       0.75      0.82      0.78       268
          R       0.49      0.41      0.45       125

avg / total       0.63      0.65      0.64       571

01/29/2018 21:11:19 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:11:19 [INFO] exp_shallowmodel: 
[[  0   5   8   1]
 [  0 103  30  31]
 [  2  25 220  21]
 [  2  36  36  51]]
01/29/2018 21:11:21 [INFO] exp_shallowmodel: ******************** dstc2 - Round 34 
01/29/2018 21:11:21 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:11:21 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:11:21 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:11:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:11:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:11:21 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:11:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:12:48 [INFO] exp_shallowmodel: train time: 86.914s
01/29/2018 21:12:48 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:12:48 [INFO] exp_shallowmodel: accuracy:   0.658
01/29/2018 21:12:48 [INFO] exp_shallowmodel: f1_score:   0.517
01/29/2018 21:12:48 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:12:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.14      0.20        14
          C       0.59      0.65      0.61       164
          F       0.76      0.80      0.78       268
          R       0.53      0.43      0.48       125

avg / total       0.65      0.66      0.65       571

01/29/2018 21:12:48 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:12:48 [INFO] exp_shallowmodel: 
[[  2   2   8   2]
 [  0 106  36  22]
 [  1  30 214  23]
 [  3  43  25  54]]
01/29/2018 21:12:50 [INFO] exp_shallowmodel: ******************** dstc2 - Round 35 
01/29/2018 21:12:50 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:12:50 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:12:50 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:12:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:12:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:12:50 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:12:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:14:21 [INFO] exp_shallowmodel: train time: 91.652s
01/29/2018 21:14:21 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:14:21 [INFO] exp_shallowmodel: accuracy:   0.636
01/29/2018 21:14:21 [INFO] exp_shallowmodel: f1_score:   0.444
01/29/2018 21:14:21 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:14:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.62      0.68      0.65       164
          F       0.74      0.78      0.76       268
          R       0.40      0.34      0.37       125

avg / total       0.61      0.64      0.62       571

01/29/2018 21:14:21 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:14:21 [INFO] exp_shallowmodel: 
[[  0   1   7   6]
 [  0 112  26  26]
 [  3  26 209  30]
 [  1  41  41  42]]
01/29/2018 21:14:23 [INFO] exp_shallowmodel: ******************** dstc2 - Round 36 
01/29/2018 21:14:23 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:14:23 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:14:23 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:14:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:14:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:14:23 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:14:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:15:49 [INFO] exp_shallowmodel: train time: 85.696s
01/29/2018 21:15:49 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:15:49 [INFO] exp_shallowmodel: accuracy:   0.623
01/29/2018 21:15:49 [INFO] exp_shallowmodel: f1_score:   0.445
01/29/2018 21:15:49 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:15:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.67      0.63       164
          F       0.72      0.74      0.73       268
          R       0.45      0.39      0.42       125

avg / total       0.61      0.62      0.61       571

01/29/2018 21:15:49 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:15:49 [INFO] exp_shallowmodel: 
[[  0   1  10   3]
 [  0 110  26  28]
 [  4  39 197  28]
 [  1  35  40  49]]
01/29/2018 21:15:51 [INFO] exp_shallowmodel: ******************** dstc2 - Round 37 
01/29/2018 21:15:51 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:15:51 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:15:51 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:15:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:15:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:15:51 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:15:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:17:17 [INFO] exp_shallowmodel: train time: 86.451s
01/29/2018 21:17:17 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:17:17 [INFO] exp_shallowmodel: accuracy:   0.622
01/29/2018 21:17:17 [INFO] exp_shallowmodel: f1_score:   0.439
01/29/2018 21:17:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:17:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.66      0.62       164
          F       0.74      0.75      0.74       268
          R       0.43      0.36      0.39       125

avg / total       0.61      0.62      0.61       571

01/29/2018 21:17:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:17:17 [INFO] exp_shallowmodel: 
[[  0   0   9   5]
 [  0 109  27  28]
 [  4  36 201  27]
 [  2  42  36  45]]
01/29/2018 21:17:19 [INFO] exp_shallowmodel: ******************** dstc2 - Round 38 
01/29/2018 21:17:19 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:17:19 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:17:19 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:17:19 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:17:19 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:17:19 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:17:19 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:18:44 [INFO] exp_shallowmodel: train time: 84.611s
01/29/2018 21:18:44 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:18:44 [INFO] exp_shallowmodel: accuracy:   0.608
01/29/2018 21:18:44 [INFO] exp_shallowmodel: f1_score:   0.431
01/29/2018 21:18:44 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:18:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.66      0.63       164
          F       0.71      0.73      0.72       268
          R       0.41      0.35      0.38       125

avg / total       0.60      0.61      0.60       571

01/29/2018 21:18:44 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:18:44 [INFO] exp_shallowmodel: 
[[  0   2   8   4]
 [  1 108  27  28]
 [  6  36 195  31]
 [  2  34  45  44]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 21:18:46 [INFO] exp_shallowmodel: ******************** dstc2 - Round 39 
01/29/2018 21:18:46 [INFO] exp_shallowmodel: #(data) = 4568
01/29/2018 21:18:46 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:18:46 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:18:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:18:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:18:46 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:18:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:20:26 [INFO] exp_shallowmodel: train time: 100.220s
01/29/2018 21:20:26 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:20:26 [INFO] exp_shallowmodel: accuracy:   0.621
01/29/2018 21:20:26 [INFO] exp_shallowmodel: f1_score:   0.471
01/29/2018 21:20:26 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:20:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.06      0.09        16
          C       0.60      0.58      0.59       169
          F       0.72      0.75      0.73       271
          R       0.46      0.48      0.47       130

avg / total       0.61      0.62      0.62       586

01/29/2018 21:20:26 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:20:26 [INFO] exp_shallowmodel: 
[[  1   0  10   5]
 [  0  98  38  33]
 [  3  31 202  35]
 [  3  33  31  63]]
01/29/2018 21:20:28 [INFO] exp_shallowmodel: ******************** dstc2 - Round 40 
01/29/2018 21:20:28 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:20:28 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:20:28 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:20:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:20:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:20:28 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:20:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:21:50 [INFO] exp_shallowmodel: train time: 82.253s
01/29/2018 21:21:50 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:21:50 [INFO] exp_shallowmodel: accuracy:   0.643
01/29/2018 21:21:50 [INFO] exp_shallowmodel: f1_score:   0.461
01/29/2018 21:21:50 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:21:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.67      0.63       164
          F       0.74      0.75      0.75       268
          R       0.50      0.44      0.47       125

avg / total       0.63      0.64      0.63       571

01/29/2018 21:21:50 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:21:50 [INFO] exp_shallowmodel: 
[[  0   3   8   3]
 [  0 110  25  29]
 [  2  42 202  22]
 [  1  30  39  55]]
01/29/2018 21:21:52 [INFO] exp_shallowmodel: ******************** dstc2 - Round 41 
01/29/2018 21:21:52 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:21:52 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:21:52 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:21:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:21:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:21:52 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:21:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:23:22 [INFO] exp_shallowmodel: train time: 89.429s
01/29/2018 21:23:22 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:23:22 [INFO] exp_shallowmodel: accuracy:   0.616
01/29/2018 21:23:22 [INFO] exp_shallowmodel: f1_score:   0.441
01/29/2018 21:23:22 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:23:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.63      0.61       164
          F       0.73      0.73      0.73       268
          R       0.44      0.42      0.43       125

avg / total       0.60      0.62      0.61       571

01/29/2018 21:23:22 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:23:22 [INFO] exp_shallowmodel: 
[[  0   1   9   4]
 [  0 104  31  29]
 [  3  37 196  32]
 [  2  37  34  52]]
01/29/2018 21:23:24 [INFO] exp_shallowmodel: ******************** dstc2 - Round 42 
01/29/2018 21:23:24 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:23:24 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:23:24 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:23:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:23:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:23:24 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:23:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:24:50 [INFO] exp_shallowmodel: train time: 86.406s
01/29/2018 21:24:50 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:24:50 [INFO] exp_shallowmodel: accuracy:   0.623
01/29/2018 21:24:50 [INFO] exp_shallowmodel: f1_score:   0.469
01/29/2018 21:24:50 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:24:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.57      0.65      0.61       164
          F       0.74      0.74      0.74       268
          R       0.44      0.39      0.41       125

avg / total       0.61      0.62      0.62       571

01/29/2018 21:24:50 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:24:50 [INFO] exp_shallowmodel: 
[[  1   2   7   4]
 [  0 107  24  33]
 [  2  41 199  26]
 [  1  37  38  49]]
01/29/2018 21:24:52 [INFO] exp_shallowmodel: ******************** dstc2 - Round 43 
01/29/2018 21:24:52 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:24:52 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:24:52 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:24:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:24:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:24:52 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:24:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:26:23 [INFO] exp_shallowmodel: train time: 91.482s
01/29/2018 21:26:23 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:26:23 [INFO] exp_shallowmodel: accuracy:   0.604
01/29/2018 21:26:23 [INFO] exp_shallowmodel: f1_score:   0.428
01/29/2018 21:26:23 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:26:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.56      0.62      0.59       164
          F       0.72      0.74      0.73       268
          R       0.43      0.37      0.39       125

avg / total       0.59      0.60      0.60       571

01/29/2018 21:26:23 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:26:23 [INFO] exp_shallowmodel: 
[[  0   2   8   4]
 [  1 101  35  27]
 [  5  34 198  31]
 [  3  42  34  46]]
01/29/2018 21:26:25 [INFO] exp_shallowmodel: ******************** dstc2 - Round 44 
01/29/2018 21:26:25 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:26:25 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:26:25 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:26:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:26:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:26:25 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:26:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:27:58 [INFO] exp_shallowmodel: train time: 92.826s
01/29/2018 21:27:58 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:27:58 [INFO] exp_shallowmodel: accuracy:   0.629
01/29/2018 21:27:58 [INFO] exp_shallowmodel: f1_score:   0.449
01/29/2018 21:27:58 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:27:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.63      0.62       164
          F       0.72      0.76      0.74       268
          R       0.46      0.42      0.44       125

avg / total       0.61      0.63      0.62       571

01/29/2018 21:27:58 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:27:58 [INFO] exp_shallowmodel: 
[[  0   4   6   4]
 [  0 104  33  27]
 [  3  33 203  29]
 [  2  31  40  52]]
01/29/2018 21:28:00 [INFO] exp_shallowmodel: ******************** dstc2 - Round 45 
01/29/2018 21:28:00 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:28:00 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:28:00 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:28:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:28:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:28:00 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:28:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:29:34 [INFO] exp_shallowmodel: train time: 93.834s
01/29/2018 21:29:34 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:29:34 [INFO] exp_shallowmodel: accuracy:   0.608
01/29/2018 21:29:34 [INFO] exp_shallowmodel: f1_score:   0.449
01/29/2018 21:29:34 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:29:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.07      0.09        14
          C       0.57      0.59      0.58       164
          F       0.72      0.76      0.74       268
          R       0.41      0.36      0.38       125

avg / total       0.59      0.61      0.60       571

01/29/2018 21:29:34 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:29:34 [INFO] exp_shallowmodel: 
[[  1   3   5   5]
 [  0  96  38  30]
 [  4  29 205  30]
 [  3  39  38  45]]
01/29/2018 21:29:36 [INFO] exp_shallowmodel: ******************** dstc2 - Round 46 
01/29/2018 21:29:36 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:29:36 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:29:36 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:29:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:29:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:29:36 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:29:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:31:06 [INFO] exp_shallowmodel: train time: 90.708s
01/29/2018 21:31:06 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:31:06 [INFO] exp_shallowmodel: accuracy:   0.625
01/29/2018 21:31:06 [INFO] exp_shallowmodel: f1_score:   0.495
01/29/2018 21:31:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:31:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.67      0.14      0.24        14
          C       0.59      0.62      0.61       164
          F       0.72      0.77      0.74       268
          R       0.43      0.37      0.40       125

avg / total       0.62      0.63      0.62       571

01/29/2018 21:31:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:31:06 [INFO] exp_shallowmodel: 
[[  2   1   7   4]
 [  0 102  31  31]
 [  0  35 207  26]
 [  1  35  43  46]]
01/29/2018 21:31:08 [INFO] exp_shallowmodel: ******************** dstc2 - Round 47 
01/29/2018 21:31:08 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:31:08 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:31:08 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:31:08 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:31:08 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:31:08 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:31:08 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:32:37 [INFO] exp_shallowmodel: train time: 88.880s
01/29/2018 21:32:37 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:32:37 [INFO] exp_shallowmodel: accuracy:   0.650
01/29/2018 21:32:37 [INFO] exp_shallowmodel: f1_score:   0.495
01/29/2018 21:32:37 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:32:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.07      0.12        14
          C       0.60      0.66      0.63       164
          F       0.74      0.76      0.75       268
          R       0.51      0.46      0.48       125

avg / total       0.64      0.65      0.64       571

01/29/2018 21:32:37 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:32:37 [INFO] exp_shallowmodel: 
[[  1   3   7   3]
 [  0 109  28  27]
 [  2  38 204  24]
 [  0  33  35  57]]
01/29/2018 21:32:39 [INFO] exp_shallowmodel: ******************** dstc2 - Round 48 
01/29/2018 21:32:39 [INFO] exp_shallowmodel: #(data) = 4583
01/29/2018 21:32:39 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:32:39 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:32:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:32:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:32:39 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:32:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:34:12 [INFO] exp_shallowmodel: train time: 92.745s
01/29/2018 21:34:12 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:34:12 [INFO] exp_shallowmodel: accuracy:   0.644
01/29/2018 21:34:12 [INFO] exp_shallowmodel: f1_score:   0.488
01/29/2018 21:34:12 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:34:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.63      0.66      0.65       164
          F       0.74      0.75      0.75       268
          R       0.47      0.45      0.46       125

avg / total       0.63      0.64      0.64       571

01/29/2018 21:34:12 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:34:12 [INFO] exp_shallowmodel: 
[[  1   0   9   4]
 [  1 109  27  27]
 [  3  32 202  31]
 [  1  32  36  56]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 21:34:14 [INFO] exp_shallowmodel: ******************** dstc2 - Round 49 
01/29/2018 21:34:14 [INFO] exp_shallowmodel: #(data) = 4568
01/29/2018 21:34:14 [INFO] exp_shallowmodel: #(feature) = 9359
01/29/2018 21:34:14 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:34:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:34:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:34:14 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:34:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:35:40 [INFO] exp_shallowmodel: train time: 85.887s
01/29/2018 21:35:40 [INFO] exp_shallowmodel: test time:  0.008s
01/29/2018 21:35:40 [INFO] exp_shallowmodel: accuracy:   0.625
01/29/2018 21:35:40 [INFO] exp_shallowmodel: f1_score:   0.444
01/29/2018 21:35:40 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:35:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.62      0.66      0.64       169
          F       0.72      0.76      0.74       271
          R       0.42      0.38      0.40       130

avg / total       0.60      0.62      0.61       586

01/29/2018 21:35:40 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:35:40 [INFO] exp_shallowmodel: 
[[  0   1  11   4]
 [  0 111  27  31]
 [  2  30 205  34]
 [  1  36  43  50]]
01/29/2018 21:35:48 [INFO] exp_shallowmodel: ******************** dstc3 - Round 0 
01/29/2018 21:35:48 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:35:48 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:35:48 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:35:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:35:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:35:48 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:35:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:37:04 [INFO] exp_shallowmodel: train time: 75.703s
01/29/2018 21:37:04 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:37:04 [INFO] exp_shallowmodel: accuracy:   0.623
01/29/2018 21:37:04 [INFO] exp_shallowmodel: f1_score:   0.488
01/29/2018 21:37:04 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:37:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.15      0.22        20
          C       0.56      0.60      0.58       169
          F       0.73      0.78      0.76       281
          R       0.43      0.36      0.39       122

avg / total       0.61      0.62      0.61       592

01/29/2018 21:37:04 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:37:04 [INFO] exp_shallowmodel: 
[[  3   1  10   6]
 [  2 102  33  32]
 [  1  40 220  20]
 [  1  39  38  44]]
01/29/2018 21:37:06 [INFO] exp_shallowmodel: ******************** dstc3 - Round 1 
01/29/2018 21:37:06 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:37:06 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:37:06 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:37:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:37:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:37:06 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:37:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:38:20 [INFO] exp_shallowmodel: train time: 73.230s
01/29/2018 21:38:20 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:38:20 [INFO] exp_shallowmodel: accuracy:   0.606
01/29/2018 21:38:20 [INFO] exp_shallowmodel: f1_score:   0.450
01/29/2018 21:38:20 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:38:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.10      0.12        20
          C       0.57      0.62      0.59       169
          F       0.74      0.77      0.75       281
          R       0.36      0.31      0.33       122

avg / total       0.59      0.61      0.60       592

01/29/2018 21:38:20 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:38:20 [INFO] exp_shallowmodel: 
[[  2   3   8   7]
 [  4 104  29  32]
 [  2  35 215  29]
 [  4  41  39  38]]
01/29/2018 21:38:22 [INFO] exp_shallowmodel: ******************** dstc3 - Round 2 
01/29/2018 21:38:22 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:38:22 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:38:22 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:38:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:38:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:38:22 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:38:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:39:41 [INFO] exp_shallowmodel: train time: 79.086s
01/29/2018 21:39:41 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:39:41 [INFO] exp_shallowmodel: accuracy:   0.595
01/29/2018 21:39:41 [INFO] exp_shallowmodel: f1_score:   0.459
01/29/2018 21:39:41 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:39:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.15      0.20        20
          C       0.52      0.56      0.54       169
          F       0.73      0.76      0.74       281
          R       0.37      0.33      0.35       122

avg / total       0.58      0.59      0.59       592

01/29/2018 21:39:41 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:39:41 [INFO] exp_shallowmodel: 
[[  3   6   9   2]
 [  3  95  33  38]
 [  0  40 214  27]
 [  4  40  38  40]]
01/29/2018 21:39:44 [INFO] exp_shallowmodel: ******************** dstc3 - Round 3 
01/29/2018 21:39:44 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:39:44 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:39:44 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:39:44 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:39:44 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:39:44 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:39:44 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:40:58 [INFO] exp_shallowmodel: train time: 73.671s
01/29/2018 21:40:58 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:40:58 [INFO] exp_shallowmodel: accuracy:   0.630
01/29/2018 21:40:58 [INFO] exp_shallowmodel: f1_score:   0.483
01/29/2018 21:40:58 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:40:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.10      0.14        20
          C       0.60      0.66      0.63       169
          F       0.73      0.75      0.74       281
          R       0.46      0.40      0.43       122

avg / total       0.62      0.63      0.62       592

01/29/2018 21:40:58 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:40:58 [INFO] exp_shallowmodel: 
[[  2   5   9   4]
 [  2 111  32  24]
 [  2  38 211  30]
 [  3  31  39  49]]
01/29/2018 21:41:00 [INFO] exp_shallowmodel: ******************** dstc3 - Round 4 
01/29/2018 21:41:00 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:41:00 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:41:00 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:41:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:41:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:41:00 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:41:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:42:17 [INFO] exp_shallowmodel: train time: 76.598s
01/29/2018 21:42:17 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:42:17 [INFO] exp_shallowmodel: accuracy:   0.581
01/29/2018 21:42:17 [INFO] exp_shallowmodel: f1_score:   0.406
01/29/2018 21:42:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:42:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.49      0.56      0.53       169
          F       0.74      0.73      0.73       281
          R       0.37      0.35      0.36       122

avg / total       0.57      0.58      0.57       592

01/29/2018 21:42:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:42:17 [INFO] exp_shallowmodel: 
[[  0   6   7   7]
 [  1  95  34  39]
 [  3  46 206  26]
 [  1  45  33  43]]
01/29/2018 21:42:20 [INFO] exp_shallowmodel: ******************** dstc3 - Round 5 
01/29/2018 21:42:20 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:42:20 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:42:20 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:42:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:42:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:42:20 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:42:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:43:33 [INFO] exp_shallowmodel: train time: 73.546s
01/29/2018 21:43:33 [INFO] exp_shallowmodel: test time:  0.013s
01/29/2018 21:43:33 [INFO] exp_shallowmodel: accuracy:   0.578
01/29/2018 21:43:33 [INFO] exp_shallowmodel: f1_score:   0.408
01/29/2018 21:43:33 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:43:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.50      0.52      0.51       169
          F       0.70      0.73      0.72       281
          R       0.42      0.39      0.41       122

avg / total       0.56      0.58      0.57       592

01/29/2018 21:43:33 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:43:33 [INFO] exp_shallowmodel: 
[[  0   4  11   5]
 [  1  88  42  38]
 [  3  48 206  24]
 [  2  36  36  48]]
01/29/2018 21:43:36 [INFO] exp_shallowmodel: ******************** dstc3 - Round 6 
01/29/2018 21:43:36 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:43:36 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:43:36 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:43:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:43:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:43:36 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:43:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:44:52 [INFO] exp_shallowmodel: train time: 75.830s
01/29/2018 21:44:52 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:44:52 [INFO] exp_shallowmodel: accuracy:   0.618
01/29/2018 21:44:52 [INFO] exp_shallowmodel: f1_score:   0.449
01/29/2018 21:44:52 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:44:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.05      0.08        20
          C       0.56      0.63      0.59       169
          F       0.75      0.77      0.76       281
          R       0.38      0.34      0.36       122

avg / total       0.60      0.62      0.61       592

01/29/2018 21:44:52 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:44:52 [INFO] exp_shallowmodel: 
[[  1   5   9   5]
 [  0 107  24  38]
 [  1  39 217  24]
 [  2  40  39  41]]
01/29/2018 21:44:54 [INFO] exp_shallowmodel: ******************** dstc3 - Round 7 
01/29/2018 21:44:54 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:44:54 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:44:54 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:44:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:44:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:44:54 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:44:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:46:03 [INFO] exp_shallowmodel: train time: 68.934s
01/29/2018 21:46:03 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:46:03 [INFO] exp_shallowmodel: accuracy:   0.586
01/29/2018 21:46:03 [INFO] exp_shallowmodel: f1_score:   0.423
01/29/2018 21:46:03 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:46:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.05      0.07        20
          C       0.51      0.57      0.54       169
          F       0.72      0.75      0.73       281
          R       0.38      0.32      0.35       122

avg / total       0.57      0.59      0.58       592

01/29/2018 21:46:03 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:46:03 [INFO] exp_shallowmodel: 
[[  1   4  10   5]
 [  0  97  38  34]
 [  3  44 210  24]
 [  4  46  33  39]]
01/29/2018 21:46:06 [INFO] exp_shallowmodel: ******************** dstc3 - Round 8 
01/29/2018 21:46:06 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:46:06 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:46:06 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:46:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:46:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:46:06 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:46:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:47:13 [INFO] exp_shallowmodel: train time: 67.410s
01/29/2018 21:47:13 [INFO] exp_shallowmodel: test time:  0.012s
01/29/2018 21:47:13 [INFO] exp_shallowmodel: accuracy:   0.581
01/29/2018 21:47:13 [INFO] exp_shallowmodel: f1_score:   0.445
01/29/2018 21:47:13 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:47:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.10      0.15        20
          C       0.52      0.58      0.55       169
          F       0.71      0.72      0.71       281
          R       0.37      0.35      0.36       122

avg / total       0.57      0.58      0.58       592

01/29/2018 21:47:13 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:47:13 [INFO] exp_shallowmodel: 
[[  2   3  10   5]
 [  2  98  39  30]
 [  1  42 201  37]
 [  1  46  32  43]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 21:47:16 [INFO] exp_shallowmodel: ******************** dstc3 - Round 9 
01/29/2018 21:47:16 [INFO] exp_shallowmodel: #(data) = 4736
01/29/2018 21:47:16 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:47:16 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:47:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:47:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:47:16 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:47:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:48:30 [INFO] exp_shallowmodel: train time: 73.868s
01/29/2018 21:48:30 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:48:30 [INFO] exp_shallowmodel: accuracy:   0.592
01/29/2018 21:48:30 [INFO] exp_shallowmodel: f1_score:   0.410
01/29/2018 21:48:30 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:48:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        28
          C       0.52      0.56      0.54       172
          F       0.73      0.78      0.75       283
          R       0.36      0.33      0.34       123

avg / total       0.56      0.59      0.58       606

01/29/2018 21:48:30 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:48:30 [INFO] exp_shallowmodel: 
[[  0   9  11   8]
 [  0  97  34  41]
 [  1  36 221  25]
 [  1  43  38  41]]
01/29/2018 21:48:32 [INFO] exp_shallowmodel: ******************** dstc3 - Round 10 
01/29/2018 21:48:32 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:48:32 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:48:32 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:48:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:48:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:48:32 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:48:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:49:58 [INFO] exp_shallowmodel: train time: 85.290s
01/29/2018 21:49:58 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:49:58 [INFO] exp_shallowmodel: accuracy:   0.615
01/29/2018 21:49:58 [INFO] exp_shallowmodel: f1_score:   0.452
01/29/2018 21:49:58 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:49:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.05      0.08        20
          C       0.57      0.62      0.59       169
          F       0.72      0.76      0.74       281
          R       0.41      0.38      0.39       122

avg / total       0.60      0.61      0.60       592

01/29/2018 21:49:58 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:49:58 [INFO] exp_shallowmodel: 
[[  1   4  11   4]
 [  1 104  32  32]
 [  2  37 213  29]
 [  1  36  39  46]]
01/29/2018 21:50:00 [INFO] exp_shallowmodel: ******************** dstc3 - Round 11 
01/29/2018 21:50:00 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:50:00 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:50:00 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:50:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:50:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:50:00 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:50:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:51:09 [INFO] exp_shallowmodel: train time: 68.824s
01/29/2018 21:51:09 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:51:09 [INFO] exp_shallowmodel: accuracy:   0.635
01/29/2018 21:51:09 [INFO] exp_shallowmodel: f1_score:   0.484
01/29/2018 21:51:09 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:51:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.10      0.16        20
          C       0.58      0.63      0.60       169
          F       0.74      0.79      0.76       281
          R       0.44      0.39      0.41       122

avg / total       0.62      0.64      0.62       592

01/29/2018 21:51:09 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:51:09 [INFO] exp_shallowmodel: 
[[  2   6   9   3]
 [  1 106  32  30]
 [  1  32 221  27]
 [  1  39  35  47]]
01/29/2018 21:51:12 [INFO] exp_shallowmodel: ******************** dstc3 - Round 12 
01/29/2018 21:51:12 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:51:12 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:51:12 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:51:12 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:51:12 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:51:12 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:51:12 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:52:22 [INFO] exp_shallowmodel: train time: 70.206s
01/29/2018 21:52:22 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:52:22 [INFO] exp_shallowmodel: accuracy:   0.606
01/29/2018 21:52:22 [INFO] exp_shallowmodel: f1_score:   0.461
01/29/2018 21:52:22 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:52:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.55      0.60      0.57       169
          F       0.75      0.75      0.75       281
          R       0.38      0.37      0.38       122

avg / total       0.60      0.61      0.60       592

01/29/2018 21:52:22 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:52:22 [INFO] exp_shallowmodel: 
[[  2   2  11   5]
 [  2 101  33  33]
 [  0  36 211  34]
 [  3  46  28  45]]
01/29/2018 21:52:25 [INFO] exp_shallowmodel: ******************** dstc3 - Round 13 
01/29/2018 21:52:25 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:52:25 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:52:25 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:52:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:52:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:52:25 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:52:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:53:44 [INFO] exp_shallowmodel: train time: 79.475s
01/29/2018 21:53:44 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:53:44 [INFO] exp_shallowmodel: accuracy:   0.654
01/29/2018 21:53:44 [INFO] exp_shallowmodel: f1_score:   0.544
01/29/2018 21:53:44 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:53:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.62      0.25      0.36        20
          C       0.58      0.60      0.59       169
          F       0.75      0.82      0.78       281
          R       0.49      0.40      0.44       122

avg / total       0.64      0.65      0.64       592

01/29/2018 21:53:44 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:53:44 [INFO] exp_shallowmodel: 
[[  5   2   9   4]
 [  0 102  36  31]
 [  2  32 231  16]
 [  1  39  33  49]]
01/29/2018 21:53:47 [INFO] exp_shallowmodel: ******************** dstc3 - Round 14 
01/29/2018 21:53:47 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:53:47 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:53:47 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:53:47 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:53:47 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:53:47 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:53:47 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:55:05 [INFO] exp_shallowmodel: train time: 77.898s
01/29/2018 21:55:05 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:55:05 [INFO] exp_shallowmodel: accuracy:   0.600
01/29/2018 21:55:05 [INFO] exp_shallowmodel: f1_score:   0.438
01/29/2018 21:55:05 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:55:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.05      0.07        20
          C       0.56      0.59      0.57       169
          F       0.72      0.75      0.73       281
          R       0.40      0.36      0.38       122

avg / total       0.58      0.60      0.59       592

01/29/2018 21:55:05 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:55:05 [INFO] exp_shallowmodel: 
[[  1   2  11   6]
 [  1  99  37  32]
 [  2  39 211  29]
 [  4  38  36  44]]
01/29/2018 21:55:07 [INFO] exp_shallowmodel: ******************** dstc3 - Round 15 
01/29/2018 21:55:07 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:55:07 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:55:07 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:55:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:55:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:55:07 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:55:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:56:28 [INFO] exp_shallowmodel: train time: 80.324s
01/29/2018 21:56:28 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:56:28 [INFO] exp_shallowmodel: accuracy:   0.600
01/29/2018 21:56:28 [INFO] exp_shallowmodel: f1_score:   0.418
01/29/2018 21:56:28 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:56:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.54      0.61      0.58       169
          F       0.74      0.75      0.74       281
          R       0.36      0.34      0.35       122

avg / total       0.58      0.60      0.59       592

01/29/2018 21:56:28 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:56:28 [INFO] exp_shallowmodel: 
[[  0   4   6  10]
 [  1 103  31  34]
 [  2  39 210  30]
 [  1  43  36  42]]
01/29/2018 21:56:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 16 
01/29/2018 21:56:30 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:56:30 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:56:30 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:56:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:56:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:56:30 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:56:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:57:48 [INFO] exp_shallowmodel: train time: 77.797s
01/29/2018 21:57:48 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:57:48 [INFO] exp_shallowmodel: accuracy:   0.596
01/29/2018 21:57:48 [INFO] exp_shallowmodel: f1_score:   0.462
01/29/2018 21:57:48 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:57:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.15      0.20        20
          C       0.49      0.57      0.53       169
          F       0.74      0.76      0.75       281
          R       0.41      0.34      0.37       122

avg / total       0.59      0.60      0.59       592

01/29/2018 21:57:48 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:57:48 [INFO] exp_shallowmodel: 
[[  3   4   8   5]
 [  1  96  36  36]
 [  2  48 213  18]
 [  4  47  30  41]]
01/29/2018 21:57:51 [INFO] exp_shallowmodel: ******************** dstc3 - Round 17 
01/29/2018 21:57:51 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:57:51 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:57:51 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:57:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:57:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:57:51 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:57:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 21:59:06 [INFO] exp_shallowmodel: train time: 75.352s
01/29/2018 21:59:06 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 21:59:06 [INFO] exp_shallowmodel: accuracy:   0.591
01/29/2018 21:59:06 [INFO] exp_shallowmodel: f1_score:   0.468
01/29/2018 21:59:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 21:59:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.15      0.23        20
          C       0.52      0.56      0.54       169
          F       0.74      0.74      0.74       281
          R       0.36      0.36      0.36       122

avg / total       0.59      0.59      0.59       592

01/29/2018 21:59:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 21:59:06 [INFO] exp_shallowmodel: 
[[  3   3   8   6]
 [  0  95  29  45]
 [  1  45 208  27]
 [  2  40  36  44]]
01/29/2018 21:59:09 [INFO] exp_shallowmodel: ******************** dstc3 - Round 18 
01/29/2018 21:59:09 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 21:59:09 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 21:59:09 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 21:59:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 21:59:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 21:59:09 [INFO] exp_shallowmodel: Training: 
01/29/2018 21:59:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:00:24 [INFO] exp_shallowmodel: train time: 75.704s
01/29/2018 22:00:24 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:00:24 [INFO] exp_shallowmodel: accuracy:   0.610
01/29/2018 22:00:24 [INFO] exp_shallowmodel: f1_score:   0.437
01/29/2018 22:00:24 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:00:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.55      0.60      0.58       169
          F       0.72      0.74      0.73       281
          R       0.46      0.43      0.44       122

avg / total       0.60      0.61      0.60       592

01/29/2018 22:00:24 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:00:24 [INFO] exp_shallowmodel: 
[[  0   5  12   3]
 [  3 101  29  36]
 [  1  49 208  23]
 [  5  27  38  52]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 22:00:27 [INFO] exp_shallowmodel: ******************** dstc3 - Round 19 
01/29/2018 22:00:27 [INFO] exp_shallowmodel: #(data) = 4736
01/29/2018 22:00:27 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:00:27 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:00:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:00:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:00:27 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:00:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:01:40 [INFO] exp_shallowmodel: train time: 72.665s
01/29/2018 22:01:40 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:01:40 [INFO] exp_shallowmodel: accuracy:   0.592
01/29/2018 22:01:40 [INFO] exp_shallowmodel: f1_score:   0.433
01/29/2018 22:01:40 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:01:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.06        28
          C       0.53      0.55      0.54       172
          F       0.71      0.76      0.74       283
          R       0.40      0.39      0.40       123

avg / total       0.57      0.59      0.58       606

01/29/2018 22:01:40 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:01:40 [INFO] exp_shallowmodel: 
[[  1  11  12   4]
 [  0  94  41  37]
 [  1  35 216  31]
 [  3  38  34  48]]
01/29/2018 22:01:42 [INFO] exp_shallowmodel: ******************** dstc3 - Round 20 
01/29/2018 22:01:42 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:01:42 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:01:42 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:01:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:01:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:01:42 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:01:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:02:53 [INFO] exp_shallowmodel: train time: 70.782s
01/29/2018 22:02:53 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:02:53 [INFO] exp_shallowmodel: accuracy:   0.581
01/29/2018 22:02:53 [INFO] exp_shallowmodel: f1_score:   0.449
01/29/2018 22:02:53 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:02:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.31      0.20      0.24        20
          C       0.53      0.55      0.54       169
          F       0.74      0.77      0.76       281
          R       0.27      0.25      0.26       122

avg / total       0.57      0.58      0.57       592

01/29/2018 22:02:53 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:02:53 [INFO] exp_shallowmodel: 
[[  4   4  10   2]
 [  2  93  30  44]
 [  3  26 217  35]
 [  4  52  36  30]]
01/29/2018 22:02:56 [INFO] exp_shallowmodel: ******************** dstc3 - Round 21 
01/29/2018 22:02:56 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:02:56 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:02:56 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:02:56 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:02:56 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:02:56 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:02:56 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:04:08 [INFO] exp_shallowmodel: train time: 71.831s
01/29/2018 22:04:08 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:04:08 [INFO] exp_shallowmodel: accuracy:   0.608
01/29/2018 22:04:08 [INFO] exp_shallowmodel: f1_score:   0.446
01/29/2018 22:04:08 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:04:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.05      0.08        20
          C       0.55      0.60      0.57       169
          F       0.71      0.76      0.74       281
          R       0.42      0.36      0.39       122

avg / total       0.59      0.61      0.60       592

01/29/2018 22:04:08 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:04:08 [INFO] exp_shallowmodel: 
[[  1   6  11   2]
 [  2 102  35  30]
 [  0  40 213  28]
 [  1  38  39  44]]
01/29/2018 22:04:10 [INFO] exp_shallowmodel: ******************** dstc3 - Round 22 
01/29/2018 22:04:10 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:04:10 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:04:10 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:04:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:04:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:04:10 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:04:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:05:27 [INFO] exp_shallowmodel: train time: 76.675s
01/29/2018 22:05:27 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:05:27 [INFO] exp_shallowmodel: accuracy:   0.610
01/29/2018 22:05:27 [INFO] exp_shallowmodel: f1_score:   0.423
01/29/2018 22:05:27 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:05:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.55      0.57      0.56       169
          F       0.74      0.79      0.76       281
          R       0.38      0.35      0.36       122

avg / total       0.59      0.61      0.60       592

01/29/2018 22:05:27 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:05:27 [INFO] exp_shallowmodel: 
[[  0   4  11   5]
 [  2  97  29  41]
 [  1  34 221  25]
 [  3  40  36  43]]
01/29/2018 22:05:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 23 
01/29/2018 22:05:30 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:05:30 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:05:30 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:05:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:05:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:05:30 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:05:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:06:43 [INFO] exp_shallowmodel: train time: 73.511s
01/29/2018 22:06:43 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:06:43 [INFO] exp_shallowmodel: accuracy:   0.596
01/29/2018 22:06:43 [INFO] exp_shallowmodel: f1_score:   0.432
01/29/2018 22:06:43 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:06:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.05      0.08        20
          C       0.55      0.60      0.58       169
          F       0.71      0.75      0.73       281
          R       0.37      0.32      0.34       122

avg / total       0.58      0.60      0.58       592

01/29/2018 22:06:43 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:06:43 [INFO] exp_shallowmodel: 
[[  1   3   9   7]
 [  2 102  34  31]
 [  0  42 211  28]
 [  2  38  43  39]]
01/29/2018 22:06:46 [INFO] exp_shallowmodel: ******************** dstc3 - Round 24 
01/29/2018 22:06:46 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:06:46 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:06:46 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:06:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:06:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:06:46 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:06:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:08:02 [INFO] exp_shallowmodel: train time: 76.706s
01/29/2018 22:08:02 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:08:02 [INFO] exp_shallowmodel: accuracy:   0.598
01/29/2018 22:08:02 [INFO] exp_shallowmodel: f1_score:   0.449
01/29/2018 22:08:02 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:08:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.10      0.14        20
          C       0.55      0.58      0.56       169
          F       0.73      0.76      0.74       281
          R       0.37      0.34      0.35       122

avg / total       0.58      0.60      0.59       592

01/29/2018 22:08:02 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:08:02 [INFO] exp_shallowmodel: 
[[  2   0   6  12]
 [  2  98  37  32]
 [  2  40 213  26]
 [  3  41  37  41]]
01/29/2018 22:08:05 [INFO] exp_shallowmodel: ******************** dstc3 - Round 25 
01/29/2018 22:08:05 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:08:05 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:08:05 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:08:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:08:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:08:05 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:08:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:09:21 [INFO] exp_shallowmodel: train time: 76.030s
01/29/2018 22:09:21 [INFO] exp_shallowmodel: test time:  0.012s
01/29/2018 22:09:21 [INFO] exp_shallowmodel: accuracy:   0.606
01/29/2018 22:09:21 [INFO] exp_shallowmodel: f1_score:   0.423
01/29/2018 22:09:21 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:09:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.59      0.59      0.59       169
          F       0.69      0.77      0.73       281
          R       0.39      0.35      0.37       122

avg / total       0.58      0.61      0.59       592

01/29/2018 22:09:21 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:09:21 [INFO] exp_shallowmodel: 
[[  0   3  12   5]
 [  1 100  37  31]
 [  1  34 216  30]
 [  1  32  46  43]]
01/29/2018 22:09:24 [INFO] exp_shallowmodel: ******************** dstc3 - Round 26 
01/29/2018 22:09:24 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:09:24 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:09:24 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:09:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:09:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:09:24 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:09:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:10:31 [INFO] exp_shallowmodel: train time: 67.459s
01/29/2018 22:10:31 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:10:31 [INFO] exp_shallowmodel: accuracy:   0.622
01/29/2018 22:10:31 [INFO] exp_shallowmodel: f1_score:   0.453
01/29/2018 22:10:31 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:10:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.05      0.08        20
          C       0.58      0.60      0.59       169
          F       0.72      0.79      0.75       281
          R       0.43      0.36      0.39       122

avg / total       0.60      0.62      0.61       592

01/29/2018 22:10:31 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:10:31 [INFO] exp_shallowmodel: 
[[  1   6   9   4]
 [  1 101  34  33]
 [  0  37 222  22]
 [  3  31  44  44]]
01/29/2018 22:10:34 [INFO] exp_shallowmodel: ******************** dstc3 - Round 27 
01/29/2018 22:10:34 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:10:34 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:10:34 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:10:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:10:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:10:34 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:10:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:11:46 [INFO] exp_shallowmodel: train time: 72.107s
01/29/2018 22:11:46 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:11:46 [INFO] exp_shallowmodel: accuracy:   0.603
01/29/2018 22:11:46 [INFO] exp_shallowmodel: f1_score:   0.467
01/29/2018 22:11:46 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:11:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.15      0.19        20
          C       0.53      0.57      0.55       169
          F       0.76      0.76      0.76       281
          R       0.37      0.35      0.36       122

avg / total       0.60      0.60      0.60       592

01/29/2018 22:11:46 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:11:46 [INFO] exp_shallowmodel: 
[[  3   6   7   4]
 [  3  97  28  41]
 [  2  37 214  28]
 [  3  42  34  43]]
01/29/2018 22:11:48 [INFO] exp_shallowmodel: ******************** dstc3 - Round 28 
01/29/2018 22:11:48 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:11:48 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:11:48 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:11:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:11:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:11:48 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:11:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:13:07 [INFO] exp_shallowmodel: train time: 78.830s
01/29/2018 22:13:07 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:13:07 [INFO] exp_shallowmodel: accuracy:   0.635
01/29/2018 22:13:07 [INFO] exp_shallowmodel: f1_score:   0.485
01/29/2018 22:13:07 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:13:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.10      0.15        20
          C       0.60      0.63      0.61       169
          F       0.76      0.78      0.77       281
          R       0.41      0.40      0.40       122

avg / total       0.63      0.64      0.63       592

01/29/2018 22:13:07 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:13:07 [INFO] exp_shallowmodel: 
[[  2   1  10   7]
 [  0 106  26  37]
 [  0  35 219  27]
 [  4  35  34  49]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 22:13:10 [INFO] exp_shallowmodel: ******************** dstc3 - Round 29 
01/29/2018 22:13:10 [INFO] exp_shallowmodel: #(data) = 4736
01/29/2018 22:13:10 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:13:10 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:13:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:13:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:13:10 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:13:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:14:24 [INFO] exp_shallowmodel: train time: 73.578s
01/29/2018 22:14:24 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:14:24 [INFO] exp_shallowmodel: accuracy:   0.597
01/29/2018 22:14:24 [INFO] exp_shallowmodel: f1_score:   0.431
01/29/2018 22:14:24 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:14:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.04      0.06        28
          C       0.54      0.54      0.54       172
          F       0.71      0.79      0.75       283
          R       0.39      0.37      0.38       123

avg / total       0.57      0.60      0.58       606

01/29/2018 22:14:24 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:14:24 [INFO] exp_shallowmodel: 
[[  1   8  15   4]
 [  1  93  38  40]
 [  2  31 223  27]
 [  2  40  36  45]]
01/29/2018 22:14:26 [INFO] exp_shallowmodel: ******************** dstc3 - Round 30 
01/29/2018 22:14:26 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:14:26 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:14:26 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:14:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:14:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:14:26 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:14:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:15:44 [INFO] exp_shallowmodel: train time: 77.445s
01/29/2018 22:15:44 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:15:44 [INFO] exp_shallowmodel: accuracy:   0.617
01/29/2018 22:15:44 [INFO] exp_shallowmodel: f1_score:   0.500
01/29/2018 22:15:44 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:15:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.20      0.29        20
          C       0.56      0.61      0.58       169
          F       0.71      0.77      0.74       281
          R       0.44      0.35      0.39       122

avg / total       0.60      0.62      0.61       592

01/29/2018 22:15:44 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:15:44 [INFO] exp_shallowmodel: 
[[  4   5   9   2]
 [  2 103  34  30]
 [  1  43 215  22]
 [  1  33  45  43]]
01/29/2018 22:15:46 [INFO] exp_shallowmodel: ******************** dstc3 - Round 31 
01/29/2018 22:15:46 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:15:46 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:15:46 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:15:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:15:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:15:46 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:15:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:17:03 [INFO] exp_shallowmodel: train time: 76.505s
01/29/2018 22:17:03 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:17:03 [INFO] exp_shallowmodel: accuracy:   0.605
01/29/2018 22:17:03 [INFO] exp_shallowmodel: f1_score:   0.468
01/29/2018 22:17:03 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:17:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.67      0.10      0.17        20
          C       0.57      0.60      0.58       169
          F       0.74      0.74      0.74       281
          R       0.37      0.39      0.38       122

avg / total       0.61      0.60      0.60       592

01/29/2018 22:17:03 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:17:03 [INFO] exp_shallowmodel: 
[[  2   3   6   9]
 [  0 102  29  38]
 [  0  40 207  34]
 [  1  35  39  47]]
01/29/2018 22:17:05 [INFO] exp_shallowmodel: ******************** dstc3 - Round 32 
01/29/2018 22:17:05 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:17:05 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:17:05 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:17:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:17:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:17:05 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:17:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:18:25 [INFO] exp_shallowmodel: train time: 79.861s
01/29/2018 22:18:25 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:18:25 [INFO] exp_shallowmodel: accuracy:   0.611
01/29/2018 22:18:25 [INFO] exp_shallowmodel: f1_score:   0.451
01/29/2018 22:18:25 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:18:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.05      0.07        20
          C       0.57      0.60      0.58       169
          F       0.73      0.75      0.74       281
          R       0.42      0.40      0.41       122

avg / total       0.60      0.61      0.60       592

01/29/2018 22:18:25 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:18:25 [INFO] exp_shallowmodel: 
[[  1   3  10   6]
 [  1 101  35  32]
 [  3  37 211  30]
 [  2  36  35  49]]
01/29/2018 22:18:28 [INFO] exp_shallowmodel: ******************** dstc3 - Round 33 
01/29/2018 22:18:28 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:18:28 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:18:28 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:18:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:18:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:18:28 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:18:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:19:39 [INFO] exp_shallowmodel: train time: 71.615s
01/29/2018 22:19:39 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:19:39 [INFO] exp_shallowmodel: accuracy:   0.593
01/29/2018 22:19:39 [INFO] exp_shallowmodel: f1_score:   0.430
01/29/2018 22:19:39 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:19:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.05      0.09        20
          C       0.54      0.58      0.56       169
          F       0.71      0.76      0.74       281
          R       0.36      0.32      0.34       122

avg / total       0.58      0.59      0.58       592

01/29/2018 22:19:39 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:19:39 [INFO] exp_shallowmodel: 
[[  1   5  11   3]
 [  0  98  33  38]
 [  0  40 213  28]
 [  2  40  41  39]]
01/29/2018 22:19:42 [INFO] exp_shallowmodel: ******************** dstc3 - Round 34 
01/29/2018 22:19:42 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:19:42 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:19:42 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:19:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:19:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:19:42 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:19:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:20:58 [INFO] exp_shallowmodel: train time: 75.427s
01/29/2018 22:20:58 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:20:58 [INFO] exp_shallowmodel: accuracy:   0.581
01/29/2018 22:20:58 [INFO] exp_shallowmodel: f1_score:   0.405
01/29/2018 22:20:58 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:20:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.54      0.59      0.56       169
          F       0.70      0.73      0.72       281
          R       0.35      0.33      0.34       122

avg / total       0.56      0.58      0.57       592

01/29/2018 22:20:58 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:20:58 [INFO] exp_shallowmodel: 
[[  0   4  13   3]
 [  1  99  31  38]
 [  2  42 205  32]
 [  3  37  42  40]]
01/29/2018 22:21:00 [INFO] exp_shallowmodel: ******************** dstc3 - Round 35 
01/29/2018 22:21:00 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:21:00 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:21:00 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:21:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:21:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:21:00 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:21:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:22:18 [INFO] exp_shallowmodel: train time: 77.457s
01/29/2018 22:22:18 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:22:18 [INFO] exp_shallowmodel: accuracy:   0.610
01/29/2018 22:22:18 [INFO] exp_shallowmodel: f1_score:   0.473
01/29/2018 22:22:18 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:22:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.15      0.21        20
          C       0.54      0.54      0.54       169
          F       0.74      0.79      0.77       281
          R       0.39      0.36      0.37       122

avg / total       0.60      0.61      0.60       592

01/29/2018 22:22:18 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:22:18 [INFO] exp_shallowmodel: 
[[  3   2  11   4]
 [  1  91  32  45]
 [  2  35 223  21]
 [  2  40  36  44]]
01/29/2018 22:22:20 [INFO] exp_shallowmodel: ******************** dstc3 - Round 36 
01/29/2018 22:22:20 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:22:20 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:22:20 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:22:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:22:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:22:20 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:22:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:23:42 [INFO] exp_shallowmodel: train time: 82.001s
01/29/2018 22:23:42 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:23:42 [INFO] exp_shallowmodel: accuracy:   0.617
01/29/2018 22:23:42 [INFO] exp_shallowmodel: f1_score:   0.432
01/29/2018 22:23:42 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:23:42 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.60      0.57       169
          F       0.73      0.77      0.75       281
          R       0.46      0.38      0.41       122

avg / total       0.59      0.62      0.60       592

01/29/2018 22:23:42 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:23:42 [INFO] exp_shallowmodel: 
[[  0   8   8   4]
 [  1 102  37  29]
 [  0  42 217  22]
 [  2  40  34  46]]
01/29/2018 22:23:45 [INFO] exp_shallowmodel: ******************** dstc3 - Round 37 
01/29/2018 22:23:45 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:23:45 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:23:45 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:23:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:23:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:23:45 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:23:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:25:02 [INFO] exp_shallowmodel: train time: 77.531s
01/29/2018 22:25:02 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:25:02 [INFO] exp_shallowmodel: accuracy:   0.610
01/29/2018 22:25:02 [INFO] exp_shallowmodel: f1_score:   0.475
01/29/2018 22:25:02 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:25:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.15      0.21        20
          C       0.55      0.63      0.59       169
          F       0.75      0.75      0.75       281
          R       0.37      0.33      0.35       122

avg / total       0.60      0.61      0.60       592

01/29/2018 22:25:02 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:25:02 [INFO] exp_shallowmodel: 
[[  3   1   8   8]
 [  1 107  29  32]
 [  1  41 211  28]
 [  3  44  35  40]]
01/29/2018 22:25:05 [INFO] exp_shallowmodel: ******************** dstc3 - Round 38 
01/29/2018 22:25:05 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:25:05 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:25:05 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:25:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:25:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:25:05 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:25:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:26:17 [INFO] exp_shallowmodel: train time: 72.118s
01/29/2018 22:26:17 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:26:17 [INFO] exp_shallowmodel: accuracy:   0.615
01/29/2018 22:26:17 [INFO] exp_shallowmodel: f1_score:   0.473
01/29/2018 22:26:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:26:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.15      0.22        20
          C       0.55      0.59      0.57       169
          F       0.74      0.80      0.77       281
          R       0.37      0.30      0.33       122

avg / total       0.60      0.61      0.60       592

01/29/2018 22:26:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:26:17 [INFO] exp_shallowmodel: 
[[  3   5  11   1]
 [  1 100  33  35]
 [  1  30 224  26]
 [  2  48  35  37]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 22:26:20 [INFO] exp_shallowmodel: ******************** dstc3 - Round 39 
01/29/2018 22:26:20 [INFO] exp_shallowmodel: #(data) = 4736
01/29/2018 22:26:20 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:26:20 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:26:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:26:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:26:20 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:26:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:27:33 [INFO] exp_shallowmodel: train time: 72.942s
01/29/2018 22:27:33 [INFO] exp_shallowmodel: test time:  0.012s
01/29/2018 22:27:33 [INFO] exp_shallowmodel: accuracy:   0.597
01/29/2018 22:27:33 [INFO] exp_shallowmodel: f1_score:   0.436
01/29/2018 22:27:33 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:27:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.06        28
          C       0.55      0.57      0.56       172
          F       0.70      0.77      0.73       283
          R       0.40      0.37      0.39       123

avg / total       0.58      0.60      0.58       606

01/29/2018 22:27:33 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:27:33 [INFO] exp_shallowmodel: 
[[  1   3  16   8]
 [  0  98  42  32]
 [  2  36 217  28]
 [  1  42  34  46]]
01/29/2018 22:27:35 [INFO] exp_shallowmodel: ******************** dstc3 - Round 40 
01/29/2018 22:27:35 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:27:35 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:27:35 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:27:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:27:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:27:35 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:27:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:28:56 [INFO] exp_shallowmodel: train time: 80.825s
01/29/2018 22:28:56 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:28:56 [INFO] exp_shallowmodel: accuracy:   0.635
01/29/2018 22:28:56 [INFO] exp_shallowmodel: f1_score:   0.482
01/29/2018 22:28:56 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:28:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.10      0.15        20
          C       0.57      0.56      0.56       169
          F       0.77      0.81      0.79       281
          R       0.42      0.43      0.42       122

avg / total       0.62      0.64      0.63       592

01/29/2018 22:28:56 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:28:56 [INFO] exp_shallowmodel: 
[[  2   2  11   5]
 [  1  94  31  43]
 [  1  28 228  24]
 [  2  41  27  52]]
01/29/2018 22:28:59 [INFO] exp_shallowmodel: ******************** dstc3 - Round 41 
01/29/2018 22:28:59 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:28:59 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:28:59 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:28:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:28:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:28:59 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:28:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:30:14 [INFO] exp_shallowmodel: train time: 75.018s
01/29/2018 22:30:14 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:30:14 [INFO] exp_shallowmodel: accuracy:   0.608
01/29/2018 22:30:14 [INFO] exp_shallowmodel: f1_score:   0.441
01/29/2018 22:30:14 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:30:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.05      0.08        20
          C       0.54      0.62      0.57       169
          F       0.75      0.77      0.76       281
          R       0.38      0.33      0.35       122

avg / total       0.59      0.61      0.60       592

01/29/2018 22:30:14 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:30:14 [INFO] exp_shallowmodel: 
[[  1   4  10   5]
 [  0 104  31  34]
 [  1  38 215  27]
 [  2  48  32  40]]
01/29/2018 22:30:16 [INFO] exp_shallowmodel: ******************** dstc3 - Round 42 
01/29/2018 22:30:16 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:30:16 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:30:16 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:30:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:30:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:30:16 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:30:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:31:33 [INFO] exp_shallowmodel: train time: 76.137s
01/29/2018 22:31:33 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:31:33 [INFO] exp_shallowmodel: accuracy:   0.600
01/29/2018 22:31:33 [INFO] exp_shallowmodel: f1_score:   0.463
01/29/2018 22:31:33 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:31:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.15      0.22        20
          C       0.50      0.53      0.52       169
          F       0.73      0.79      0.76       281
          R       0.40      0.32      0.35       122

avg / total       0.58      0.60      0.59       592

01/29/2018 22:31:33 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:31:33 [INFO] exp_shallowmodel: 
[[  3   5   9   3]
 [  3  90  41  35]
 [  0  37 223  21]
 [  1  48  34  39]]
01/29/2018 22:31:35 [INFO] exp_shallowmodel: ******************** dstc3 - Round 43 
01/29/2018 22:31:35 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:31:35 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:31:35 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:31:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:31:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:31:35 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:31:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:32:50 [INFO] exp_shallowmodel: train time: 75.095s
01/29/2018 22:32:50 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:32:50 [INFO] exp_shallowmodel: accuracy:   0.637
01/29/2018 22:32:50 [INFO] exp_shallowmodel: f1_score:   0.492
01/29/2018 22:32:50 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:32:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.67      0.10      0.17        20
          C       0.60      0.63      0.61       169
          F       0.75      0.78      0.76       281
          R       0.42      0.41      0.41       122

avg / total       0.64      0.64      0.63       592

01/29/2018 22:32:50 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:32:50 [INFO] exp_shallowmodel: 
[[  2   1  14   3]
 [  0 107  25  37]
 [  0  34 218  29]
 [  1  37  34  50]]
01/29/2018 22:32:53 [INFO] exp_shallowmodel: ******************** dstc3 - Round 44 
01/29/2018 22:32:53 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:32:53 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:32:53 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:32:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:32:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:32:53 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:32:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:34:16 [INFO] exp_shallowmodel: train time: 82.808s
01/29/2018 22:34:16 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:34:16 [INFO] exp_shallowmodel: accuracy:   0.596
01/29/2018 22:34:16 [INFO] exp_shallowmodel: f1_score:   0.452
01/29/2018 22:34:16 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:34:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.52      0.56      0.54       169
          F       0.72      0.75      0.74       281
          R       0.41      0.36      0.38       122

avg / total       0.58      0.60      0.59       592

01/29/2018 22:34:16 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:34:16 [INFO] exp_shallowmodel: 
[[  2   3  11   4]
 [  2  95  36  36]
 [  1  45 212  23]
 [  2  40  36  44]]
01/29/2018 22:34:18 [INFO] exp_shallowmodel: ******************** dstc3 - Round 45 
01/29/2018 22:34:18 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:34:18 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:34:18 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:34:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:34:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:34:18 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:34:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:35:29 [INFO] exp_shallowmodel: train time: 71.294s
01/29/2018 22:35:29 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:35:29 [INFO] exp_shallowmodel: accuracy:   0.611
01/29/2018 22:35:29 [INFO] exp_shallowmodel: f1_score:   0.425
01/29/2018 22:35:29 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:35:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.56      0.59      0.57       169
          F       0.73      0.78      0.76       281
          R       0.39      0.35      0.37       122

avg / total       0.59      0.61      0.60       592

01/29/2018 22:35:29 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:35:29 [INFO] exp_shallowmodel: 
[[  0   1  14   5]
 [  1  99  28  41]
 [  0  39 220  22]
 [  4  37  38  43]]
01/29/2018 22:35:32 [INFO] exp_shallowmodel: ******************** dstc3 - Round 46 
01/29/2018 22:35:32 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:35:32 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:35:32 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:35:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:35:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:35:32 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:35:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:36:44 [INFO] exp_shallowmodel: train time: 71.452s
01/29/2018 22:36:44 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:36:44 [INFO] exp_shallowmodel: accuracy:   0.600
01/29/2018 22:36:44 [INFO] exp_shallowmodel: f1_score:   0.450
01/29/2018 22:36:44 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:36:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.10      0.14        20
          C       0.57      0.58      0.57       169
          F       0.70      0.77      0.73       281
          R       0.38      0.33      0.35       122

avg / total       0.58      0.60      0.59       592

01/29/2018 22:36:44 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:36:44 [INFO] exp_shallowmodel: 
[[  2   3  10   5]
 [  2  98  37  32]
 [  1  37 215  28]
 [  3  34  45  40]]
01/29/2018 22:36:46 [INFO] exp_shallowmodel: ******************** dstc3 - Round 47 
01/29/2018 22:36:46 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:36:46 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:36:46 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:36:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:36:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:36:46 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:36:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:38:04 [INFO] exp_shallowmodel: train time: 77.516s
01/29/2018 22:38:04 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:38:04 [INFO] exp_shallowmodel: accuracy:   0.605
01/29/2018 22:38:04 [INFO] exp_shallowmodel: f1_score:   0.435
01/29/2018 22:38:04 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:38:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.05      0.08        20
          C       0.57      0.60      0.58       169
          F       0.73      0.78      0.75       281
          R       0.34      0.31      0.33       122

avg / total       0.59      0.60      0.59       592

01/29/2018 22:38:04 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:38:04 [INFO] exp_shallowmodel: 
[[  1   2   8   9]
 [  1 101  29  38]
 [  0  37 218  26]
 [  4  37  43  38]]
01/29/2018 22:38:06 [INFO] exp_shallowmodel: ******************** dstc3 - Round 48 
01/29/2018 22:38:06 [INFO] exp_shallowmodel: #(data) = 4750
01/29/2018 22:38:06 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:38:06 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:38:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:38:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:38:06 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:38:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:39:18 [INFO] exp_shallowmodel: train time: 71.992s
01/29/2018 22:39:18 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:39:18 [INFO] exp_shallowmodel: accuracy:   0.627
01/29/2018 22:39:18 [INFO] exp_shallowmodel: f1_score:   0.438
01/29/2018 22:39:18 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:39:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.58      0.61      0.59       169
          F       0.73      0.79      0.76       281
          R       0.44      0.37      0.40       122

avg / total       0.60      0.63      0.61       592

01/29/2018 22:39:18 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:39:18 [INFO] exp_shallowmodel: 
[[  0   7  10   3]
 [  2 103  35  29]
 [  3  30 223  25]
 [  0  39  38  45]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 22:39:21 [INFO] exp_shallowmodel: ******************** dstc3 - Round 49 
01/29/2018 22:39:21 [INFO] exp_shallowmodel: #(data) = 4736
01/29/2018 22:39:21 [INFO] exp_shallowmodel: #(feature) = 12446
01/29/2018 22:39:21 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:39:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:39:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:39:21 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:39:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:40:38 [INFO] exp_shallowmodel: train time: 76.848s
01/29/2018 22:40:38 [INFO] exp_shallowmodel: test time:  0.011s
01/29/2018 22:40:38 [INFO] exp_shallowmodel: accuracy:   0.566
01/29/2018 22:40:38 [INFO] exp_shallowmodel: f1_score:   0.391
01/29/2018 22:40:38 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:40:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        28
          C       0.53      0.53      0.53       172
          F       0.69      0.75      0.72       283
          R       0.31      0.32      0.31       123

avg / total       0.54      0.57      0.55       606

01/29/2018 22:40:38 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:40:38 [INFO] exp_shallowmodel: 
[[  0   7  11  10]
 [  0  91  36  45]
 [  0  38 213  32]
 [  0  35  49  39]]
01/29/2018 22:40:51 [INFO] exp_shallowmodel: ******************** family - Round 0 
01/29/2018 22:40:51 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:40:51 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:40:51 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:40:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:40:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:40:51 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:40:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:41:59 [INFO] exp_shallowmodel: train time: 67.819s
01/29/2018 22:41:59 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:41:59 [INFO] exp_shallowmodel: accuracy:   0.733
01/29/2018 22:41:59 [INFO] exp_shallowmodel: f1_score:   0.392
01/29/2018 22:41:59 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:41:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.09      0.13        23
          C       0.25      0.15      0.19        27
          F       0.78      0.95      0.85       250
          R       0.62      0.29      0.39        52

avg / total       0.68      0.73      0.69       352

01/29/2018 22:41:59 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:41:59 [INFO] exp_shallowmodel: 
[[  2   0  20   1]
 [  0   4  19   4]
 [  5   4 237   4]
 [  0   8  29  15]]
01/29/2018 22:42:04 [INFO] exp_shallowmodel: ******************** family - Round 1 
01/29/2018 22:42:04 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:42:04 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:42:04 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:42:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:42:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:42:04 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:42:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:43:12 [INFO] exp_shallowmodel: train time: 67.500s
01/29/2018 22:43:12 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:43:12 [INFO] exp_shallowmodel: accuracy:   0.750
01/29/2018 22:43:12 [INFO] exp_shallowmodel: f1_score:   0.418
01/29/2018 22:43:12 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:43:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.55      0.22      0.32        27
          F       0.77      0.96      0.86       250
          R       0.59      0.33      0.42        52

avg / total       0.71      0.75      0.70       352

01/29/2018 22:43:12 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:43:12 [INFO] exp_shallowmodel: 
[[  1   1  19   2]
 [  0   6  18   3]
 [  1   2 240   7]
 [  0   2  33  17]]
01/29/2018 22:43:17 [INFO] exp_shallowmodel: ******************** family - Round 2 
01/29/2018 22:43:17 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:43:17 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:43:17 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:43:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:43:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:43:17 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:43:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:44:23 [INFO] exp_shallowmodel: train time: 66.292s
01/29/2018 22:44:23 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:44:23 [INFO] exp_shallowmodel: accuracy:   0.759
01/29/2018 22:44:23 [INFO] exp_shallowmodel: f1_score:   0.436
01/29/2018 22:44:23 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:44:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.13      0.21        23
          C       0.38      0.19      0.25        27
          F       0.78      0.98      0.87       250
          R       0.79      0.29      0.42        52

avg / total       0.73      0.76      0.71       352

01/29/2018 22:44:23 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:44:23 [INFO] exp_shallowmodel: 
[[  3   0  20   0]
 [  1   5  18   3]
 [  0   5 244   1]
 [  2   3  32  15]]
01/29/2018 22:44:28 [INFO] exp_shallowmodel: ******************** family - Round 3 
01/29/2018 22:44:28 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:44:28 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:44:28 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:44:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:44:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:44:28 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:44:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:45:38 [INFO] exp_shallowmodel: train time: 69.399s
01/29/2018 22:45:38 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 22:45:38 [INFO] exp_shallowmodel: accuracy:   0.730
01/29/2018 22:45:38 [INFO] exp_shallowmodel: f1_score:   0.369
01/29/2018 22:45:38 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:45:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.60      0.22      0.32        27
          F       0.76      0.96      0.85       250
          R       0.44      0.23      0.30        52

avg / total       0.65      0.73      0.67       352

01/29/2018 22:45:38 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:45:38 [INFO] exp_shallowmodel: 
[[  0   0  20   3]
 [  0   6  18   3]
 [  1   1 239   9]
 [  1   3  36  12]]
01/29/2018 22:45:43 [INFO] exp_shallowmodel: ******************** family - Round 4 
01/29/2018 22:45:43 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:45:43 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:45:43 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:45:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:45:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:45:43 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:45:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:46:52 [INFO] exp_shallowmodel: train time: 68.738s
01/29/2018 22:46:52 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:46:52 [INFO] exp_shallowmodel: accuracy:   0.744
01/29/2018 22:46:52 [INFO] exp_shallowmodel: f1_score:   0.419
01/29/2018 22:46:52 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:46:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.09      0.14        23
          C       0.40      0.15      0.22        27
          F       0.78      0.94      0.86       250
          R       0.59      0.38      0.47        52

avg / total       0.69      0.74      0.70       352

01/29/2018 22:46:52 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:46:52 [INFO] exp_shallowmodel: 
[[  2   1  16   4]
 [  0   4  21   2]
 [  3   3 236   8]
 [  1   2  29  20]]
01/29/2018 22:46:57 [INFO] exp_shallowmodel: ******************** family - Round 5 
01/29/2018 22:46:57 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:46:57 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:46:57 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:46:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:46:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:46:57 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:46:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:48:06 [INFO] exp_shallowmodel: train time: 69.010s
01/29/2018 22:48:06 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 22:48:06 [INFO] exp_shallowmodel: accuracy:   0.759
01/29/2018 22:48:06 [INFO] exp_shallowmodel: f1_score:   0.420
01/29/2018 22:48:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:48:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.75      0.22      0.34        27
          F       0.79      0.96      0.87       250
          R       0.61      0.38      0.47        52

avg / total       0.70      0.76      0.71       352

01/29/2018 22:48:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:48:06 [INFO] exp_shallowmodel: 
[[  0   1  22   0]
 [  1   6  13   7]
 [  3   0 241   6]
 [  0   1  31  20]]
01/29/2018 22:48:11 [INFO] exp_shallowmodel: ******************** family - Round 6 
01/29/2018 22:48:11 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:48:11 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:48:11 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:48:11 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:48:11 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:48:11 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:48:11 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:49:20 [INFO] exp_shallowmodel: train time: 68.719s
01/29/2018 22:49:20 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:49:20 [INFO] exp_shallowmodel: accuracy:   0.730
01/29/2018 22:49:20 [INFO] exp_shallowmodel: f1_score:   0.388
01/29/2018 22:49:20 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:49:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.04      0.07        23
          C       0.50      0.26      0.34        27
          F       0.78      0.95      0.85       250
          R       0.44      0.21      0.29        52

avg / total       0.66      0.73      0.68       352

01/29/2018 22:49:20 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:49:20 [INFO] exp_shallowmodel: 
[[  1   1  17   4]
 [  0   7  15   5]
 [  3   4 238   5]
 [  2   2  37  11]]
01/29/2018 22:49:26 [INFO] exp_shallowmodel: ******************** family - Round 7 
01/29/2018 22:49:26 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:49:26 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:49:26 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:49:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:49:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:49:26 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:49:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:50:31 [INFO] exp_shallowmodel: train time: 65.464s
01/29/2018 22:50:31 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 22:50:31 [INFO] exp_shallowmodel: accuracy:   0.727
01/29/2018 22:50:31 [INFO] exp_shallowmodel: f1_score:   0.392
01/29/2018 22:50:31 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:50:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.75      0.13      0.22        23
          C       0.44      0.15      0.22        27
          F       0.77      0.95      0.85       250
          R       0.39      0.21      0.28        52

avg / total       0.68      0.73      0.67       352

01/29/2018 22:50:31 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:50:31 [INFO] exp_shallowmodel: 
[[  3   0  17   3]
 [  1   4  18   4]
 [  0   2 238  10]
 [  0   3  38  11]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 22:50:36 [INFO] exp_shallowmodel: ******************** family - Round 8 
01/29/2018 22:50:36 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:50:36 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:50:36 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:50:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:50:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:50:36 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:50:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:51:44 [INFO] exp_shallowmodel: train time: 67.588s
01/29/2018 22:51:44 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 22:51:44 [INFO] exp_shallowmodel: accuracy:   0.739
01/29/2018 22:51:44 [INFO] exp_shallowmodel: f1_score:   0.380
01/29/2018 22:51:44 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:51:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.54      0.26      0.35        27
          F       0.79      0.96      0.87       250
          R       0.44      0.23      0.30        52

avg / total       0.66      0.74      0.69       352

01/29/2018 22:51:44 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:51:44 [INFO] exp_shallowmodel: 
[[  0   0  21   2]
 [  0   7  14   6]
 [  1   1 241   7]
 [  4   5  31  12]]
01/29/2018 22:51:49 [INFO] exp_shallowmodel: ******************** family - Round 9 
01/29/2018 22:51:49 [INFO] exp_shallowmodel: #(data) = 2816
01/29/2018 22:51:49 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:51:49 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:51:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:51:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:51:49 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:51:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:52:58 [INFO] exp_shallowmodel: train time: 68.259s
01/29/2018 22:52:58 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:52:58 [INFO] exp_shallowmodel: accuracy:   0.704
01/29/2018 22:52:58 [INFO] exp_shallowmodel: f1_score:   0.321
01/29/2018 22:52:58 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:52:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        25
          C       0.33      0.11      0.17        27
          F       0.75      0.96      0.84       251
          R       0.44      0.20      0.28        59

avg / total       0.61      0.70      0.64       362

01/29/2018 22:52:58 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:52:58 [INFO] exp_shallowmodel: 
[[  0   1  21   3]
 [  1   3  19   4]
 [  2   1 240   8]
 [  1   4  42  12]]
01/29/2018 22:53:03 [INFO] exp_shallowmodel: ******************** family - Round 10 
01/29/2018 22:53:03 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:53:03 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:53:03 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:53:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:53:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:53:03 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:53:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:54:10 [INFO] exp_shallowmodel: train time: 66.633s
01/29/2018 22:54:10 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 22:54:10 [INFO] exp_shallowmodel: accuracy:   0.759
01/29/2018 22:54:10 [INFO] exp_shallowmodel: f1_score:   0.410
01/29/2018 22:54:10 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:54:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.55      0.22      0.32        27
          F       0.79      0.97      0.87       250
          R       0.67      0.35      0.46        52

avg / total       0.70      0.76      0.71       352

01/29/2018 22:54:10 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:54:10 [INFO] exp_shallowmodel: 
[[  0   2  20   1]
 [  0   6  17   4]
 [  2   1 243   4]
 [  3   2  29  18]]
01/29/2018 22:54:15 [INFO] exp_shallowmodel: ******************** family - Round 11 
01/29/2018 22:54:15 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:54:15 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:54:15 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:54:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:54:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:54:15 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:54:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:55:23 [INFO] exp_shallowmodel: train time: 67.416s
01/29/2018 22:55:23 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 22:55:23 [INFO] exp_shallowmodel: accuracy:   0.719
01/29/2018 22:55:23 [INFO] exp_shallowmodel: f1_score:   0.345
01/29/2018 22:55:23 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:55:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.07        23
          C       0.27      0.15      0.19        27
          F       0.77      0.95      0.85       250
          R       0.42      0.19      0.26        52

avg / total       0.65      0.72      0.66       352

01/29/2018 22:55:23 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:55:23 [INFO] exp_shallowmodel: 
[[  1   2  18   2]
 [  1   4  17   5]
 [  2   3 238   7]
 [  0   6  36  10]]
01/29/2018 22:55:28 [INFO] exp_shallowmodel: ******************** family - Round 12 
01/29/2018 22:55:28 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:55:28 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:55:28 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:55:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:55:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:55:28 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:55:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:56:34 [INFO] exp_shallowmodel: train time: 66.069s
01/29/2018 22:56:34 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 22:56:34 [INFO] exp_shallowmodel: accuracy:   0.730
01/29/2018 22:56:34 [INFO] exp_shallowmodel: f1_score:   0.385
01/29/2018 22:56:34 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:56:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.07        23
          C       0.47      0.26      0.33        27
          F       0.78      0.95      0.86       250
          R       0.39      0.21      0.28        52

avg / total       0.66      0.73      0.68       352

01/29/2018 22:56:34 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:56:34 [INFO] exp_shallowmodel: 
[[  1   0  19   3]
 [  0   7  14   6]
 [  1   3 238   8]
 [  2   5  34  11]]
01/29/2018 22:56:39 [INFO] exp_shallowmodel: ******************** family - Round 13 
01/29/2018 22:56:39 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:56:39 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:56:39 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:56:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:56:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:56:39 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:56:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:57:46 [INFO] exp_shallowmodel: train time: 66.720s
01/29/2018 22:57:46 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:57:46 [INFO] exp_shallowmodel: accuracy:   0.759
01/29/2018 22:57:46 [INFO] exp_shallowmodel: f1_score:   0.444
01/29/2018 22:57:46 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:57:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.13      0.21        23
          C       0.47      0.26      0.33        27
          F       0.79      0.97      0.87       250
          R       0.56      0.27      0.36        52

avg / total       0.72      0.76      0.71       352

01/29/2018 22:57:46 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:57:46 [INFO] exp_shallowmodel: 
[[  3   1  16   3]
 [  0   7  15   5]
 [  1   3 243   3]
 [  2   4  32  14]]
01/29/2018 22:57:52 [INFO] exp_shallowmodel: ******************** family - Round 14 
01/29/2018 22:57:52 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:57:52 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:57:52 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:57:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:57:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:57:52 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:57:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 22:59:00 [INFO] exp_shallowmodel: train time: 68.917s
01/29/2018 22:59:00 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 22:59:00 [INFO] exp_shallowmodel: accuracy:   0.753
01/29/2018 22:59:00 [INFO] exp_shallowmodel: f1_score:   0.365
01/29/2018 22:59:00 [INFO] exp_shallowmodel: classification report:
01/29/2018 22:59:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.33      0.15      0.21        27
          F       0.79      0.99      0.88       250
          R       0.64      0.27      0.38        52

avg / total       0.68      0.75      0.69       352

01/29/2018 22:59:00 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 22:59:01 [INFO] exp_shallowmodel: 
[[  0   2  17   4]
 [  1   4  19   3]
 [  1   1 247   1]
 [  2   5  31  14]]
01/29/2018 22:59:06 [INFO] exp_shallowmodel: ******************** family - Round 15 
01/29/2018 22:59:06 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 22:59:06 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 22:59:06 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 22:59:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 22:59:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 22:59:06 [INFO] exp_shallowmodel: Training: 
01/29/2018 22:59:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:00:15 [INFO] exp_shallowmodel: train time: 69.342s
01/29/2018 23:00:15 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:00:15 [INFO] exp_shallowmodel: accuracy:   0.724
01/29/2018 23:00:15 [INFO] exp_shallowmodel: f1_score:   0.341
01/29/2018 23:00:15 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:00:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.36      0.19      0.24        27
          F       0.77      0.96      0.85       250
          R       0.43      0.19      0.27        52

avg / total       0.64      0.72      0.66       352

01/29/2018 23:00:15 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:00:15 [INFO] exp_shallowmodel: 
[[  0   0  19   4]
 [  0   5  18   4]
 [  3   2 240   5]
 [  0   7  35  10]]
01/29/2018 23:00:20 [INFO] exp_shallowmodel: ******************** family - Round 16 
01/29/2018 23:00:20 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:00:20 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:00:20 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:00:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:00:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:00:20 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:00:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:01:28 [INFO] exp_shallowmodel: train time: 68.102s
01/29/2018 23:01:28 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:01:28 [INFO] exp_shallowmodel: accuracy:   0.724
01/29/2018 23:01:28 [INFO] exp_shallowmodel: f1_score:   0.361
01/29/2018 23:01:28 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:01:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.50      0.19      0.27        27
          F       0.76      0.95      0.85       250
          R       0.48      0.25      0.33        52

avg / total       0.65      0.72      0.67       352

01/29/2018 23:01:28 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:01:28 [INFO] exp_shallowmodel: 
[[  0   1  18   4]
 [  1   5  19   2]
 [  2   3 237   8]
 [  2   1  36  13]]
01/29/2018 23:01:34 [INFO] exp_shallowmodel: ******************** family - Round 17 
01/29/2018 23:01:34 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:01:34 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:01:34 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:01:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:01:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:01:34 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:01:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:02:42 [INFO] exp_shallowmodel: train time: 68.521s
01/29/2018 23:02:42 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:02:42 [INFO] exp_shallowmodel: accuracy:   0.741
01/29/2018 23:02:42 [INFO] exp_shallowmodel: f1_score:   0.410
01/29/2018 23:02:42 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:02:42 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.55      0.22      0.32        27
          F       0.77      0.95      0.85       250
          R       0.55      0.31      0.40        52

avg / total       0.70      0.74      0.69       352

01/29/2018 23:02:42 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:02:42 [INFO] exp_shallowmodel: 
[[  1   0  21   1]
 [  0   6  16   5]
 [  1   4 238   7]
 [  0   1  35  16]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 23:02:47 [INFO] exp_shallowmodel: ******************** family - Round 18 
01/29/2018 23:02:47 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:02:47 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:02:47 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:02:47 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:02:47 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:02:47 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:02:47 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:03:56 [INFO] exp_shallowmodel: train time: 68.541s
01/29/2018 23:03:56 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:03:56 [INFO] exp_shallowmodel: accuracy:   0.727
01/29/2018 23:03:56 [INFO] exp_shallowmodel: f1_score:   0.370
01/29/2018 23:03:56 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:03:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.07        23
          C       0.50      0.15      0.23        27
          F       0.78      0.95      0.85       250
          R       0.40      0.27      0.32        52

avg / total       0.67      0.73      0.68       352

01/29/2018 23:03:56 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:03:56 [INFO] exp_shallowmodel: 
[[  1   0  18   4]
 [  1   4  16   6]
 [  0   2 237  11]
 [  2   2  34  14]]
01/29/2018 23:04:01 [INFO] exp_shallowmodel: ******************** family - Round 19 
01/29/2018 23:04:01 [INFO] exp_shallowmodel: #(data) = 2816
01/29/2018 23:04:01 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:04:01 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:04:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:04:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:04:01 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:04:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:05:11 [INFO] exp_shallowmodel: train time: 69.235s
01/29/2018 23:05:11 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:05:11 [INFO] exp_shallowmodel: accuracy:   0.715
01/29/2018 23:05:11 [INFO] exp_shallowmodel: f1_score:   0.391
01/29/2018 23:05:11 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:05:11 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.12      0.18        25
          C       0.62      0.19      0.29        27
          F       0.75      0.96      0.84       251
          R       0.39      0.19      0.25        59

avg / total       0.66      0.72      0.66       362

01/29/2018 23:05:11 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:05:11 [INFO] exp_shallowmodel: 
[[  3   0  18   4]
 [  0   5  16   6]
 [  2   2 240   7]
 [  3   1  44  11]]
01/29/2018 23:05:16 [INFO] exp_shallowmodel: ******************** family - Round 20 
01/29/2018 23:05:16 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:05:16 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:05:16 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:05:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:05:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:05:16 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:05:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:06:25 [INFO] exp_shallowmodel: train time: 68.903s
01/29/2018 23:06:25 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:06:25 [INFO] exp_shallowmodel: accuracy:   0.761
01/29/2018 23:06:25 [INFO] exp_shallowmodel: f1_score:   0.408
01/29/2018 23:06:25 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:06:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.45      0.19      0.26        27
          F       0.79      0.97      0.87       250
          R       0.66      0.40      0.50        52

avg / total       0.69      0.76      0.71       352

01/29/2018 23:06:25 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:06:25 [INFO] exp_shallowmodel: 
[[  0   1  18   4]
 [  1   5  18   3]
 [  1   3 242   4]
 [  1   2  28  21]]
01/29/2018 23:06:30 [INFO] exp_shallowmodel: ******************** family - Round 21 
01/29/2018 23:06:30 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:06:30 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:06:30 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:06:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:06:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:06:30 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:06:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:07:37 [INFO] exp_shallowmodel: train time: 66.536s
01/29/2018 23:07:37 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:07:37 [INFO] exp_shallowmodel: accuracy:   0.753
01/29/2018 23:07:37 [INFO] exp_shallowmodel: f1_score:   0.423
01/29/2018 23:07:37 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:07:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        23
          C       0.50      0.26      0.34        27
          F       0.78      0.96      0.86       250
          R       0.64      0.31      0.42        52

avg / total       0.70      0.75      0.71       352

01/29/2018 23:07:37 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:07:37 [INFO] exp_shallowmodel: 
[[  1   1  20   1]
 [  2   7  16   2]
 [  1   2 241   6]
 [  1   4  31  16]]
01/29/2018 23:07:42 [INFO] exp_shallowmodel: ******************** family - Round 22 
01/29/2018 23:07:42 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:07:42 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:07:42 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:07:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:07:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:07:42 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:07:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:08:50 [INFO] exp_shallowmodel: train time: 68.297s
01/29/2018 23:08:50 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:08:50 [INFO] exp_shallowmodel: accuracy:   0.753
01/29/2018 23:08:50 [INFO] exp_shallowmodel: f1_score:   0.409
01/29/2018 23:08:50 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:08:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.09      0.14        23
          C       0.42      0.19      0.26        27
          F       0.79      0.98      0.87       250
          R       0.58      0.27      0.37        52

avg / total       0.70      0.75      0.70       352

01/29/2018 23:08:50 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:08:50 [INFO] exp_shallowmodel: 
[[  2   0  19   2]
 [  0   5  18   4]
 [  1   1 244   4]
 [  3   6  29  14]]
01/29/2018 23:08:56 [INFO] exp_shallowmodel: ******************** family - Round 23 
01/29/2018 23:08:56 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:08:56 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:08:56 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:08:56 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:08:56 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:08:56 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:08:56 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:10:04 [INFO] exp_shallowmodel: train time: 68.123s
01/29/2018 23:10:04 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:10:04 [INFO] exp_shallowmodel: accuracy:   0.750
01/29/2018 23:10:04 [INFO] exp_shallowmodel: f1_score:   0.407
01/29/2018 23:10:04 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:10:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.70      0.26      0.38        27
          F       0.78      0.96      0.86       250
          R       0.52      0.31      0.39        52

avg / total       0.68      0.75      0.70       352

01/29/2018 23:10:04 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:10:04 [INFO] exp_shallowmodel: 
[[  0   0  19   4]
 [  0   7  16   4]
 [  0   2 241   7]
 [  2   1  33  16]]
01/29/2018 23:10:09 [INFO] exp_shallowmodel: ******************** family - Round 24 
01/29/2018 23:10:09 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:10:09 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:10:09 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:10:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:10:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:10:09 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:10:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:11:17 [INFO] exp_shallowmodel: train time: 68.267s
01/29/2018 23:11:17 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:11:17 [INFO] exp_shallowmodel: accuracy:   0.756
01/29/2018 23:11:17 [INFO] exp_shallowmodel: f1_score:   0.447
01/29/2018 23:11:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:11:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.13      0.21        23
          C       0.50      0.26      0.34        27
          F       0.78      0.97      0.87       250
          R       0.61      0.27      0.37        52

avg / total       0.72      0.76      0.71       352

01/29/2018 23:11:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:11:17 [INFO] exp_shallowmodel: 
[[  3   1  18   1]
 [  0   7  17   3]
 [  1   2 242   5]
 [  2   4  32  14]]
01/29/2018 23:11:23 [INFO] exp_shallowmodel: ******************** family - Round 25 
01/29/2018 23:11:23 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:11:23 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:11:23 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:11:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:11:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:11:23 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:11:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:12:28 [INFO] exp_shallowmodel: train time: 64.897s
01/29/2018 23:12:28 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:12:28 [INFO] exp_shallowmodel: accuracy:   0.724
01/29/2018 23:12:28 [INFO] exp_shallowmodel: f1_score:   0.343
01/29/2018 23:12:28 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:12:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.15      0.07      0.10        27
          F       0.76      0.96      0.85       250
          R       0.54      0.25      0.34        52

avg / total       0.67      0.72      0.67       352

01/29/2018 23:12:28 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:12:28 [INFO] exp_shallowmodel: 
[[  1   0  22   0]
 [  0   2  20   5]
 [  1   4 239   6]
 [  0   7  32  13]]
01/29/2018 23:12:33 [INFO] exp_shallowmodel: ******************** family - Round 26 
01/29/2018 23:12:33 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:12:33 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:12:33 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:12:33 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:12:33 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:12:33 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:12:33 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:13:38 [INFO] exp_shallowmodel: train time: 64.730s
01/29/2018 23:13:38 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:13:38 [INFO] exp_shallowmodel: accuracy:   0.733
01/29/2018 23:13:38 [INFO] exp_shallowmodel: f1_score:   0.375
01/29/2018 23:13:38 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:13:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.04      0.06        23
          C       0.43      0.11      0.18        27
          F       0.78      0.95      0.85       250
          R       0.53      0.33      0.40        52

avg / total       0.67      0.73      0.68       352

01/29/2018 23:13:38 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:13:38 [INFO] exp_shallowmodel: 
[[  1   0  19   3]
 [  1   3  19   4]
 [  4   1 237   8]
 [  2   3  30  17]]
01/29/2018 23:13:43 [INFO] exp_shallowmodel: ******************** family - Round 27 
01/29/2018 23:13:43 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:13:43 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:13:43 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:13:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:13:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:13:43 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:13:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:14:48 [INFO] exp_shallowmodel: train time: 64.380s
01/29/2018 23:14:48 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:14:48 [INFO] exp_shallowmodel: accuracy:   0.747
01/29/2018 23:14:48 [INFO] exp_shallowmodel: f1_score:   0.394
01/29/2018 23:14:48 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:14:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.60      0.22      0.32        27
          F       0.77      0.97      0.86       250
          R       0.62      0.29      0.39        52

avg / total       0.68      0.75      0.69       352

01/29/2018 23:14:48 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:14:48 [INFO] exp_shallowmodel: 
[[  0   0  21   2]
 [  0   6  17   4]
 [  2   3 242   3]
 [  1   1  35  15]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 23:14:53 [INFO] exp_shallowmodel: ******************** family - Round 28 
01/29/2018 23:14:53 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:14:53 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:14:53 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:14:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:14:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:14:53 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:14:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:15:55 [INFO] exp_shallowmodel: train time: 62.659s
01/29/2018 23:15:55 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:15:55 [INFO] exp_shallowmodel: accuracy:   0.741
01/29/2018 23:15:55 [INFO] exp_shallowmodel: f1_score:   0.403
01/29/2018 23:15:55 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:15:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.04      0.08        23
          C       0.70      0.26      0.38        27
          F       0.78      0.96      0.86       250
          R       0.40      0.23      0.29        52

avg / total       0.69      0.74      0.69       352

01/29/2018 23:15:55 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:15:55 [INFO] exp_shallowmodel: 
[[  1   1  17   4]
 [  1   7  12   7]
 [  1   1 241   7]
 [  0   1  39  12]]
01/29/2018 23:16:00 [INFO] exp_shallowmodel: ******************** family - Round 29 
01/29/2018 23:16:00 [INFO] exp_shallowmodel: #(data) = 2816
01/29/2018 23:16:00 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:16:00 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:16:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:16:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:16:00 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:16:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:17:04 [INFO] exp_shallowmodel: train time: 63.740s
01/29/2018 23:17:04 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:17:04 [INFO] exp_shallowmodel: accuracy:   0.696
01/29/2018 23:17:04 [INFO] exp_shallowmodel: f1_score:   0.297
01/29/2018 23:17:04 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:17:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        25
          C       0.09      0.04      0.05        27
          F       0.74      0.95      0.83       251
          R       0.50      0.22      0.31        59

avg / total       0.60      0.70      0.63       362

01/29/2018 23:17:04 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:17:04 [INFO] exp_shallowmodel: 
[[  0   1  21   3]
 [  0   1  22   4]
 [  3   4 238   6]
 [  0   5  41  13]]
01/29/2018 23:17:09 [INFO] exp_shallowmodel: ******************** family - Round 30 
01/29/2018 23:17:09 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:17:09 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:17:09 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:17:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:17:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:17:09 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:17:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:18:11 [INFO] exp_shallowmodel: train time: 61.977s
01/29/2018 23:18:11 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:18:11 [INFO] exp_shallowmodel: accuracy:   0.744
01/29/2018 23:18:11 [INFO] exp_shallowmodel: f1_score:   0.427
01/29/2018 23:18:11 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:18:11 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.09      0.14        23
          C       0.50      0.26      0.34        27
          F       0.77      0.96      0.86       250
          R       0.58      0.27      0.37        52

avg / total       0.70      0.74      0.70       352

01/29/2018 23:18:11 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:18:11 [INFO] exp_shallowmodel: 
[[  2   1  19   1]
 [  0   7  18   2]
 [  0   4 239   7]
 [  3   2  33  14]]
01/29/2018 23:18:17 [INFO] exp_shallowmodel: ******************** family - Round 31 
01/29/2018 23:18:17 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:18:17 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:18:17 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:18:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:18:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:18:17 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:18:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:19:21 [INFO] exp_shallowmodel: train time: 64.823s
01/29/2018 23:19:21 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:19:21 [INFO] exp_shallowmodel: accuracy:   0.730
01/29/2018 23:19:21 [INFO] exp_shallowmodel: f1_score:   0.373
01/29/2018 23:19:21 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:19:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        23
          C       0.29      0.15      0.20        27
          F       0.78      0.95      0.86       250
          R       0.52      0.29      0.37        52

avg / total       0.67      0.73      0.68       352

01/29/2018 23:19:21 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:19:21 [INFO] exp_shallowmodel: 
[[  1   2  20   0]
 [  0   4  18   5]
 [  2   2 237   9]
 [  2   6  29  15]]
01/29/2018 23:19:27 [INFO] exp_shallowmodel: ******************** family - Round 32 
01/29/2018 23:19:27 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:19:27 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:19:27 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:19:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:19:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:19:27 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:19:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:20:29 [INFO] exp_shallowmodel: train time: 62.667s
01/29/2018 23:20:29 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:20:29 [INFO] exp_shallowmodel: accuracy:   0.747
01/29/2018 23:20:29 [INFO] exp_shallowmodel: f1_score:   0.392
01/29/2018 23:20:29 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:20:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        23
          C       0.57      0.15      0.24        27
          F       0.78      0.97      0.86       250
          R       0.57      0.31      0.40        52

avg / total       0.69      0.75      0.69       352

01/29/2018 23:20:29 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:20:29 [INFO] exp_shallowmodel: 
[[  1   1  19   2]
 [  0   4  18   5]
 [  3   0 242   5]
 [  1   2  33  16]]
01/29/2018 23:20:34 [INFO] exp_shallowmodel: ******************** family - Round 33 
01/29/2018 23:20:34 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:20:34 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:20:34 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:20:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:20:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:20:34 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:20:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:21:34 [INFO] exp_shallowmodel: train time: 59.398s
01/29/2018 23:21:34 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:21:34 [INFO] exp_shallowmodel: accuracy:   0.724
01/29/2018 23:21:34 [INFO] exp_shallowmodel: f1_score:   0.381
01/29/2018 23:21:34 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:21:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.09      0.14        23
          C       0.38      0.19      0.25        27
          F       0.75      0.95      0.84       250
          R       0.59      0.19      0.29        52

avg / total       0.68      0.72      0.67       352

01/29/2018 23:21:34 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:21:34 [INFO] exp_shallowmodel: 
[[  2   1  20   0]
 [  0   5  19   3]
 [  2   6 238   4]
 [  1   1  40  10]]
01/29/2018 23:21:39 [INFO] exp_shallowmodel: ******************** family - Round 34 
01/29/2018 23:21:39 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:21:39 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:21:39 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:21:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:21:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:21:39 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:21:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:22:40 [INFO] exp_shallowmodel: train time: 61.486s
01/29/2018 23:22:40 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:22:40 [INFO] exp_shallowmodel: accuracy:   0.747
01/29/2018 23:22:40 [INFO] exp_shallowmodel: f1_score:   0.396
01/29/2018 23:22:40 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:22:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.04      0.08        23
          C       0.38      0.22      0.28        27
          F       0.79      0.97      0.87       250
          R       0.54      0.27      0.36        52

avg / total       0.69      0.75      0.70       352

01/29/2018 23:22:40 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:22:40 [INFO] exp_shallowmodel: 
[[  1   0  16   6]
 [  1   6  17   3]
 [  1   4 242   3]
 [  0   6  32  14]]
01/29/2018 23:22:46 [INFO] exp_shallowmodel: ******************** family - Round 35 
01/29/2018 23:22:46 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:22:46 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:22:46 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:22:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:22:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:22:46 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:22:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:23:49 [INFO] exp_shallowmodel: train time: 63.782s
01/29/2018 23:23:49 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:23:49 [INFO] exp_shallowmodel: accuracy:   0.702
01/29/2018 23:23:49 [INFO] exp_shallowmodel: f1_score:   0.317
01/29/2018 23:23:49 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:23:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.33      0.11      0.17        27
          F       0.76      0.93      0.84       250
          R       0.34      0.21      0.26        52

avg / total       0.62      0.70      0.65       352

01/29/2018 23:23:49 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:23:49 [INFO] exp_shallowmodel: 
[[  0   1  20   2]
 [  2   3  15   7]
 [  2   3 233  12]
 [  1   2  38  11]]
01/29/2018 23:23:55 [INFO] exp_shallowmodel: ******************** family - Round 36 
01/29/2018 23:23:55 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:23:55 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:23:55 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:23:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:23:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:23:55 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:23:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:24:57 [INFO] exp_shallowmodel: train time: 62.002s
01/29/2018 23:24:57 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:24:57 [INFO] exp_shallowmodel: accuracy:   0.707
01/29/2018 23:24:57 [INFO] exp_shallowmodel: f1_score:   0.317
01/29/2018 23:24:57 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:24:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.07        23
          C       0.40      0.15      0.22        27
          F       0.77      0.96      0.85       250
          R       0.19      0.10      0.13        52

avg / total       0.62      0.71      0.65       352

01/29/2018 23:24:57 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:24:57 [INFO] exp_shallowmodel: 
[[  1   1  18   3]
 [  0   4  14   9]
 [  0   1 239  10]
 [  3   4  40   5]]
01/29/2018 23:25:02 [INFO] exp_shallowmodel: ******************** family - Round 37 
01/29/2018 23:25:02 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:25:02 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:25:02 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:25:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:25:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:25:02 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:25:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:25:59 [INFO] exp_shallowmodel: train time: 56.890s
01/29/2018 23:25:59 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:25:59 [INFO] exp_shallowmodel: accuracy:   0.741
01/29/2018 23:25:59 [INFO] exp_shallowmodel: f1_score:   0.394
01/29/2018 23:25:59 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:25:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       1.00      0.17      0.30        23
          C       0.25      0.07      0.11        27
          F       0.76      0.98      0.85       250
          R       0.58      0.21      0.31        52

avg / total       0.71      0.74      0.68       352

01/29/2018 23:25:59 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:25:59 [INFO] exp_shallowmodel: 
[[  4   1  17   1]
 [  0   2  22   3]
 [  0   2 244   4]
 [  0   3  38  11]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 23:26:04 [INFO] exp_shallowmodel: ******************** family - Round 38 
01/29/2018 23:26:04 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:26:04 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:26:04 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:26:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:26:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:26:04 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:26:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:27:06 [INFO] exp_shallowmodel: train time: 61.643s
01/29/2018 23:27:06 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:27:06 [INFO] exp_shallowmodel: accuracy:   0.753
01/29/2018 23:27:06 [INFO] exp_shallowmodel: f1_score:   0.438
01/29/2018 23:27:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:27:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.67      0.17      0.28        23
          C       0.50      0.22      0.31        27
          F       0.78      0.98      0.87       250
          R       0.52      0.21      0.30        52

avg / total       0.71      0.75      0.70       352

01/29/2018 23:27:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:27:06 [INFO] exp_shallowmodel: 
[[  4   0  17   2]
 [  0   6  16   5]
 [  0   3 244   3]
 [  2   3  36  11]]
01/29/2018 23:27:11 [INFO] exp_shallowmodel: ******************** family - Round 39 
01/29/2018 23:27:11 [INFO] exp_shallowmodel: #(data) = 2816
01/29/2018 23:27:11 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:27:11 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:27:11 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:27:11 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:27:11 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:27:11 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:28:10 [INFO] exp_shallowmodel: train time: 59.083s
01/29/2018 23:28:10 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:28:10 [INFO] exp_shallowmodel: accuracy:   0.735
01/29/2018 23:28:10 [INFO] exp_shallowmodel: f1_score:   0.395
01/29/2018 23:28:10 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:28:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        25
          C       0.46      0.22      0.30        27
          F       0.76      0.96      0.85       251
          R       0.66      0.32      0.43        59

avg / total       0.67      0.73      0.68       362

01/29/2018 23:28:10 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:28:10 [INFO] exp_shallowmodel: 
[[  0   0  23   2]
 [  0   6  18   3]
 [  1   4 241   5]
 [  1   3  36  19]]
01/29/2018 23:28:15 [INFO] exp_shallowmodel: ******************** family - Round 40 
01/29/2018 23:28:15 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:28:15 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:28:15 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:28:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:28:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:28:15 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:28:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:29:17 [INFO] exp_shallowmodel: train time: 61.960s
01/29/2018 23:29:17 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:29:17 [INFO] exp_shallowmodel: accuracy:   0.727
01/29/2018 23:29:17 [INFO] exp_shallowmodel: f1_score:   0.348
01/29/2018 23:29:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:29:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.50      0.26      0.34        27
          F       0.77      0.97      0.86       250
          R       0.35      0.13      0.19        52

avg / total       0.64      0.73      0.66       352

01/29/2018 23:29:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:29:17 [INFO] exp_shallowmodel: 
[[  0   0  18   5]
 [  1   7  16   3]
 [  1   2 242   5]
 [  1   5  39   7]]
01/29/2018 23:29:23 [INFO] exp_shallowmodel: ******************** family - Round 41 
01/29/2018 23:29:23 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:29:23 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:29:23 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:29:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:29:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:29:23 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:29:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:30:22 [INFO] exp_shallowmodel: train time: 59.679s
01/29/2018 23:30:22 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:30:22 [INFO] exp_shallowmodel: accuracy:   0.764
01/29/2018 23:30:22 [INFO] exp_shallowmodel: f1_score:   0.397
01/29/2018 23:30:22 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:30:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.40      0.15      0.22        27
          F       0.79      0.98      0.87       250
          R       0.66      0.40      0.50        52

avg / total       0.69      0.76      0.71       352

01/29/2018 23:30:22 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:30:22 [INFO] exp_shallowmodel: 
[[  0   2  18   3]
 [  0   4  19   4]
 [  0   2 244   4]
 [  0   2  29  21]]
01/29/2018 23:30:27 [INFO] exp_shallowmodel: ******************** family - Round 42 
01/29/2018 23:30:27 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:30:27 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:30:27 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:30:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:30:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:30:27 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:30:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:31:27 [INFO] exp_shallowmodel: train time: 59.593s
01/29/2018 23:31:27 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:31:27 [INFO] exp_shallowmodel: accuracy:   0.756
01/29/2018 23:31:27 [INFO] exp_shallowmodel: f1_score:   0.388
01/29/2018 23:31:27 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:31:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.45      0.19      0.26        27
          F       0.79      0.98      0.87       250
          R       0.57      0.33      0.41        52

avg / total       0.68      0.76      0.70       352

01/29/2018 23:31:27 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:31:27 [INFO] exp_shallowmodel: 
[[  0   2  16   5]
 [  0   5  18   4]
 [  2   0 244   4]
 [  1   4  30  17]]
01/29/2018 23:31:32 [INFO] exp_shallowmodel: ******************** family - Round 43 
01/29/2018 23:31:32 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:31:32 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:31:32 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:31:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:31:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:31:32 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:31:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:32:35 [INFO] exp_shallowmodel: train time: 62.326s
01/29/2018 23:32:35 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:32:35 [INFO] exp_shallowmodel: accuracy:   0.741
01/29/2018 23:32:35 [INFO] exp_shallowmodel: f1_score:   0.387
01/29/2018 23:32:35 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:32:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.09      0.14        23
          C       0.33      0.11      0.17        27
          F       0.77      0.96      0.86       250
          R       0.56      0.29      0.38        52

avg / total       0.68      0.74      0.69       352

01/29/2018 23:32:35 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:32:35 [INFO] exp_shallowmodel: 
[[  2   1  17   3]
 [  1   3  18   5]
 [  2   3 241   4]
 [  0   2  35  15]]
01/29/2018 23:32:40 [INFO] exp_shallowmodel: ******************** family - Round 44 
01/29/2018 23:32:40 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:32:40 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:32:40 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:32:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:32:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:32:40 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:32:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:33:39 [INFO] exp_shallowmodel: train time: 59.059s
01/29/2018 23:33:39 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:33:39 [INFO] exp_shallowmodel: accuracy:   0.739
01/29/2018 23:33:39 [INFO] exp_shallowmodel: f1_score:   0.384
01/29/2018 23:33:39 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:33:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.04      0.06        23
          C       0.40      0.15      0.22        27
          F       0.78      0.96      0.86       250
          R       0.55      0.31      0.40        52

avg / total       0.68      0.74      0.69       352

01/29/2018 23:33:39 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:33:39 [INFO] exp_shallowmodel: 
[[  1   0  20   2]
 [  0   4  16   7]
 [  5   2 239   4]
 [  2   4  30  16]]
01/29/2018 23:33:44 [INFO] exp_shallowmodel: ******************** family - Round 45 
01/29/2018 23:33:44 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:33:44 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:33:44 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:33:44 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:33:44 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:33:44 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:33:44 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:34:46 [INFO] exp_shallowmodel: train time: 61.947s
01/29/2018 23:34:46 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:34:46 [INFO] exp_shallowmodel: accuracy:   0.733
01/29/2018 23:34:46 [INFO] exp_shallowmodel: f1_score:   0.384
01/29/2018 23:34:46 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:34:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.60      0.13      0.21        23
          C       0.20      0.07      0.11        27
          F       0.78      0.95      0.86       250
          R       0.47      0.29      0.36        52

avg / total       0.68      0.73      0.68       352

01/29/2018 23:34:46 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:34:46 [INFO] exp_shallowmodel: 
[[  3   0  18   2]
 [  0   2  15  10]
 [  2   5 238   5]
 [  0   3  34  15]]
01/29/2018 23:34:51 [INFO] exp_shallowmodel: ******************** family - Round 46 
01/29/2018 23:34:51 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:34:51 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:34:51 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:34:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:34:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:34:51 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:34:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:35:55 [INFO] exp_shallowmodel: train time: 63.945s
01/29/2018 23:35:55 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:35:55 [INFO] exp_shallowmodel: accuracy:   0.724
01/29/2018 23:35:55 [INFO] exp_shallowmodel: f1_score:   0.364
01/29/2018 23:35:55 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:35:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.42      0.19      0.26        27
          F       0.76      0.94      0.84       250
          R       0.54      0.27      0.36        52

avg / total       0.65      0.72      0.67       352

01/29/2018 23:35:55 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:35:55 [INFO] exp_shallowmodel: 
[[  0   1  17   5]
 [  0   5  21   1]
 [  3   5 236   6]
 [  0   1  37  14]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 23:36:00 [INFO] exp_shallowmodel: ******************** family - Round 47 
01/29/2018 23:36:00 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:36:00 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:36:00 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:36:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:36:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:36:00 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:36:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:37:04 [INFO] exp_shallowmodel: train time: 63.955s
01/29/2018 23:37:04 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:37:04 [INFO] exp_shallowmodel: accuracy:   0.727
01/29/2018 23:37:04 [INFO] exp_shallowmodel: f1_score:   0.344
01/29/2018 23:37:04 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:37:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.04      0.08        23
          C       0.20      0.07      0.11        27
          F       0.76      0.96      0.85       250
          R       0.54      0.25      0.34        52

avg / total       0.66      0.73      0.67       352

01/29/2018 23:37:04 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:37:04 [INFO] exp_shallowmodel: 
[[  1   1  19   2]
 [  1   2  22   2]
 [  0   3 240   7]
 [  1   4  34  13]]
01/29/2018 23:37:09 [INFO] exp_shallowmodel: ******************** family - Round 48 
01/29/2018 23:37:09 [INFO] exp_shallowmodel: #(data) = 2826
01/29/2018 23:37:09 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:37:09 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:37:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:37:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:37:09 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:37:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:38:10 [INFO] exp_shallowmodel: train time: 61.215s
01/29/2018 23:38:10 [INFO] exp_shallowmodel: test time:  0.020s
01/29/2018 23:38:10 [INFO] exp_shallowmodel: accuracy:   0.722
01/29/2018 23:38:10 [INFO] exp_shallowmodel: f1_score:   0.361
01/29/2018 23:38:10 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:38:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.04      0.07        23
          C       0.50      0.22      0.31        27
          F       0.76      0.96      0.85       250
          R       0.38      0.15      0.22        52

avg / total       0.65      0.72      0.66       352

01/29/2018 23:38:10 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:38:10 [INFO] exp_shallowmodel: 
[[  1   1  20   1]
 [  2   6  15   4]
 [  1   2 239   8]
 [  2   3  39   8]]
01/29/2018 23:38:16 [INFO] exp_shallowmodel: ******************** family - Round 49 
01/29/2018 23:38:16 [INFO] exp_shallowmodel: #(data) = 2816
01/29/2018 23:38:16 [INFO] exp_shallowmodel: #(feature) = 41252
01/29/2018 23:38:16 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:38:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:38:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:38:16 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:38:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:39:17 [INFO] exp_shallowmodel: train time: 61.702s
01/29/2018 23:39:17 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:39:17 [INFO] exp_shallowmodel: accuracy:   0.718
01/29/2018 23:39:17 [INFO] exp_shallowmodel: f1_score:   0.384
01/29/2018 23:39:17 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:39:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        25
          C       0.45      0.19      0.26        27
          F       0.75      0.95      0.84       251
          R       0.57      0.27      0.37        59

avg / total       0.66      0.72      0.66       362

01/29/2018 23:39:17 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:39:17 [INFO] exp_shallowmodel: 
[[  1   1  22   1]
 [  1   5  16   5]
 [  2   5 238   6]
 [  1   0  42  16]]
01/29/2018 23:39:37 [INFO] exp_shallowmodel: ******************** ghome - Round 0 
01/29/2018 23:39:37 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:39:37 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:39:37 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:39:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:39:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:39:37 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:39:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:40:51 [INFO] exp_shallowmodel: train time: 73.110s
01/29/2018 23:40:51 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:40:51 [INFO] exp_shallowmodel: accuracy:   0.778
01/29/2018 23:40:51 [INFO] exp_shallowmodel: f1_score:   0.402
01/29/2018 23:40:51 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:40:51 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.32      0.15      0.21        59
          C       0.50      0.08      0.14        12
          F       0.81      0.96      0.88       396
          R       0.74      0.25      0.38        55

avg / total       0.74      0.78      0.73       522

01/29/2018 23:40:51 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:40:51 [INFO] exp_shallowmodel: 
[[  9   0  47   3]
 [  1   1  10   0]
 [ 11   1 382   2]
 [  7   0  34  14]]
01/29/2018 23:40:56 [INFO] exp_shallowmodel: ******************** ghome - Round 1 
01/29/2018 23:40:56 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:40:56 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:40:56 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:40:56 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:40:56 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:40:56 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:40:56 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:42:19 [INFO] exp_shallowmodel: train time: 82.997s
01/29/2018 23:42:19 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:42:19 [INFO] exp_shallowmodel: accuracy:   0.789
01/29/2018 23:42:19 [INFO] exp_shallowmodel: f1_score:   0.417
01/29/2018 23:42:19 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:42:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.59      0.22      0.32        59
          C       0.50      0.08      0.14        12
          F       0.82      0.97      0.89       396
          R       0.46      0.24      0.31        55

avg / total       0.75      0.79      0.75       522

01/29/2018 23:42:19 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:42:19 [INFO] exp_shallowmodel: 
[[ 13   0  41   5]
 [  3   1   7   1]
 [  1   1 385   9]
 [  5   0  37  13]]
01/29/2018 23:42:25 [INFO] exp_shallowmodel: ******************** ghome - Round 2 
01/29/2018 23:42:25 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:42:25 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:42:25 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:42:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:42:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:42:25 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:42:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:43:45 [INFO] exp_shallowmodel: train time: 80.502s
01/29/2018 23:43:45 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:43:45 [INFO] exp_shallowmodel: accuracy:   0.778
01/29/2018 23:43:45 [INFO] exp_shallowmodel: f1_score:   0.390
01/29/2018 23:43:45 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:43:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.42      0.19      0.26        59
          C       0.00      0.00      0.00        12
          F       0.81      0.95      0.88       396
          R       0.60      0.33      0.42        55

avg / total       0.73      0.78      0.74       522

01/29/2018 23:43:45 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:43:45 [INFO] exp_shallowmodel: 
[[ 11   1  40   7]
 [  2   0  10   0]
 [ 13   1 377   5]
 [  0   1  36  18]]
01/29/2018 23:43:51 [INFO] exp_shallowmodel: ******************** ghome - Round 3 
01/29/2018 23:43:51 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:43:51 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:43:51 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:43:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:43:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:43:51 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:43:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:45:09 [INFO] exp_shallowmodel: train time: 78.076s
01/29/2018 23:45:09 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:45:09 [INFO] exp_shallowmodel: accuracy:   0.766
01/29/2018 23:45:09 [INFO] exp_shallowmodel: f1_score:   0.356
01/29/2018 23:45:09 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:45:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.46      0.20      0.28        59
          C       0.00      0.00      0.00        12
          F       0.81      0.95      0.88       396
          R       0.38      0.20      0.26        55

avg / total       0.71      0.77      0.73       522

01/29/2018 23:45:09 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:45:09 [INFO] exp_shallowmodel: 
[[ 12   0  41   6]
 [  0   0   8   4]
 [  7   4 377   8]
 [  7   0  37  11]]
01/29/2018 23:45:14 [INFO] exp_shallowmodel: ******************** ghome - Round 4 
01/29/2018 23:45:14 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:45:14 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:45:14 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:45:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:45:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:45:14 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:45:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:46:32 [INFO] exp_shallowmodel: train time: 77.652s
01/29/2018 23:46:32 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:46:32 [INFO] exp_shallowmodel: accuracy:   0.785
01/29/2018 23:46:32 [INFO] exp_shallowmodel: f1_score:   0.454
01/29/2018 23:46:32 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:46:32 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.45      0.24      0.31        59
          C       0.67      0.17      0.27        12
          F       0.82      0.96      0.88       396
          R       0.58      0.25      0.35        55

avg / total       0.75      0.79      0.75       522

01/29/2018 23:46:32 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:46:32 [INFO] exp_shallowmodel: 
[[ 14   1  40   4]
 [  0   2   9   1]
 [ 11   0 380   5]
 [  6   0  35  14]]
01/29/2018 23:46:37 [INFO] exp_shallowmodel: ******************** ghome - Round 5 
01/29/2018 23:46:37 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:46:37 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:46:37 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:46:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:46:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:46:37 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:46:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:47:54 [INFO] exp_shallowmodel: train time: 76.610s
01/29/2018 23:47:54 [INFO] exp_shallowmodel: test time:  0.023s
01/29/2018 23:47:54 [INFO] exp_shallowmodel: accuracy:   0.782
01/29/2018 23:47:54 [INFO] exp_shallowmodel: f1_score:   0.383
01/29/2018 23:47:54 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:47:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.52      0.20      0.29        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.52      0.27      0.36        55

avg / total       0.73      0.78      0.74       522

01/29/2018 23:47:54 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:47:54 [INFO] exp_shallowmodel: 
[[ 12   0  43   4]
 [  1   0   9   2]
 [  7   0 381   8]
 [  3   1  36  15]]
01/29/2018 23:47:59 [INFO] exp_shallowmodel: ******************** ghome - Round 6 
01/29/2018 23:47:59 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:47:59 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:47:59 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:47:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:47:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:47:59 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:47:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:49:24 [INFO] exp_shallowmodel: train time: 84.727s
01/29/2018 23:49:24 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:49:24 [INFO] exp_shallowmodel: accuracy:   0.782
01/29/2018 23:49:24 [INFO] exp_shallowmodel: f1_score:   0.390
01/29/2018 23:49:24 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:49:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.71      0.20      0.32        59
          C       0.33      0.08      0.13        12
          F       0.81      0.97      0.88       396
          R       0.38      0.16      0.23        55

avg / total       0.74      0.78      0.73       522

01/29/2018 23:49:24 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:49:24 [INFO] exp_shallowmodel: 
[[ 12   1  38   8]
 [  1   1   9   1]
 [  3   1 386   6]
 [  1   0  45   9]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/29/2018 23:49:30 [INFO] exp_shallowmodel: ******************** ghome - Round 7 
01/29/2018 23:49:30 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:49:30 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:49:30 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:49:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:49:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:49:30 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:49:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:50:41 [INFO] exp_shallowmodel: train time: 71.463s
01/29/2018 23:50:41 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:50:41 [INFO] exp_shallowmodel: accuracy:   0.778
01/29/2018 23:50:41 [INFO] exp_shallowmodel: f1_score:   0.388
01/29/2018 23:50:41 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:50:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.12      0.17        59
          C       0.50      0.08      0.14        12
          F       0.81      0.97      0.88       396
          R       0.56      0.25      0.35        55

avg / total       0.72      0.78      0.73       522

01/29/2018 23:50:41 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:50:41 [INFO] exp_shallowmodel: 
[[  7   0  47   5]
 [  0   1   9   2]
 [  8   0 384   4]
 [  6   1  34  14]]
01/29/2018 23:50:47 [INFO] exp_shallowmodel: ******************** ghome - Round 8 
01/29/2018 23:50:47 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:50:47 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:50:47 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:50:47 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:50:47 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:50:47 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:50:47 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:52:08 [INFO] exp_shallowmodel: train time: 81.779s
01/29/2018 23:52:08 [INFO] exp_shallowmodel: test time:  0.021s
01/29/2018 23:52:08 [INFO] exp_shallowmodel: accuracy:   0.768
01/29/2018 23:52:08 [INFO] exp_shallowmodel: f1_score:   0.367
01/29/2018 23:52:08 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:52:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.10      0.16        59
          C       0.50      0.08      0.14        12
          F       0.80      0.96      0.88       396
          R       0.44      0.22      0.29        55

avg / total       0.71      0.77      0.72       522

01/29/2018 23:52:08 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:52:08 [INFO] exp_shallowmodel: 
[[  6   1  45   7]
 [  0   1  11   0]
 [  6   0 382   8]
 [  6   0  37  12]]
01/29/2018 23:52:14 [INFO] exp_shallowmodel: ******************** ghome - Round 9 
01/29/2018 23:52:14 [INFO] exp_shallowmodel: #(data) = 4176
01/29/2018 23:52:14 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:52:14 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:52:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:52:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:52:14 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:52:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:53:30 [INFO] exp_shallowmodel: train time: 76.122s
01/29/2018 23:53:30 [INFO] exp_shallowmodel: test time:  0.023s
01/29/2018 23:53:30 [INFO] exp_shallowmodel: accuracy:   0.755
01/29/2018 23:53:30 [INFO] exp_shallowmodel: f1_score:   0.337
01/29/2018 23:53:30 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:53:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.46      0.19      0.27        64
          C       0.00      0.00      0.00        14
          F       0.79      0.97      0.87       402
          R       0.41      0.14      0.21        63

avg / total       0.69      0.76      0.70       543

01/29/2018 23:53:30 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:53:30 [INFO] exp_shallowmodel: 
[[ 12   0  47   5]
 [  2   0  11   1]
 [  6   0 389   7]
 [  6   2  46   9]]
01/29/2018 23:53:35 [INFO] exp_shallowmodel: ******************** ghome - Round 10 
01/29/2018 23:53:35 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:53:35 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:53:35 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:53:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:53:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:53:35 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:53:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:54:45 [INFO] exp_shallowmodel: train time: 69.831s
01/29/2018 23:54:45 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:54:45 [INFO] exp_shallowmodel: accuracy:   0.772
01/29/2018 23:54:45 [INFO] exp_shallowmodel: f1_score:   0.350
01/29/2018 23:54:45 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:54:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.45      0.15      0.23        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.43      0.22      0.29        55

avg / total       0.71      0.77      0.72       522

01/29/2018 23:54:45 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:54:45 [INFO] exp_shallowmodel: 
[[  9   0  42   8]
 [  2   0  10   0]
 [  5   1 382   8]
 [  4   2  37  12]]
01/29/2018 23:54:51 [INFO] exp_shallowmodel: ******************** ghome - Round 11 
01/29/2018 23:54:51 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:54:51 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:54:51 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:54:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:54:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:54:51 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:54:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:56:06 [INFO] exp_shallowmodel: train time: 75.479s
01/29/2018 23:56:06 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:56:06 [INFO] exp_shallowmodel: accuracy:   0.764
01/29/2018 23:56:06 [INFO] exp_shallowmodel: f1_score:   0.374
01/29/2018 23:56:06 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:56:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.42      0.17      0.24        59
          C       0.33      0.08      0.13        12
          F       0.81      0.95      0.87       396
          R       0.38      0.18      0.25        55

avg / total       0.71      0.76      0.72       522

01/29/2018 23:56:06 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:56:06 [INFO] exp_shallowmodel: 
[[ 10   0  45   4]
 [  0   1   9   2]
 [  7   1 378  10]
 [  7   1  37  10]]
01/29/2018 23:56:12 [INFO] exp_shallowmodel: ******************** ghome - Round 12 
01/29/2018 23:56:12 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:56:12 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:56:12 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:56:12 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:56:12 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:56:12 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:56:12 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:57:36 [INFO] exp_shallowmodel: train time: 84.343s
01/29/2018 23:57:36 [INFO] exp_shallowmodel: test time:  0.022s
01/29/2018 23:57:36 [INFO] exp_shallowmodel: accuracy:   0.762
01/29/2018 23:57:36 [INFO] exp_shallowmodel: f1_score:   0.354
01/29/2018 23:57:36 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:57:36 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.17      0.24        59
          C       0.00      0.00      0.00        12
          F       0.80      0.95      0.87       396
          R       0.43      0.24      0.31        55

avg / total       0.70      0.76      0.72       522

01/29/2018 23:57:36 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:57:36 [INFO] exp_shallowmodel: 
[[ 10   1  44   4]
 [  0   0  11   1]
 [  9   0 375  12]
 [  6   0  36  13]]
01/29/2018 23:57:42 [INFO] exp_shallowmodel: ******************** ghome - Round 13 
01/29/2018 23:57:42 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:57:42 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:57:42 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:57:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:57:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:57:42 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:57:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/29/2018 23:58:57 [INFO] exp_shallowmodel: train time: 74.821s
01/29/2018 23:58:57 [INFO] exp_shallowmodel: test time:  0.023s
01/29/2018 23:58:57 [INFO] exp_shallowmodel: accuracy:   0.766
01/29/2018 23:58:57 [INFO] exp_shallowmodel: f1_score:   0.378
01/29/2018 23:58:57 [INFO] exp_shallowmodel: classification report:
01/29/2018 23:58:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.20        59
          C       0.50      0.08      0.14        12
          F       0.80      0.96      0.87       396
          R       0.44      0.22      0.29        55

avg / total       0.71      0.77      0.72       522

01/29/2018 23:58:57 [INFO] exp_shallowmodel: confusion matrix:
01/29/2018 23:58:57 [INFO] exp_shallowmodel: 
[[  8   1  44   6]
 [  1   1  10   0]
 [  8   0 379   9]
 [  3   0  40  12]]
01/29/2018 23:59:02 [INFO] exp_shallowmodel: ******************** ghome - Round 14 
01/29/2018 23:59:02 [INFO] exp_shallowmodel: #(data) = 4197
01/29/2018 23:59:02 [INFO] exp_shallowmodel: #(feature) = 29589
01/29/2018 23:59:02 [INFO] exp_shallowmodel: ================================================================================
01/29/2018 23:59:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/29/2018 23:59:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/29/2018 23:59:02 [INFO] exp_shallowmodel: Training: 
01/29/2018 23:59:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:00:17 [INFO] exp_shallowmodel: train time: 75.044s
01/30/2018 00:00:17 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:00:17 [INFO] exp_shallowmodel: accuracy:   0.780
01/30/2018 00:00:17 [INFO] exp_shallowmodel: f1_score:   0.364
01/30/2018 00:00:17 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:00:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.44      0.20      0.28        59
          C       0.00      0.00      0.00        12
          F       0.82      0.97      0.89       396
          R       0.41      0.22      0.29        55

avg / total       0.72      0.78      0.74       522

01/30/2018 00:00:17 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:00:17 [INFO] exp_shallowmodel: 
[[ 12   1  34  12]
 [  3   0   8   1]
 [  9   0 383   4]
 [  3   0  40  12]]
01/30/2018 00:00:23 [INFO] exp_shallowmodel: ******************** ghome - Round 15 
01/30/2018 00:00:23 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:00:23 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:00:23 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:00:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:00:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:00:23 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:00:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:01:47 [INFO] exp_shallowmodel: train time: 84.363s
01/30/2018 00:01:47 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:01:47 [INFO] exp_shallowmodel: accuracy:   0.789
01/30/2018 00:01:47 [INFO] exp_shallowmodel: f1_score:   0.362
01/30/2018 00:01:47 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:01:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.56      0.17      0.26        59
          C       0.00      0.00      0.00        12
          F       0.81      0.99      0.89       396
          R       0.61      0.20      0.30        55

avg / total       0.74      0.79      0.73       522

01/30/2018 00:01:47 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:01:47 [INFO] exp_shallowmodel: 
[[ 10   0  46   3]
 [  2   0   9   1]
 [  2   0 391   3]
 [  4   1  39  11]]
01/30/2018 00:01:53 [INFO] exp_shallowmodel: ******************** ghome - Round 16 
01/30/2018 00:01:53 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:01:53 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:01:53 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:01:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:01:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:01:53 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:01:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:03:10 [INFO] exp_shallowmodel: train time: 77.603s
01/30/2018 00:03:10 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:03:10 [INFO] exp_shallowmodel: accuracy:   0.764
01/30/2018 00:03:10 [INFO] exp_shallowmodel: f1_score:   0.325
01/30/2018 00:03:10 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:03:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.19      0.27        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.26      0.11      0.15        55

avg / total       0.69      0.76      0.71       522

01/30/2018 00:03:10 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:03:10 [INFO] exp_shallowmodel: 
[[ 11   1  39   8]
 [  1   0   9   2]
 [  6   1 382   7]
 [  5   0  44   6]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/30/2018 00:03:16 [INFO] exp_shallowmodel: ******************** ghome - Round 17 
01/30/2018 00:03:16 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:03:16 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:03:16 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:03:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:03:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:03:16 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:03:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:04:37 [INFO] exp_shallowmodel: train time: 80.974s
01/30/2018 00:04:37 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:04:37 [INFO] exp_shallowmodel: accuracy:   0.749
01/30/2018 00:04:37 [INFO] exp_shallowmodel: f1_score:   0.345
01/30/2018 00:04:37 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:04:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.12      0.17        59
          C       0.50      0.08      0.14        12
          F       0.79      0.95      0.86       396
          R       0.32      0.15      0.20        55

avg / total       0.68      0.75      0.70       522

01/30/2018 00:04:37 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:04:37 [INFO] exp_shallowmodel: 
[[  7   0  49   3]
 [  2   1   6   3]
 [  9   1 375  11]
 [  3   0  44   8]]
01/30/2018 00:04:42 [INFO] exp_shallowmodel: ******************** ghome - Round 18 
01/30/2018 00:04:42 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:04:42 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:04:42 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:04:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:04:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:04:42 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:04:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:06:02 [INFO] exp_shallowmodel: train time: 79.218s
01/30/2018 00:06:02 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:06:02 [INFO] exp_shallowmodel: accuracy:   0.757
01/30/2018 00:06:02 [INFO] exp_shallowmodel: f1_score:   0.360
01/30/2018 00:06:02 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:06:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.15      0.22        59
          C       1.00      0.08      0.15        12
          F       0.80      0.95      0.87       396
          R       0.32      0.15      0.20        55

avg / total       0.71      0.76      0.71       522

01/30/2018 00:06:02 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:06:02 [INFO] exp_shallowmodel: 
[[  9   0  44   6]
 [  0   1   9   2]
 [ 10   0 377   9]
 [  4   0  43   8]]
01/30/2018 00:06:07 [INFO] exp_shallowmodel: ******************** ghome - Round 19 
01/30/2018 00:06:07 [INFO] exp_shallowmodel: #(data) = 4176
01/30/2018 00:06:07 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:06:07 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:06:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:06:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:06:07 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:06:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:07:20 [INFO] exp_shallowmodel: train time: 72.823s
01/30/2018 00:07:20 [INFO] exp_shallowmodel: test time:  0.023s
01/30/2018 00:07:20 [INFO] exp_shallowmodel: accuracy:   0.762
01/30/2018 00:07:20 [INFO] exp_shallowmodel: f1_score:   0.401
01/30/2018 00:07:20 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:07:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.42      0.17      0.24        64
          C       0.33      0.07      0.12        14
          F       0.80      0.96      0.87       402
          R       0.53      0.29      0.37        63

avg / total       0.71      0.76      0.72       543

01/30/2018 00:07:20 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:07:20 [INFO] exp_shallowmodel: 
[[ 11   1  46   6]
 [  2   1  10   1]
 [  8   1 384   9]
 [  5   0  40  18]]
01/30/2018 00:07:26 [INFO] exp_shallowmodel: ******************** ghome - Round 20 
01/30/2018 00:07:26 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:07:26 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:07:26 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:07:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:07:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:07:26 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:07:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:08:33 [INFO] exp_shallowmodel: train time: 66.831s
01/30/2018 00:08:33 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:08:33 [INFO] exp_shallowmodel: accuracy:   0.784
01/30/2018 00:08:33 [INFO] exp_shallowmodel: f1_score:   0.428
01/30/2018 00:08:33 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:08:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.22      0.31        59
          C       1.00      0.08      0.15        12
          F       0.82      0.96      0.88       396
          R       0.52      0.29      0.37        55

avg / total       0.75      0.78      0.75       522

01/30/2018 00:08:33 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:08:33 [INFO] exp_shallowmodel: 
[[ 13   0  41   5]
 [  0   1   9   2]
 [  9   0 379   8]
 [  4   0  35  16]]
01/30/2018 00:08:38 [INFO] exp_shallowmodel: ******************** ghome - Round 21 
01/30/2018 00:08:38 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:08:38 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:08:38 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:08:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:08:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:08:38 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:08:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:09:58 [INFO] exp_shallowmodel: train time: 80.310s
01/30/2018 00:09:58 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:09:58 [INFO] exp_shallowmodel: accuracy:   0.772
01/30/2018 00:09:58 [INFO] exp_shallowmodel: f1_score:   0.389
01/30/2018 00:09:58 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:09:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.17      0.25        59
          C       0.33      0.08      0.13        12
          F       0.81      0.96      0.88       396
          R       0.46      0.22      0.30        55

avg / total       0.72      0.77      0.73       522

01/30/2018 00:09:58 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:09:58 [INFO] exp_shallowmodel: 
[[ 10   1  44   4]
 [  0   1   8   3]
 [  9   0 380   7]
 [  2   1  40  12]]
01/30/2018 00:10:04 [INFO] exp_shallowmodel: ******************** ghome - Round 22 
01/30/2018 00:10:04 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:10:04 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:10:04 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:10:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:10:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:10:04 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:10:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:11:19 [INFO] exp_shallowmodel: train time: 74.654s
01/30/2018 00:11:19 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:11:19 [INFO] exp_shallowmodel: accuracy:   0.774
01/30/2018 00:11:19 [INFO] exp_shallowmodel: f1_score:   0.391
01/30/2018 00:11:19 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:11:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.36      0.17      0.23        59
          C       0.50      0.08      0.14        12
          F       0.81      0.96      0.88       396
          R       0.55      0.22      0.31        55

avg / total       0.72      0.77      0.73       522

01/30/2018 00:11:19 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:11:19 [INFO] exp_shallowmodel: 
[[ 10   0  46   3]
 [  2   1   9   0]
 [  7   1 381   7]
 [  9   0  34  12]]
01/30/2018 00:11:24 [INFO] exp_shallowmodel: ******************** ghome - Round 23 
01/30/2018 00:11:24 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:11:24 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:11:24 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:11:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:11:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:11:24 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:11:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:12:43 [INFO] exp_shallowmodel: train time: 78.428s
01/30/2018 00:12:43 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:12:43 [INFO] exp_shallowmodel: accuracy:   0.776
01/30/2018 00:12:43 [INFO] exp_shallowmodel: f1_score:   0.356
01/30/2018 00:12:43 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:12:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.12      0.18        59
          C       0.00      0.00      0.00        12
          F       0.81      0.97      0.88       396
          R       0.54      0.27      0.36        55

avg / total       0.71      0.78      0.73       522

01/30/2018 00:12:43 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:12:43 [INFO] exp_shallowmodel: 
[[  7   1  44   7]
 [  1   0  11   0]
 [  6   1 383   6]
 [  4   0  36  15]]
01/30/2018 00:12:48 [INFO] exp_shallowmodel: ******************** ghome - Round 24 
01/30/2018 00:12:48 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:12:48 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:12:48 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:12:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:12:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:12:48 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:12:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:14:08 [INFO] exp_shallowmodel: train time: 79.977s
01/30/2018 00:14:08 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:14:08 [INFO] exp_shallowmodel: accuracy:   0.761
01/30/2018 00:14:08 [INFO] exp_shallowmodel: f1_score:   0.325
01/30/2018 00:14:08 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:14:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.15      0.22        59
          C       0.00      0.00      0.00        12
          F       0.79      0.96      0.87       396
          R       0.40      0.15      0.21        55

avg / total       0.69      0.76      0.71       522

01/30/2018 00:14:08 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:14:08 [INFO] exp_shallowmodel: 
[[  9   0  46   4]
 [  2   0   9   1]
 [  9   0 380   7]
 [  3   0  44   8]]
01/30/2018 00:14:14 [INFO] exp_shallowmodel: ******************** ghome - Round 25 
01/30/2018 00:14:14 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:14:14 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:14:14 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:14:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:14:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:14:14 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:14:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:15:34 [INFO] exp_shallowmodel: train time: 80.026s
01/30/2018 00:15:34 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:15:34 [INFO] exp_shallowmodel: accuracy:   0.766
01/30/2018 00:15:34 [INFO] exp_shallowmodel: f1_score:   0.341
01/30/2018 00:15:34 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:15:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.10      0.15        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.52      0.25      0.34        55

avg / total       0.70      0.77      0.72       522

01/30/2018 00:15:34 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:15:34 [INFO] exp_shallowmodel: 
[[  6   0  45   8]
 [  1   0  11   0]
 [ 10   1 380   5]
 [  5   0  36  14]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/30/2018 00:15:39 [INFO] exp_shallowmodel: ******************** ghome - Round 26 
01/30/2018 00:15:39 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:15:39 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:15:39 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:15:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:15:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:15:39 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:15:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:17:05 [INFO] exp_shallowmodel: train time: 85.515s
01/30/2018 00:17:05 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:17:05 [INFO] exp_shallowmodel: accuracy:   0.768
01/30/2018 00:17:05 [INFO] exp_shallowmodel: f1_score:   0.375
01/30/2018 00:17:05 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:17:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.19      0.25        59
          C       1.00      0.08      0.15        12
          F       0.80      0.96      0.88       396
          R       0.42      0.15      0.22        55

avg / total       0.72      0.77      0.72       522

01/30/2018 00:17:05 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:17:05 [INFO] exp_shallowmodel: 
[[ 11   0  44   4]
 [  1   1  10   0]
 [  8   0 381   7]
 [  8   0  39   8]]
01/30/2018 00:17:10 [INFO] exp_shallowmodel: ******************** ghome - Round 27 
01/30/2018 00:17:10 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:17:10 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:17:10 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:17:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:17:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:17:10 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:17:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:18:31 [INFO] exp_shallowmodel: train time: 80.833s
01/30/2018 00:18:31 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:18:31 [INFO] exp_shallowmodel: accuracy:   0.762
01/30/2018 00:18:31 [INFO] exp_shallowmodel: f1_score:   0.329
01/30/2018 00:18:31 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:18:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.35      0.15      0.21        59
          C       0.00      0.00      0.00        12
          F       0.80      0.96      0.87       396
          R       0.39      0.16      0.23        55

avg / total       0.69      0.76      0.71       522

01/30/2018 00:18:31 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:18:31 [INFO] exp_shallowmodel: 
[[  9   0  45   5]
 [  0   0  10   2]
 [  9   0 380   7]
 [  8   0  38   9]]
01/30/2018 00:18:37 [INFO] exp_shallowmodel: ******************** ghome - Round 28 
01/30/2018 00:18:37 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:18:37 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:18:37 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:18:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:18:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:18:37 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:18:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:19:53 [INFO] exp_shallowmodel: train time: 76.063s
01/30/2018 00:19:53 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:19:53 [INFO] exp_shallowmodel: accuracy:   0.784
01/30/2018 00:19:53 [INFO] exp_shallowmodel: f1_score:   0.364
01/30/2018 00:19:53 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:19:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.15      0.23        59
          C       0.00      0.00      0.00        12
          F       0.82      0.97      0.89       396
          R       0.48      0.25      0.33        55

avg / total       0.73      0.78      0.74       522

01/30/2018 00:19:53 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:19:53 [INFO] exp_shallowmodel: 
[[  9   0  42   8]
 [  1   0  10   1]
 [  2   2 386   6]
 [  6   0  35  14]]
01/30/2018 00:19:58 [INFO] exp_shallowmodel: ******************** ghome - Round 29 
01/30/2018 00:19:58 [INFO] exp_shallowmodel: #(data) = 4176
01/30/2018 00:19:58 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:19:58 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:19:58 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:19:58 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:19:58 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:19:58 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:21:17 [INFO] exp_shallowmodel: train time: 79.342s
01/30/2018 00:21:17 [INFO] exp_shallowmodel: test time:  0.023s
01/30/2018 00:21:17 [INFO] exp_shallowmodel: accuracy:   0.762
01/30/2018 00:21:17 [INFO] exp_shallowmodel: f1_score:   0.387
01/30/2018 00:21:17 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:21:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.31      0.12      0.18        64
          C       0.33      0.07      0.12        14
          F       0.80      0.96      0.88       402
          R       0.55      0.29      0.37        63

avg / total       0.70      0.76      0.72       543

01/30/2018 00:21:17 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:21:17 [INFO] exp_shallowmodel: 
[[  8   1  48   7]
 [  4   1   6   3]
 [  9   1 387   5]
 [  5   0  40  18]]
01/30/2018 00:21:23 [INFO] exp_shallowmodel: ******************** ghome - Round 30 
01/30/2018 00:21:23 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:21:23 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:21:23 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:21:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:21:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:21:23 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:21:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:22:43 [INFO] exp_shallowmodel: train time: 79.561s
01/30/2018 00:22:43 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:22:43 [INFO] exp_shallowmodel: accuracy:   0.762
01/30/2018 00:22:43 [INFO] exp_shallowmodel: f1_score:   0.341
01/30/2018 00:22:43 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:22:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.19      0.27        59
          C       0.00      0.00      0.00        12
          F       0.80      0.95      0.87       396
          R       0.36      0.16      0.22        55

avg / total       0.70      0.76      0.71       522

01/30/2018 00:22:43 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:22:43 [INFO] exp_shallowmodel: 
[[ 11   0  43   5]
 [  2   0  10   0]
 [  6   1 378  11]
 [  4   0  42   9]]
01/30/2018 00:22:48 [INFO] exp_shallowmodel: ******************** ghome - Round 31 
01/30/2018 00:22:48 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:22:48 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:22:48 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:22:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:22:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:22:48 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:22:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:24:09 [INFO] exp_shallowmodel: train time: 80.760s
01/30/2018 00:24:09 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:24:09 [INFO] exp_shallowmodel: accuracy:   0.761
01/30/2018 00:24:09 [INFO] exp_shallowmodel: f1_score:   0.330
01/30/2018 00:24:09 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:24:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.41      0.12      0.18        59
          C       0.00      0.00      0.00        12
          F       0.79      0.96      0.86       396
          R       0.53      0.18      0.27        55

avg / total       0.70      0.76      0.70       522

01/30/2018 00:24:09 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:24:09 [INFO] exp_shallowmodel: 
[[  7   1  48   3]
 [  0   0  12   0]
 [  9   1 380   6]
 [  1   0  44  10]]
01/30/2018 00:24:15 [INFO] exp_shallowmodel: ******************** ghome - Round 32 
01/30/2018 00:24:15 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:24:15 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:24:15 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:24:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:24:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:24:15 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:24:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:25:34 [INFO] exp_shallowmodel: train time: 79.471s
01/30/2018 00:25:34 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:25:34 [INFO] exp_shallowmodel: accuracy:   0.770
01/30/2018 00:25:34 [INFO] exp_shallowmodel: f1_score:   0.390
01/30/2018 00:25:34 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:25:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.47      0.14      0.21        59
          C       1.00      0.08      0.15        12
          F       0.79      0.96      0.87       396
          R       0.54      0.24      0.33        55

avg / total       0.73      0.77      0.72       522

01/30/2018 00:25:34 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:25:34 [INFO] exp_shallowmodel: 
[[  8   0  49   2]
 [  0   1  11   0]
 [  7   0 380   9]
 [  2   0  40  13]]
01/30/2018 00:25:40 [INFO] exp_shallowmodel: ******************** ghome - Round 33 
01/30/2018 00:25:40 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:25:40 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:25:40 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:25:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:25:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:25:40 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:25:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:26:47 [INFO] exp_shallowmodel: train time: 67.541s
01/30/2018 00:26:47 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:26:47 [INFO] exp_shallowmodel: accuracy:   0.778
01/30/2018 00:26:47 [INFO] exp_shallowmodel: f1_score:   0.402
01/30/2018 00:26:47 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:26:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.41      0.15      0.22        59
          C       0.50      0.08      0.14        12
          F       0.81      0.96      0.88       396
          R       0.54      0.27      0.36        55

avg / total       0.73      0.78      0.73       522

01/30/2018 00:26:47 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:26:47 [INFO] exp_shallowmodel: 
[[  9   0  45   5]
 [  1   1   9   1]
 [  7   1 381   7]
 [  5   0  35  15]]
01/30/2018 00:26:52 [INFO] exp_shallowmodel: ******************** ghome - Round 34 
01/30/2018 00:26:52 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:26:52 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:26:52 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:26:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:26:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:26:52 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:26:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:28:15 [INFO] exp_shallowmodel: train time: 82.102s
01/30/2018 00:28:15 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:28:15 [INFO] exp_shallowmodel: accuracy:   0.738
01/30/2018 00:28:15 [INFO] exp_shallowmodel: f1_score:   0.300
01/30/2018 00:28:15 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:28:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.16      0.07      0.10        59
          C       0.33      0.08      0.13        12
          F       0.79      0.95      0.86       396
          R       0.21      0.07      0.11        55

avg / total       0.65      0.74      0.68       522

01/30/2018 00:28:15 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:28:15 [INFO] exp_shallowmodel: 
[[  4   0  49   6]
 [  1   1   9   1]
 [ 11   1 376   8]
 [  9   1  41   4]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/30/2018 00:28:20 [INFO] exp_shallowmodel: ******************** ghome - Round 35 
01/30/2018 00:28:20 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:28:20 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:28:20 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:28:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:28:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:28:20 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:28:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:29:43 [INFO] exp_shallowmodel: train time: 82.631s
01/30/2018 00:29:43 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:29:43 [INFO] exp_shallowmodel: accuracy:   0.784
01/30/2018 00:29:43 [INFO] exp_shallowmodel: f1_score:   0.375
01/30/2018 00:29:43 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:29:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.52      0.20      0.29        59
          C       0.00      0.00      0.00        12
          F       0.81      0.97      0.88       396
          R       0.52      0.24      0.33        55

avg / total       0.73      0.78      0.74       522

01/30/2018 00:29:43 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:29:43 [INFO] exp_shallowmodel: 
[[ 12   0  43   4]
 [  1   0   9   2]
 [  6   0 384   6]
 [  4   1  37  13]]
01/30/2018 00:29:49 [INFO] exp_shallowmodel: ******************** ghome - Round 36 
01/30/2018 00:29:49 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:29:49 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:29:49 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:29:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:29:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:29:49 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:29:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:31:01 [INFO] exp_shallowmodel: train time: 72.052s
01/30/2018 00:31:01 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:31:01 [INFO] exp_shallowmodel: accuracy:   0.776
01/30/2018 00:31:01 [INFO] exp_shallowmodel: f1_score:   0.435
01/30/2018 00:31:01 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:31:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.41      0.19      0.26        59
          C       0.50      0.25      0.33        12
          F       0.83      0.96      0.89       396
          R       0.38      0.20      0.26        55

avg / total       0.72      0.78      0.74       522

01/30/2018 00:31:01 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:31:01 [INFO] exp_shallowmodel: 
[[ 11   1  40   7]
 [  1   3   6   2]
 [  7   0 380   9]
 [  8   2  34  11]]
01/30/2018 00:31:06 [INFO] exp_shallowmodel: ******************** ghome - Round 37 
01/30/2018 00:31:06 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:31:06 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:31:06 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:31:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:31:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:31:06 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:31:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:32:24 [INFO] exp_shallowmodel: train time: 77.235s
01/30/2018 00:32:24 [INFO] exp_shallowmodel: test time:  0.023s
01/30/2018 00:32:24 [INFO] exp_shallowmodel: accuracy:   0.759
01/30/2018 00:32:24 [INFO] exp_shallowmodel: f1_score:   0.376
01/30/2018 00:32:24 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:32:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.42      0.17      0.24        59
          C       1.00      0.08      0.15        12
          F       0.80      0.95      0.87       396
          R       0.37      0.18      0.24        55

avg / total       0.71      0.76      0.71       522

01/30/2018 00:32:24 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:32:24 [INFO] exp_shallowmodel: 
[[ 10   0  44   5]
 [  0   1  10   1]
 [ 10   0 375  11]
 [  4   0  41  10]]
01/30/2018 00:32:29 [INFO] exp_shallowmodel: ******************** ghome - Round 38 
01/30/2018 00:32:29 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:32:29 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:32:29 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:32:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:32:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:32:29 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:32:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:33:43 [INFO] exp_shallowmodel: train time: 73.481s
01/30/2018 00:33:43 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:33:43 [INFO] exp_shallowmodel: accuracy:   0.770
01/30/2018 00:33:43 [INFO] exp_shallowmodel: f1_score:   0.353
01/30/2018 00:33:43 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:33:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.17      0.25        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.41      0.22      0.29        55

avg / total       0.71      0.77      0.72       522

01/30/2018 00:33:43 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:33:43 [INFO] exp_shallowmodel: 
[[ 10   0  42   7]
 [  1   0  11   0]
 [  4   2 380  10]
 [  6   0  37  12]]
01/30/2018 00:33:48 [INFO] exp_shallowmodel: ******************** ghome - Round 39 
01/30/2018 00:33:48 [INFO] exp_shallowmodel: #(data) = 4176
01/30/2018 00:33:48 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:33:48 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:33:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:33:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:33:48 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:33:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:34:54 [INFO] exp_shallowmodel: train time: 65.973s
01/30/2018 00:34:54 [INFO] exp_shallowmodel: test time:  0.023s
01/30/2018 00:34:54 [INFO] exp_shallowmodel: accuracy:   0.757
01/30/2018 00:34:54 [INFO] exp_shallowmodel: f1_score:   0.361
01/30/2018 00:34:54 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:34:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.36      0.12      0.19        64
          C       1.00      0.07      0.13        14
          F       0.79      0.97      0.87       402
          R       0.48      0.17      0.26        63

avg / total       0.71      0.76      0.70       543

01/30/2018 00:34:54 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:34:54 [INFO] exp_shallowmodel: 
[[  8   0  51   5]
 [  3   1   9   1]
 [  5   0 391   6]
 [  6   0  46  11]]
01/30/2018 00:35:00 [INFO] exp_shallowmodel: ******************** ghome - Round 40 
01/30/2018 00:35:00 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:35:00 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:35:00 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:35:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:35:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:35:00 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:35:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:36:20 [INFO] exp_shallowmodel: train time: 80.187s
01/30/2018 00:36:20 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:36:20 [INFO] exp_shallowmodel: accuracy:   0.759
01/30/2018 00:36:20 [INFO] exp_shallowmodel: f1_score:   0.365
01/30/2018 00:36:20 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:36:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.44      0.19      0.26        59
          C       1.00      0.08      0.15        12
          F       0.80      0.95      0.87       396
          R       0.27      0.13      0.17        55

avg / total       0.71      0.76      0.71       522

01/30/2018 00:36:20 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:36:20 [INFO] exp_shallowmodel: 
[[ 11   0  44   4]
 [  2   1   7   2]
 [  6   0 377  13]
 [  6   0  42   7]]
01/30/2018 00:36:26 [INFO] exp_shallowmodel: ******************** ghome - Round 41 
01/30/2018 00:36:26 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:36:26 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:36:26 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:36:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:36:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:36:26 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:36:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:37:48 [INFO] exp_shallowmodel: train time: 82.003s
01/30/2018 00:37:48 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:37:48 [INFO] exp_shallowmodel: accuracy:   0.778
01/30/2018 00:37:48 [INFO] exp_shallowmodel: f1_score:   0.394
01/30/2018 00:37:48 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:37:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.47      0.14      0.21        59
          C       1.00      0.08      0.15        12
          F       0.81      0.97      0.88       396
          R       0.48      0.25      0.33        55

avg / total       0.74      0.78      0.73       522

01/30/2018 00:37:48 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:37:48 [INFO] exp_shallowmodel: 
[[  8   0  47   4]
 [  0   1   8   3]
 [  5   0 383   8]
 [  4   0  37  14]]
01/30/2018 00:37:53 [INFO] exp_shallowmodel: ******************** ghome - Round 42 
01/30/2018 00:37:53 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:37:53 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:37:53 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:37:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:37:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:37:53 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:37:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:39:13 [INFO] exp_shallowmodel: train time: 79.546s
01/30/2018 00:39:13 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:39:13 [INFO] exp_shallowmodel: accuracy:   0.774
01/30/2018 00:39:13 [INFO] exp_shallowmodel: f1_score:   0.414
01/30/2018 00:39:13 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:39:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.20      0.29        59
          C       1.00      0.08      0.15        12
          F       0.81      0.95      0.88       396
          R       0.45      0.27      0.34        55

avg / total       0.74      0.77      0.74       522

01/30/2018 00:39:13 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:39:13 [INFO] exp_shallowmodel: 
[[ 12   0  40   7]
 [  1   1   9   1]
 [ 10   0 376  10]
 [  2   0  38  15]]
01/30/2018 00:39:18 [INFO] exp_shallowmodel: ******************** ghome - Round 43 
01/30/2018 00:39:18 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:39:18 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:39:18 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:39:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:39:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:39:18 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:39:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:40:47 [INFO] exp_shallowmodel: train time: 88.435s
01/30/2018 00:40:47 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:40:47 [INFO] exp_shallowmodel: accuracy:   0.770
01/30/2018 00:40:47 [INFO] exp_shallowmodel: f1_score:   0.344
01/30/2018 00:40:47 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:40:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.17      0.24        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.43      0.18      0.26        55

avg / total       0.71      0.77      0.72       522

01/30/2018 00:40:47 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:40:47 [INFO] exp_shallowmodel: 
[[ 10   0  43   6]
 [  2   0   8   2]
 [  6   3 382   5]
 [  7   1  37  10]]
01/30/2018 00:40:52 [INFO] exp_shallowmodel: ******************** ghome - Round 44 
01/30/2018 00:40:52 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:40:52 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:40:52 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:40:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:40:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:40:52 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:40:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:42:08 [INFO] exp_shallowmodel: train time: 76.229s
01/30/2018 00:42:08 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:42:08 [INFO] exp_shallowmodel: accuracy:   0.778
01/30/2018 00:42:08 [INFO] exp_shallowmodel: f1_score:   0.363
01/30/2018 00:42:08 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:42:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.61      0.19      0.29        59
          C       0.00      0.00      0.00        12
          F       0.81      0.97      0.88       396
          R       0.41      0.22      0.29        55

avg / total       0.73      0.78      0.73       522

01/30/2018 00:42:08 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:42:08 [INFO] exp_shallowmodel: 
[[ 11   0  40   8]
 [  1   0  10   1]
 [  3   2 383   8]
 [  3   0  40  12]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/30/2018 00:42:14 [INFO] exp_shallowmodel: ******************** ghome - Round 45 
01/30/2018 00:42:14 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:42:14 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:42:14 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:42:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:42:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:42:14 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:42:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:43:25 [INFO] exp_shallowmodel: train time: 71.381s
01/30/2018 00:43:25 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:43:25 [INFO] exp_shallowmodel: accuracy:   0.762
01/30/2018 00:43:25 [INFO] exp_shallowmodel: f1_score:   0.343
01/30/2018 00:43:25 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:43:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.07      0.10        59
          C       0.50      0.08      0.14        12
          F       0.80      0.97      0.88       396
          R       0.38      0.18      0.25        55

avg / total       0.69      0.76      0.71       522

01/30/2018 00:43:25 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:43:25 [INFO] exp_shallowmodel: 
[[  4   0  47   8]
 [  2   1   7   2]
 [  6   1 383   6]
 [  6   0  39  10]]
01/30/2018 00:43:31 [INFO] exp_shallowmodel: ******************** ghome - Round 46 
01/30/2018 00:43:31 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:43:31 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:43:31 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:43:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:43:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:43:31 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:43:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:44:54 [INFO] exp_shallowmodel: train time: 83.447s
01/30/2018 00:44:54 [INFO] exp_shallowmodel: test time:  0.022s
01/30/2018 00:44:54 [INFO] exp_shallowmodel: accuracy:   0.764
01/30/2018 00:44:54 [INFO] exp_shallowmodel: f1_score:   0.371
01/30/2018 00:44:54 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:44:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.20      0.29        59
          C       0.50      0.08      0.14        12
          F       0.80      0.96      0.87       396
          R       0.35      0.13      0.19        55

avg / total       0.71      0.76      0.72       522

01/30/2018 00:44:54 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:44:54 [INFO] exp_shallowmodel: 
[[ 12   0  44   3]
 [  1   1   9   1]
 [  7   1 379   9]
 [  5   0  43   7]]
01/30/2018 00:45:00 [INFO] exp_shallowmodel: ******************** ghome - Round 47 
01/30/2018 00:45:00 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:45:00 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:45:00 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:45:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:45:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:45:00 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:45:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:46:16 [INFO] exp_shallowmodel: train time: 75.879s
01/30/2018 00:46:16 [INFO] exp_shallowmodel: test time:  0.023s
01/30/2018 00:46:16 [INFO] exp_shallowmodel: accuracy:   0.761
01/30/2018 00:46:16 [INFO] exp_shallowmodel: f1_score:   0.324
01/30/2018 00:46:16 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:46:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.35      0.15      0.21        59
          C       0.00      0.00      0.00        12
          F       0.80      0.96      0.87       396
          R       0.38      0.15      0.21        55

avg / total       0.69      0.76      0.71       522

01/30/2018 00:46:16 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:46:16 [INFO] exp_shallowmodel: 
[[  9   0  44   6]
 [  1   0  10   1]
 [ 10   0 380   6]
 [  6   0  41   8]]
01/30/2018 00:46:21 [INFO] exp_shallowmodel: ******************** ghome - Round 48 
01/30/2018 00:46:21 [INFO] exp_shallowmodel: #(data) = 4197
01/30/2018 00:46:21 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:46:21 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:46:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:46:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:46:21 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:46:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:47:27 [INFO] exp_shallowmodel: train time: 65.803s
01/30/2018 00:47:27 [INFO] exp_shallowmodel: test time:  0.021s
01/30/2018 00:47:27 [INFO] exp_shallowmodel: accuracy:   0.766
01/30/2018 00:47:27 [INFO] exp_shallowmodel: f1_score:   0.353
01/30/2018 00:47:27 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:47:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.19      0.27        59
          C       0.00      0.00      0.00        12
          F       0.80      0.95      0.87       396
          R       0.41      0.20      0.27        55

avg / total       0.71      0.77      0.72       522

01/30/2018 00:47:27 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:47:27 [INFO] exp_shallowmodel: 
[[ 11   0  44   4]
 [  1   0  11   0]
 [  4   2 378  12]
 [  6   0  38  11]]
01/30/2018 00:47:32 [INFO] exp_shallowmodel: ******************** ghome - Round 49 
01/30/2018 00:47:32 [INFO] exp_shallowmodel: #(data) = 4176
01/30/2018 00:47:32 [INFO] exp_shallowmodel: #(feature) = 29589
01/30/2018 00:47:32 [INFO] exp_shallowmodel: ================================================================================
01/30/2018 00:47:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/30/2018 00:47:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/30/2018 00:47:32 [INFO] exp_shallowmodel: Training: 
01/30/2018 00:47:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/30/2018 00:48:24 [INFO] exp_shallowmodel: train time: 51.578s
01/30/2018 00:48:24 [INFO] exp_shallowmodel: test time:  0.023s
01/30/2018 00:48:24 [INFO] exp_shallowmodel: accuracy:   0.755
01/30/2018 00:48:24 [INFO] exp_shallowmodel: f1_score:   0.355
01/30/2018 00:48:24 [INFO] exp_shallowmodel: classification report:
01/30/2018 00:48:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.44      0.17      0.25        64
          C       0.00      0.00      0.00        14
          F       0.79      0.96      0.86       402
          R       0.50      0.22      0.31        63

avg / total       0.69      0.76      0.70       543

01/30/2018 00:48:24 [INFO] exp_shallowmodel: confusion matrix:
01/30/2018 00:48:24 [INFO] exp_shallowmodel: 
[[ 11   0  50   3]
 [  4   0   9   1]
 [  7   0 385  10]
 [  3   0  46  14]]
Done: 20180130-004832
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
