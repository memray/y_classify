/ihome/pbrusilosky/rum20/packages/anaconda3/bin/python -m dialogue.classify.task_runner -experiment_mode cross_validation -selected_feature_set_id 6 -add_similarity_feature -selected_context_id 2
No. of param settings = 1
[('experiment_mode', 'cross_validation'), ('deep_model', False), ('selected_context_id', 2), ('selected_feature_set_id', 6), ('similarity_feature', True), ('k_feature_to_keep', -1), ('k_component_for_pca', -1)]
01/22/2018 21:32:36 [INFO] configuration: experiment_mode  :   cross_validation
01/22/2018 21:32:36 [INFO] configuration: deep_model  :   False
01/22/2018 21:32:36 [INFO] configuration: selected_context_id  :   2
01/22/2018 21:32:36 [INFO] configuration: selected_feature_set_id  :   6
01/22/2018 21:32:36 [INFO] configuration: similarity_feature  :   True
01/22/2018 21:32:36 [INFO] configuration: k_feature_to_keep  :   -1
01/22/2018 21:32:36 [INFO] configuration: k_component_for_pca  :   -1
01/22/2018 21:32:36 [INFO] configuration: seed  :   154316847
01/22/2018 21:32:36 [INFO] configuration: root_path  :   /ihome/pbrusilosky/rum20/y_classify
01/22/2018 21:32:36 [INFO] configuration: task_name  :   utterance_type
01/22/2018 21:32:36 [INFO] configuration: timemark  :   20180122-213236
01/22/2018 21:32:36 [INFO] configuration: context_set  :   last
01/22/2018 21:32:36 [INFO] configuration: utterance_names  :   ['last_user_utterance', 'last_system_utterance', 'current_user_utterance', 'next_system_utterance', 'next_user_utterance']
01/22/2018 21:32:36 [INFO] configuration: utterance_range  :   ['current_user_utterance', 'last_system_utterance', 'current_user_utterance']
01/22/2018 21:32:36 [INFO] configuration: feature_set  :   6-w2v
01/22/2018 21:32:36 [INFO] configuration: feature_set_number  :   ['9']
01/22/2018 21:32:36 [INFO] configuration: experiment_name  :   20180122-213236.cross_validation.context=last.feature=6-w2v.similarity=true
01/22/2018 21:32:36 [INFO] configuration: experiment_path  :   /ihome/pbrusilosky/rum20/y_classify/output/20180122-213236.cross_validation.context=last.feature=6-w2v.similarity=true
01/22/2018 21:32:36 [INFO] configuration: log_path  :   /ihome/pbrusilosky/rum20/y_classify/output/20180122-213236.cross_validation.context=last.feature=6-w2v.similarity=true/output.log
01/22/2018 21:32:36 [INFO] configuration: valid_type  :   {'F', 'A', 'C', 'R'}
01/22/2018 21:32:36 [INFO] configuration: data_name  :   
01/22/2018 21:32:36 [INFO] configuration: data_names  :   ['dstc2', 'dstc3', 'family', 'ghome']
01/22/2018 21:32:36 [INFO] configuration: raw_feature_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.raw_feature.pkl
01/22/2018 21:32:36 [INFO] configuration: extracted_feature_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.extracted_feature.pkl
01/22/2018 21:32:36 [INFO] configuration: pipeline_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.pipeline.pkl
01/22/2018 21:32:36 [INFO] configuration: metrics  :   ['accuracy', 'precision', 'recall', 'f1_score', 'training_time', 'test_time']
01/22/2018 21:32:36 [INFO] configuration: do_cross_validation  :   True
01/22/2018 21:32:36 [INFO] configuration: #division  :   5
01/22/2018 21:32:36 [INFO] configuration: #cross_validation  :   10
01/22/2018 21:32:36 [INFO] configuration: cv_index_cache_path  :   
01/22/2018 21:32:36 [INFO] configuration: action_words  :   {'timer', 'video', 'part', 'price', 'tell', 'south', 'skip', 'else', 'clear', 'post', 'moder', 'temperatur', 'discard', 'start', 'delet', 'moderate', 'telephon', 'delete', 'telephone', 'music', 'north', 'time', 'next', 'reminder', 'reminders', 'play', 'cast', 'remov', 'shuffle', 'snooz', 'findcare', 'ani', 'alarm', 'reminds', 'room', 'volum', 'show', 'song', 'shuffl', 'light', 'watch', 'expens', 'remind', 'expensive', 'add', 'snooze', 'address', 'food', 'share', 'matter', 'turn', 'els', 'item', 'remove', 'centr', 'temperature', 'any', 'member', 'stop', 'number', 'findcar', 'help', 'centre', 'items', 'volume', 'cheap', 'list', 'phone', 'weather', 'area'}
01/22/2018 21:32:36 [INFO] configuration: corenlp_jars  :   ('/Users/memray/Project/stanford/stanford-corenlp-full-3.8.0/*', '/Users/memray/Project/stanford/stanford-corenlp-full-3.8.0/stanford-english-kbp-corenlp-2017-06-09-models.jar')
01/22/2018 21:32:36 [INFO] configuration: lda_topic_number  :   50
01/22/2018 21:32:36 [INFO] configuration: lda_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.topic=50.lda.pkl
01/22/2018 21:32:36 [INFO] configuration: gensim_corpus_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.corpus.pkl
01/22/2018 21:32:36 [INFO] configuration: gensim_dict_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.dict
01/22/2018 21:32:36 [INFO] configuration: w2v_path  :   /home/memray/Data/glove/GoogleNews-vectors-negative300.bin
01/22/2018 21:32:36 [INFO] configuration: w2v_vector_length  :   300
01/22/2018 21:32:36 [INFO] configuration: d2v_vector_length  :   300
01/22/2018 21:32:36 [INFO] configuration: d2v_window_size  :   5
01/22/2018 21:32:36 [INFO] configuration: d2v_min_count  :   2
01/22/2018 21:32:36 [INFO] configuration: d2v_model_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.doc2vec.dim=300.window=5.min_count=2.model
01/22/2018 21:32:36 [INFO] configuration: d2v_vector_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.doc2vec.dim=300.window=5.min_count=2.vector
01/22/2018 21:32:36 [INFO] configuration: num_word_keep  :   {'dstc2': 300, 'dstc3': 300, 'family': 1000, 'ghome': 1000}
01/22/2018 21:32:36 [INFO] configuration: batch_size  :   128
01/22/2018 21:32:36 [INFO] configuration: max_epoch  :   50
01/22/2018 21:32:36 [INFO] configuration: early_stop_tolerance  :   2
01/22/2018 21:32:36 [INFO] configuration: concat_sents  :   False
01/22/2018 21:32:36 [INFO] configuration: cnn_setting  :   {'model': 'multichannel', 'early_stopping': True, 'word_dim': 300, 'filters': [3, 4, 5], 'filter_num': [100, 100, 100], 'class_size': 4, 'batch_size': 128, 'learning_rate': 0.001, 'norm_limit': 10, 'dropout_prob': 0.0, 'sentence_num': 3}
01/22/2018 21:32:36 [INFO] configuration: skipthought_setting  :   {'skipthought_model_path': '/Users/memray/Data/skip-thought', 'skipthought_data_path': '/ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.skip-thought.biskip.vector', 'fixed_emb': True, 'sentence_num': 3, 'hidden_size': 2400, 'class_size': 4, 'learning_rate': 0.0001, 'norm_limit': 3, 'dropout_prob': 0.5}
01/22/2018 21:32:36 [INFO] configuration: lstm_setting  :   {'model': 'non-static', 'hidden_size': 32, 'embedding_size': 300, 'num_layers': 1, 'bidirectional': False, 'learning_rate': 0.001, 'class_size': 4, 'norm_limit': 2, 'clip_grad_norm': 2, 'dropout_prob': 0.1}
01/22/2018 21:32:38 [INFO] exp_shallowmodel: ******************** dstc2 - Round 0 
01/22/2018 21:32:38 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:32:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:32:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:32:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:32:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:32:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:32:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:33:23 [INFO] exp_shallowmodel: train time: 44.228s
01/22/2018 21:33:23 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:33:23 [INFO] exp_shallowmodel: accuracy:   0.664
01/22/2018 21:33:23 [INFO] exp_shallowmodel: f1_score:   0.498
01/22/2018 21:33:23 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:33:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.63      0.72      0.67       164
          F       0.75      0.79      0.77       268
          R       0.51      0.39      0.44       125

avg / total       0.65      0.66      0.65       571

01/22/2018 21:33:23 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:33:23 [INFO] exp_shallowmodel: 
[[  1   3   4   6]
 [  0 118  25  21]
 [  1  36 211  20]
 [  2  31  43  49]]
01/22/2018 21:33:23 [INFO] exp_shallowmodel: ******************** dstc2 - Round 1 
01/22/2018 21:33:23 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:33:23 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:33:23 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:33:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:33:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:33:23 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:33:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:34:06 [INFO] exp_shallowmodel: train time: 42.930s
01/22/2018 21:34:06 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:34:06 [INFO] exp_shallowmodel: accuracy:   0.613
01/22/2018 21:34:06 [INFO] exp_shallowmodel: f1_score:   0.463
01/22/2018 21:34:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:34:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.07      0.12        14
          C       0.57      0.66      0.61       164
          F       0.69      0.73      0.71       268
          R       0.47      0.37      0.41       125

avg / total       0.60      0.61      0.60       571

01/22/2018 21:34:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:34:06 [INFO] exp_shallowmodel: 
[[  1   1   6   6]
 [  0 108  30  26]
 [  0  54 195  19]
 [  2  27  50  46]]
01/22/2018 21:34:06 [INFO] exp_shallowmodel: ******************** dstc2 - Round 2 
01/22/2018 21:34:06 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:34:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:34:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:34:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:34:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:34:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:34:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:34:48 [INFO] exp_shallowmodel: train time: 42.478s
01/22/2018 21:34:48 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:34:48 [INFO] exp_shallowmodel: accuracy:   0.630
01/22/2018 21:34:48 [INFO] exp_shallowmodel: f1_score:   0.473
01/22/2018 21:34:48 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:34:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.58      0.67      0.62       164
          F       0.74      0.75      0.74       268
          R       0.47      0.39      0.43       125

avg / total       0.62      0.63      0.62       571

01/22/2018 21:34:48 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:34:48 [INFO] exp_shallowmodel: 
[[  1   2   5   6]
 [  2 110  27  25]
 [  1  43 200  24]
 [  2  36  38  49]]
01/22/2018 21:34:49 [INFO] exp_shallowmodel: ******************** dstc2 - Round 3 
01/22/2018 21:34:49 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:34:49 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:34:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:34:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:34:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:34:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:34:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:35:34 [INFO] exp_shallowmodel: train time: 45.520s
01/22/2018 21:35:34 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:35:34 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 21:35:34 [INFO] exp_shallowmodel: f1_score:   0.429
01/22/2018 21:35:34 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:35:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.68      0.63       164
          F       0.68      0.74      0.71       268
          R       0.46      0.31      0.37       125

avg / total       0.59      0.61      0.60       571

01/22/2018 21:35:34 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:35:34 [INFO] exp_shallowmodel: 
[[  0   1   8   5]
 [  1 111  33  19]
 [  4  43 199  22]
 [  2  32  52  39]]
01/22/2018 21:35:34 [INFO] exp_shallowmodel: ******************** dstc2 - Round 4 
01/22/2018 21:35:34 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:35:34 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:35:34 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:35:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:35:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:35:34 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:35:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:36:17 [INFO] exp_shallowmodel: train time: 42.785s
01/22/2018 21:36:17 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:36:17 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 21:36:17 [INFO] exp_shallowmodel: f1_score:   0.467
01/22/2018 21:36:17 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:36:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.07      0.11        14
          C       0.57      0.71      0.63       164
          F       0.70      0.72      0.71       268
          R       0.50      0.36      0.42       125

avg / total       0.61      0.62      0.61       571

01/22/2018 21:36:17 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:36:17 [INFO] exp_shallowmodel: 
[[  1   2   8   3]
 [  3 116  29  16]
 [  0  49 193  26]
 [  1  35  44  45]]
01/22/2018 21:36:17 [INFO] exp_shallowmodel: ******************** dstc2 - Round 5 
01/22/2018 21:36:17 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:36:17 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:36:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:36:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:36:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:36:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:36:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:37:02 [INFO] exp_shallowmodel: train time: 44.998s
01/22/2018 21:37:02 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:37:02 [INFO] exp_shallowmodel: accuracy:   0.585
01/22/2018 21:37:02 [INFO] exp_shallowmodel: f1_score:   0.408
01/22/2018 21:37:02 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:37:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.53      0.59      0.56       164
          F       0.68      0.74      0.71       268
          R       0.44      0.32      0.37       125

avg / total       0.57      0.58      0.57       571

01/22/2018 21:37:02 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:37:02 [INFO] exp_shallowmodel: 
[[  0   5   6   3]
 [  1  97  44  22]
 [  4  41 197  26]
 [  1  41  43  40]]
01/22/2018 21:37:02 [INFO] exp_shallowmodel: ******************** dstc2 - Round 6 
01/22/2018 21:37:02 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:37:02 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:37:02 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:37:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:37:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:37:02 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:37:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:37:45 [INFO] exp_shallowmodel: train time: 42.832s
01/22/2018 21:37:45 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:37:45 [INFO] exp_shallowmodel: accuracy:   0.667
01/22/2018 21:37:45 [INFO] exp_shallowmodel: f1_score:   0.464
01/22/2018 21:37:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:37:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.63      0.76      0.69       164
          F       0.74      0.81      0.77       268
          R       0.51      0.32      0.39       125

avg / total       0.64      0.67      0.65       571

01/22/2018 21:37:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:37:45 [INFO] exp_shallowmodel: 
[[  0   3   7   4]
 [  0 125  25  14]
 [  1  30 216  21]
 [  3  39  43  40]]
01/22/2018 21:37:45 [INFO] exp_shallowmodel: ******************** dstc2 - Round 7 
01/22/2018 21:37:45 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:37:45 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:37:45 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:37:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:37:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:37:45 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:37:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:38:28 [INFO] exp_shallowmodel: train time: 42.891s
01/22/2018 21:38:28 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:38:28 [INFO] exp_shallowmodel: accuracy:   0.604
01/22/2018 21:38:28 [INFO] exp_shallowmodel: f1_score:   0.449
01/22/2018 21:38:28 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:38:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.07      0.12        14
          C       0.57      0.65      0.61       164
          F       0.69      0.75      0.72       268
          R       0.40      0.30      0.35       125

avg / total       0.59      0.60      0.59       571

01/22/2018 21:38:28 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:38:28 [INFO] exp_shallowmodel: 
[[  1   3   6   4]
 [  0 106  35  23]
 [  0  38 200  30]
 [  1  38  48  38]]
01/22/2018 21:38:29 [INFO] exp_shallowmodel: ******************** dstc2 - Round 8 
01/22/2018 21:38:29 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:38:29 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:38:29 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:38:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:38:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:38:29 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:38:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:39:12 [INFO] exp_shallowmodel: train time: 43.965s
01/22/2018 21:39:12 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:39:12 [INFO] exp_shallowmodel: accuracy:   0.585
01/22/2018 21:39:12 [INFO] exp_shallowmodel: f1_score:   0.451
01/22/2018 21:39:12 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:39:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.14      0.17        14
          C       0.54      0.71      0.61       164
          F       0.71      0.67      0.69       268
          R       0.39      0.28      0.33       125

avg / total       0.58      0.58      0.58       571

01/22/2018 21:39:12 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:39:12 [INFO] exp_shallowmodel: 
[[  2   1   6   5]
 [  1 117  24  22]
 [  2  58 180  28]
 [  4  41  45  35]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 21:39:13 [INFO] exp_shallowmodel: ******************** dstc2 - Round 9 
01/22/2018 21:39:13 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 21:39:13 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:39:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:39:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:39:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:39:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:39:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:39:54 [INFO] exp_shallowmodel: train time: 41.826s
01/22/2018 21:39:54 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:39:54 [INFO] exp_shallowmodel: accuracy:   0.614
01/22/2018 21:39:54 [INFO] exp_shallowmodel: f1_score:   0.454
01/22/2018 21:39:54 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:39:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.06      0.09        16
          C       0.57      0.69      0.62       169
          F       0.69      0.74      0.71       271
          R       0.51      0.32      0.39       130

avg / total       0.60      0.61      0.60       586

01/22/2018 21:39:54 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:39:54 [INFO] exp_shallowmodel: 
[[  1   4   7   4]
 [  2 116  34  17]
 [  2  48 201  20]
 [  2  36  50  42]]
01/22/2018 21:39:55 [INFO] exp_shallowmodel: ******************** dstc2 - Round 10 
01/22/2018 21:39:55 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:39:55 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:39:55 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:39:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:39:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:39:55 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:39:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:40:39 [INFO] exp_shallowmodel: train time: 44.698s
01/22/2018 21:40:39 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:40:39 [INFO] exp_shallowmodel: accuracy:   0.599
01/22/2018 21:40:39 [INFO] exp_shallowmodel: f1_score:   0.421
01/22/2018 21:40:39 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:40:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.68      0.62       164
          F       0.69      0.71      0.70       268
          R       0.43      0.32      0.37       125

avg / total       0.58      0.60      0.59       571

01/22/2018 21:40:39 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:40:39 [INFO] exp_shallowmodel: 
[[  0   2   7   5]
 [  2 111  36  15]
 [  1  44 191  32]
 [  2  39  44  40]]
01/22/2018 21:40:39 [INFO] exp_shallowmodel: ******************** dstc2 - Round 11 
01/22/2018 21:40:39 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:40:39 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:40:39 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:40:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:40:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:40:39 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:40:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:41:21 [INFO] exp_shallowmodel: train time: 41.115s
01/22/2018 21:41:21 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:41:21 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 21:41:21 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 21:41:21 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:41:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.65      0.60       164
          F       0.71      0.74      0.72       268
          R       0.47      0.38      0.42       125

avg / total       0.60      0.61      0.60       571

01/22/2018 21:41:21 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:41:21 [INFO] exp_shallowmodel: 
[[  0   2   8   4]
 [  2 106  30  26]
 [  0  47 198  23]
 [  3  32  43  47]]
01/22/2018 21:41:21 [INFO] exp_shallowmodel: ******************** dstc2 - Round 12 
01/22/2018 21:41:21 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:41:21 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:41:21 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:41:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:41:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:41:21 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:41:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:41:59 [INFO] exp_shallowmodel: train time: 38.354s
01/22/2018 21:41:59 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:41:59 [INFO] exp_shallowmodel: accuracy:   0.585
01/22/2018 21:41:59 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 21:41:59 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:41:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.21        14
          C       0.53      0.60      0.56       164
          F       0.68      0.72      0.70       268
          R       0.43      0.32      0.37       125

avg / total       0.57      0.58      0.57       571

01/22/2018 21:41:59 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:41:59 [INFO] exp_shallowmodel: 
[[  2   1   9   2]
 [  0  98  41  25]
 [  1  46 194  27]
 [  2  40  43  40]]
01/22/2018 21:41:59 [INFO] exp_shallowmodel: ******************** dstc2 - Round 13 
01/22/2018 21:41:59 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:41:59 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:41:59 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:41:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:41:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:41:59 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:41:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:42:40 [INFO] exp_shallowmodel: train time: 40.591s
01/22/2018 21:42:40 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:42:40 [INFO] exp_shallowmodel: accuracy:   0.627
01/22/2018 21:42:40 [INFO] exp_shallowmodel: f1_score:   0.451
01/22/2018 21:42:40 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:42:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.71      0.64       164
          F       0.70      0.72      0.71       268
          R       0.53      0.38      0.45       125

avg / total       0.62      0.63      0.62       571

01/22/2018 21:42:40 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:42:40 [INFO] exp_shallowmodel: 
[[  0   2   5   7]
 [  2 116  32  14]
 [  4  49 194  21]
 [  2  30  45  48]]
01/22/2018 21:42:40 [INFO] exp_shallowmodel: ******************** dstc2 - Round 14 
01/22/2018 21:42:40 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:42:40 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:42:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:42:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:42:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:42:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:42:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:43:22 [INFO] exp_shallowmodel: train time: 41.636s
01/22/2018 21:43:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:43:22 [INFO] exp_shallowmodel: accuracy:   0.620
01/22/2018 21:43:22 [INFO] exp_shallowmodel: f1_score:   0.458
01/22/2018 21:43:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:43:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.09      0.07      0.08        14
          C       0.59      0.66      0.62       164
          F       0.73      0.75      0.74       268
          R       0.44      0.35      0.39       125

avg / total       0.61      0.62      0.61       571

01/22/2018 21:43:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:43:22 [INFO] exp_shallowmodel: 
[[  1   4   8   1]
 [  3 109  28  24]
 [  2  36 200  30]
 [  5  37  39  44]]
01/22/2018 21:43:22 [INFO] exp_shallowmodel: ******************** dstc2 - Round 15 
01/22/2018 21:43:22 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:43:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:43:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:43:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:43:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:43:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:43:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:44:06 [INFO] exp_shallowmodel: train time: 43.941s
01/22/2018 21:44:06 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:44:06 [INFO] exp_shallowmodel: accuracy:   0.620
01/22/2018 21:44:06 [INFO] exp_shallowmodel: f1_score:   0.465
01/22/2018 21:44:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:44:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.56      0.68      0.61       164
          F       0.71      0.73      0.72       268
          R       0.51      0.37      0.43       125

avg / total       0.61      0.62      0.61       571

01/22/2018 21:44:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:44:06 [INFO] exp_shallowmodel: 
[[  1   1   7   5]
 [  2 111  33  18]
 [  2  48 196  22]
 [  1  37  41  46]]
01/22/2018 21:44:06 [INFO] exp_shallowmodel: ******************** dstc2 - Round 16 
01/22/2018 21:44:06 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:44:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:44:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:44:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:44:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:44:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:44:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:44:47 [INFO] exp_shallowmodel: train time: 40.797s
01/22/2018 21:44:47 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:44:47 [INFO] exp_shallowmodel: accuracy:   0.658
01/22/2018 21:44:47 [INFO] exp_shallowmodel: f1_score:   0.467
01/22/2018 21:44:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:44:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.63      0.76      0.69       164
          F       0.74      0.76      0.75       268
          R       0.50      0.37      0.42       125

avg / total       0.64      0.66      0.64       571

01/22/2018 21:44:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:44:47 [INFO] exp_shallowmodel: 
[[  0   2   7   5]
 [  2 125  17  20]
 [  2  40 205  21]
 [  0  31  48  46]]
01/22/2018 21:44:47 [INFO] exp_shallowmodel: ******************** dstc2 - Round 17 
01/22/2018 21:44:47 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:44:47 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:44:47 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:44:47 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:44:47 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:44:47 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:44:47 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:45:29 [INFO] exp_shallowmodel: train time: 42.372s
01/22/2018 21:45:29 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:45:29 [INFO] exp_shallowmodel: accuracy:   0.597
01/22/2018 21:45:29 [INFO] exp_shallowmodel: f1_score:   0.417
01/22/2018 21:45:29 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:45:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.63      0.60       164
          F       0.68      0.75      0.71       268
          R       0.44      0.30      0.36       125

avg / total       0.58      0.60      0.59       571

01/22/2018 21:45:29 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:45:29 [INFO] exp_shallowmodel: 
[[  0   3   7   4]
 [  2 103  35  24]
 [  2  45 200  21]
 [  7  30  50  38]]
01/22/2018 21:45:29 [INFO] exp_shallowmodel: ******************** dstc2 - Round 18 
01/22/2018 21:45:29 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:45:29 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:45:29 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:45:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:45:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:45:29 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:45:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:46:13 [INFO] exp_shallowmodel: train time: 43.528s
01/22/2018 21:46:13 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:46:13 [INFO] exp_shallowmodel: accuracy:   0.618
01/22/2018 21:46:13 [INFO] exp_shallowmodel: f1_score:   0.448
01/22/2018 21:46:13 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:46:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.07      0.10        14
          C       0.54      0.67      0.60       164
          F       0.72      0.77      0.74       268
          R       0.47      0.28      0.35       125

avg / total       0.60      0.62      0.60       571

01/22/2018 21:46:13 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:46:13 [INFO] exp_shallowmodel: 
[[  1   2   7   4]
 [  1 110  30  23]
 [  2  47 207  12]
 [  3  43  44  35]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 21:46:13 [INFO] exp_shallowmodel: ******************** dstc2 - Round 19 
01/22/2018 21:46:13 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 21:46:13 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:46:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:46:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:46:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:46:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:46:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:46:57 [INFO] exp_shallowmodel: train time: 43.582s
01/22/2018 21:46:57 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:46:57 [INFO] exp_shallowmodel: accuracy:   0.599
01/22/2018 21:46:57 [INFO] exp_shallowmodel: f1_score:   0.421
01/22/2018 21:46:57 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:46:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.60      0.67      0.63       169
          F       0.68      0.73      0.70       271
          R       0.41      0.30      0.35       130

avg / total       0.58      0.60      0.58       586

01/22/2018 21:46:57 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:46:57 [INFO] exp_shallowmodel: 
[[  0   4   5   7]
 [  1 114  30  24]
 [  4  44 198  25]
 [  3  29  59  39]]
01/22/2018 21:46:57 [INFO] exp_shallowmodel: ******************** dstc2 - Round 20 
01/22/2018 21:46:57 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:46:57 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:46:57 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:46:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:46:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:46:57 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:46:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:47:39 [INFO] exp_shallowmodel: train time: 42.124s
01/22/2018 21:47:39 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:47:39 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 21:47:39 [INFO] exp_shallowmodel: f1_score:   0.434
01/22/2018 21:47:39 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:47:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.63      0.60       164
          F       0.70      0.74      0.72       268
          R       0.48      0.38      0.42       125

avg / total       0.60      0.61      0.60       571

01/22/2018 21:47:39 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:47:39 [INFO] exp_shallowmodel: 
[[  0   3   7   4]
 [  3 103  35  23]
 [  1  45 199  23]
 [  2  31  45  47]]
01/22/2018 21:47:39 [INFO] exp_shallowmodel: ******************** dstc2 - Round 21 
01/22/2018 21:47:39 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:47:39 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:47:39 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:47:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:47:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:47:39 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:47:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:48:22 [INFO] exp_shallowmodel: train time: 42.766s
01/22/2018 21:48:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:48:22 [INFO] exp_shallowmodel: accuracy:   0.597
01/22/2018 21:48:22 [INFO] exp_shallowmodel: f1_score:   0.461
01/22/2018 21:48:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:48:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.14      0.20        14
          C       0.53      0.59      0.55       164
          F       0.72      0.76      0.74       268
          R       0.39      0.31      0.35       125

avg / total       0.58      0.60      0.59       571

01/22/2018 21:48:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:48:22 [INFO] exp_shallowmodel: 
[[  2   3   6   3]
 [  0  96  32  36]
 [  1  42 204  21]
 [  3  41  42  39]]
01/22/2018 21:48:22 [INFO] exp_shallowmodel: ******************** dstc2 - Round 22 
01/22/2018 21:48:22 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:48:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:48:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:48:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:48:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:48:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:48:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:49:06 [INFO] exp_shallowmodel: train time: 43.688s
01/22/2018 21:49:06 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:49:06 [INFO] exp_shallowmodel: accuracy:   0.608
01/22/2018 21:49:06 [INFO] exp_shallowmodel: f1_score:   0.448
01/22/2018 21:49:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:49:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.57      0.69      0.62       164
          F       0.70      0.73      0.71       268
          R       0.43      0.30      0.36       125

avg / total       0.59      0.61      0.59       571

01/22/2018 21:49:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:49:06 [INFO] exp_shallowmodel: 
[[  1   2   3   8]
 [  1 113  30  20]
 [  0  50 195  23]
 [  4  33  50  38]]
01/22/2018 21:49:06 [INFO] exp_shallowmodel: ******************** dstc2 - Round 23 
01/22/2018 21:49:06 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:49:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:49:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:49:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:49:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:49:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:49:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:49:52 [INFO] exp_shallowmodel: train time: 46.053s
01/22/2018 21:49:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:49:52 [INFO] exp_shallowmodel: accuracy:   0.639
01/22/2018 21:49:52 [INFO] exp_shallowmodel: f1_score:   0.455
01/22/2018 21:49:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:49:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.72      0.65       164
          F       0.74      0.74      0.74       268
          R       0.49      0.38      0.43       125

avg / total       0.62      0.64      0.63       571

01/22/2018 21:49:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:49:52 [INFO] exp_shallowmodel: 
[[  0   4   6   4]
 [  2 118  23  21]
 [  3  41 199  25]
 [  0  36  41  48]]
01/22/2018 21:49:52 [INFO] exp_shallowmodel: ******************** dstc2 - Round 24 
01/22/2018 21:49:52 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:49:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:49:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:49:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:49:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:49:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:49:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:50:35 [INFO] exp_shallowmodel: train time: 42.982s
01/22/2018 21:50:35 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:50:35 [INFO] exp_shallowmodel: accuracy:   0.616
01/22/2018 21:50:35 [INFO] exp_shallowmodel: f1_score:   0.463
01/22/2018 21:50:35 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:50:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.59      0.73      0.65       164
          F       0.71      0.71      0.71       268
          R       0.44      0.34      0.39       125

avg / total       0.60      0.62      0.61       571

01/22/2018 21:50:35 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:50:35 [INFO] exp_shallowmodel: 
[[  1   3   5   5]
 [  0 119  25  20]
 [  0  50 189  29]
 [  3  30  49  43]]
01/22/2018 21:50:35 [INFO] exp_shallowmodel: ******************** dstc2 - Round 25 
01/22/2018 21:50:35 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:50:35 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:50:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:50:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:50:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:50:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:50:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:51:16 [INFO] exp_shallowmodel: train time: 41.135s
01/22/2018 21:51:16 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:51:16 [INFO] exp_shallowmodel: accuracy:   0.595
01/22/2018 21:51:16 [INFO] exp_shallowmodel: f1_score:   0.421
01/22/2018 21:51:16 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:51:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.63      0.61       164
          F       0.67      0.72      0.70       268
          R       0.42      0.34      0.38       125

avg / total       0.58      0.60      0.58       571

01/22/2018 21:51:16 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:51:16 [INFO] exp_shallowmodel: 
[[  0   4   8   2]
 [  2 104  35  23]
 [  1  41 194  32]
 [  3  27  53  42]]
01/22/2018 21:51:17 [INFO] exp_shallowmodel: ******************** dstc2 - Round 26 
01/22/2018 21:51:17 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:51:17 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:51:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:51:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:51:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:51:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:51:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:52:00 [INFO] exp_shallowmodel: train time: 43.297s
01/22/2018 21:52:00 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:52:00 [INFO] exp_shallowmodel: accuracy:   0.637
01/22/2018 21:52:00 [INFO] exp_shallowmodel: f1_score:   0.498
01/22/2018 21:52:00 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:52:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.14      0.22        14
          C       0.58      0.70      0.63       164
          F       0.73      0.77      0.75       268
          R       0.47      0.33      0.39       125

avg / total       0.62      0.64      0.62       571

01/22/2018 21:52:00 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:52:00 [INFO] exp_shallowmodel: 
[[  2   2   5   5]
 [  1 114  29  20]
 [  1  39 207  21]
 [  0  42  42  41]]
01/22/2018 21:52:00 [INFO] exp_shallowmodel: ******************** dstc2 - Round 27 
01/22/2018 21:52:00 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:52:00 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:52:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:52:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:52:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:52:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:52:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:52:40 [INFO] exp_shallowmodel: train time: 40.154s
01/22/2018 21:52:40 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:52:40 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 21:52:40 [INFO] exp_shallowmodel: f1_score:   0.440
01/22/2018 21:52:40 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:52:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.71      0.63       164
          F       0.73      0.74      0.73       268
          R       0.49      0.34      0.40       125

avg / total       0.61      0.62      0.61       571

01/22/2018 21:52:40 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:52:40 [INFO] exp_shallowmodel: 
[[  0   5   3   6]
 [  3 116  28  17]
 [  4  46 197  21]
 [  4  38  41  42]]
01/22/2018 21:52:40 [INFO] exp_shallowmodel: ******************** dstc2 - Round 28 
01/22/2018 21:52:40 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:52:40 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:52:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:52:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:52:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:52:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:52:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:53:22 [INFO] exp_shallowmodel: train time: 41.948s
01/22/2018 21:53:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:53:22 [INFO] exp_shallowmodel: accuracy:   0.634
01/22/2018 21:53:22 [INFO] exp_shallowmodel: f1_score:   0.441
01/22/2018 21:53:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:53:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.70      0.64       164
          F       0.73      0.78      0.75       268
          R       0.48      0.31      0.38       125

avg / total       0.61      0.63      0.62       571

01/22/2018 21:53:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:53:22 [INFO] exp_shallowmodel: 
[[  0   1   7   6]
 [  2 114  33  15]
 [  1  37 209  21]
 [  4  43  39  39]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 21:53:22 [INFO] exp_shallowmodel: ******************** dstc2 - Round 29 
01/22/2018 21:53:22 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 21:53:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:53:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:53:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:53:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:53:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:53:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:54:05 [INFO] exp_shallowmodel: train time: 42.974s
01/22/2018 21:54:05 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:54:05 [INFO] exp_shallowmodel: accuracy:   0.608
01/22/2018 21:54:05 [INFO] exp_shallowmodel: f1_score:   0.452
01/22/2018 21:54:05 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:54:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.06      0.10        16
          C       0.59      0.66      0.62       169
          F       0.68      0.74      0.71       271
          R       0.45      0.34      0.39       130

avg / total       0.59      0.61      0.59       586

01/22/2018 21:54:05 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:54:05 [INFO] exp_shallowmodel: 
[[  1   2   7   6]
 [  0 111  39  19]
 [  1  41 200  29]
 [  3  34  49  44]]
01/22/2018 21:54:06 [INFO] exp_shallowmodel: ******************** dstc2 - Round 30 
01/22/2018 21:54:06 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:54:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:54:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:54:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:54:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:54:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:54:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:54:49 [INFO] exp_shallowmodel: train time: 43.194s
01/22/2018 21:54:49 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:54:49 [INFO] exp_shallowmodel: accuracy:   0.599
01/22/2018 21:54:49 [INFO] exp_shallowmodel: f1_score:   0.409
01/22/2018 21:54:49 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:54:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.55      0.71      0.62       164
          F       0.71      0.72      0.72       268
          R       0.36      0.25      0.30       125

avg / total       0.57      0.60      0.58       571

01/22/2018 21:54:49 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:54:49 [INFO] exp_shallowmodel: 
[[  0   3   4   7]
 [  1 117  28  18]
 [  1  44 194  29]
 [  0  48  46  31]]
01/22/2018 21:54:49 [INFO] exp_shallowmodel: ******************** dstc2 - Round 31 
01/22/2018 21:54:49 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:54:49 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:54:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:54:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:54:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:54:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:54:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:55:32 [INFO] exp_shallowmodel: train time: 43.488s
01/22/2018 21:55:32 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:55:32 [INFO] exp_shallowmodel: accuracy:   0.627
01/22/2018 21:55:32 [INFO] exp_shallowmodel: f1_score:   0.471
01/22/2018 21:55:32 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:55:32 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.59      0.73      0.65       164
          F       0.71      0.72      0.72       268
          R       0.49      0.35      0.41       125

avg / total       0.61      0.63      0.61       571

01/22/2018 21:55:32 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:55:32 [INFO] exp_shallowmodel: 
[[  1   1   7   5]
 [  1 119  22  22]
 [  1  54 194  19]
 [  1  29  51  44]]
01/22/2018 21:55:33 [INFO] exp_shallowmodel: ******************** dstc2 - Round 32 
01/22/2018 21:55:33 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:55:33 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:55:33 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:55:33 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:55:33 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:55:33 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:55:33 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:56:18 [INFO] exp_shallowmodel: train time: 45.561s
01/22/2018 21:56:18 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:56:18 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 21:56:18 [INFO] exp_shallowmodel: f1_score:   0.430
01/22/2018 21:56:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:56:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.07      0.08        14
          C       0.55      0.62      0.58       164
          F       0.70      0.74      0.72       268
          R       0.38      0.30      0.33       125

avg / total       0.58      0.59      0.58       571

01/22/2018 21:56:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:56:18 [INFO] exp_shallowmodel: 
[[  1   2   6   5]
 [  2 101  27  34]
 [  3  45 198  22]
 [  4  34  50  37]]
01/22/2018 21:56:18 [INFO] exp_shallowmodel: ******************** dstc2 - Round 33 
01/22/2018 21:56:18 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:56:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:56:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:56:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:56:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:56:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:56:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:57:02 [INFO] exp_shallowmodel: train time: 43.333s
01/22/2018 21:57:02 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:57:02 [INFO] exp_shallowmodel: accuracy:   0.646
01/22/2018 21:57:02 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 21:57:02 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:57:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.68      0.64       164
          F       0.73      0.78      0.75       268
          R       0.52      0.38      0.44       125

avg / total       0.63      0.65      0.63       571

01/22/2018 21:57:02 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:57:02 [INFO] exp_shallowmodel: 
[[  0   5   5   4]
 [  1 112  31  20]
 [  1  38 209  20]
 [  4  30  43  48]]
01/22/2018 21:57:02 [INFO] exp_shallowmodel: ******************** dstc2 - Round 34 
01/22/2018 21:57:02 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:57:02 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:57:02 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:57:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:57:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:57:02 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:57:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:57:45 [INFO] exp_shallowmodel: train time: 43.350s
01/22/2018 21:57:45 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:57:45 [INFO] exp_shallowmodel: accuracy:   0.636
01/22/2018 21:57:45 [INFO] exp_shallowmodel: f1_score:   0.475
01/22/2018 21:57:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:57:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.56      0.70      0.62       164
          F       0.72      0.76      0.74       268
          R       0.54      0.35      0.43       125

avg / total       0.62      0.64      0.62       571

01/22/2018 21:57:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:57:45 [INFO] exp_shallowmodel: 
[[  1   4   8   1]
 [  2 115  32  15]
 [  0  44 203  21]
 [  1  41  39  44]]
01/22/2018 21:57:45 [INFO] exp_shallowmodel: ******************** dstc2 - Round 35 
01/22/2018 21:57:45 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:57:45 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:57:45 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:57:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:57:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:57:45 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:57:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:58:27 [INFO] exp_shallowmodel: train time: 41.645s
01/22/2018 21:58:27 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:58:27 [INFO] exp_shallowmodel: accuracy:   0.616
01/22/2018 21:58:27 [INFO] exp_shallowmodel: f1_score:   0.480
01/22/2018 21:58:27 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:58:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.14      0.19        14
          C       0.59      0.68      0.63       164
          F       0.69      0.74      0.71       268
          R       0.48      0.33      0.39       125

avg / total       0.60      0.62      0.60       571

01/22/2018 21:58:27 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:58:27 [INFO] exp_shallowmodel: 
[[  2   4   6   2]
 [  0 111  40  13]
 [  2  38 198  30]
 [  3  36  45  41]]
01/22/2018 21:58:27 [INFO] exp_shallowmodel: ******************** dstc2 - Round 36 
01/22/2018 21:58:27 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:58:27 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:58:27 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:58:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:58:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:58:27 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:58:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:59:09 [INFO] exp_shallowmodel: train time: 41.844s
01/22/2018 21:59:09 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:59:09 [INFO] exp_shallowmodel: accuracy:   0.616
01/22/2018 21:59:09 [INFO] exp_shallowmodel: f1_score:   0.439
01/22/2018 21:59:09 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:59:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.56      0.68      0.62       164
          F       0.70      0.72      0.71       268
          R       0.50      0.38      0.43       125

avg / total       0.60      0.62      0.60       571

01/22/2018 21:59:09 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:59:09 [INFO] exp_shallowmodel: 
[[  0   3   6   5]
 [  1 112  33  18]
 [  2  49 193  24]
 [  0  35  43  47]]
01/22/2018 21:59:09 [INFO] exp_shallowmodel: ******************** dstc2 - Round 37 
01/22/2018 21:59:09 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:59:09 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:59:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:59:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:59:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:59:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:59:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:59:52 [INFO] exp_shallowmodel: train time: 42.854s
01/22/2018 21:59:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 21:59:52 [INFO] exp_shallowmodel: accuracy:   0.604
01/22/2018 21:59:52 [INFO] exp_shallowmodel: f1_score:   0.451
01/22/2018 21:59:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:59:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.07      0.11        14
          C       0.58      0.65      0.62       164
          F       0.68      0.73      0.70       268
          R       0.44      0.34      0.38       125

avg / total       0.59      0.60      0.59       571

01/22/2018 21:59:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:59:52 [INFO] exp_shallowmodel: 
[[  1   3   7   3]
 [  1 107  37  19]
 [  1  40 195  32]
 [  2  33  48  42]]
01/22/2018 21:59:52 [INFO] exp_shallowmodel: ******************** dstc2 - Round 38 
01/22/2018 21:59:52 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:59:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 21:59:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:59:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:59:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:59:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:59:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:00:38 [INFO] exp_shallowmodel: train time: 45.693s
01/22/2018 22:00:38 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:00:38 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:00:38 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 22:00:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:00:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.69      0.64       164
          F       0.69      0.73      0.71       268
          R       0.47      0.34      0.40       125

avg / total       0.60      0.61      0.60       571

01/22/2018 22:00:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:00:38 [INFO] exp_shallowmodel: 
[[  0   2   6   6]
 [  0 113  32  19]
 [  3  46 195  24]
 [  4  30  48  43]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:00:38 [INFO] exp_shallowmodel: ******************** dstc2 - Round 39 
01/22/2018 22:00:38 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 22:00:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:00:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:00:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:00:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:00:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:00:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:01:21 [INFO] exp_shallowmodel: train time: 43.351s
01/22/2018 22:01:21 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:01:21 [INFO] exp_shallowmodel: accuracy:   0.585
01/22/2018 22:01:21 [INFO] exp_shallowmodel: f1_score:   0.413
01/22/2018 22:01:21 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:01:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.57      0.62      0.59       169
          F       0.67      0.72      0.69       271
          R       0.41      0.33      0.37       130

avg / total       0.56      0.59      0.57       586

01/22/2018 22:01:21 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:01:21 [INFO] exp_shallowmodel: 
[[  0   2   9   5]
 [  0 104  40  25]
 [  1  42 196  32]
 [  4  34  49  43]]
01/22/2018 22:01:21 [INFO] exp_shallowmodel: ******************** dstc2 - Round 40 
01/22/2018 22:01:21 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:01:21 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:01:21 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:01:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:01:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:01:21 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:01:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:02:03 [INFO] exp_shallowmodel: train time: 41.240s
01/22/2018 22:02:03 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:02:03 [INFO] exp_shallowmodel: accuracy:   0.604
01/22/2018 22:02:03 [INFO] exp_shallowmodel: f1_score:   0.418
01/22/2018 22:02:03 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:02:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.55      0.68      0.61       164
          F       0.70      0.74      0.72       268
          R       0.44      0.29      0.35       125

avg / total       0.58      0.60      0.59       571

01/22/2018 22:02:03 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:02:03 [INFO] exp_shallowmodel: 
[[  0   6   5   3]
 [  1 112  27  24]
 [  1  51 197  19]
 [  1  35  53  36]]
01/22/2018 22:02:03 [INFO] exp_shallowmodel: ******************** dstc2 - Round 41 
01/22/2018 22:02:03 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:02:03 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:02:03 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:02:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:02:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:02:03 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:02:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:02:48 [INFO] exp_shallowmodel: train time: 44.956s
01/22/2018 22:02:48 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:02:48 [INFO] exp_shallowmodel: accuracy:   0.604
01/22/2018 22:02:48 [INFO] exp_shallowmodel: f1_score:   0.457
01/22/2018 22:02:48 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:02:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.55      0.62      0.58       164
          F       0.68      0.72      0.70       268
          R       0.52      0.39      0.45       125

avg / total       0.59      0.60      0.60       571

01/22/2018 22:02:48 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:02:48 [INFO] exp_shallowmodel: 
[[  1   1  11   1]
 [  0 101  43  20]
 [  4  45 194  25]
 [  1  37  38  49]]
01/22/2018 22:02:48 [INFO] exp_shallowmodel: ******************** dstc2 - Round 42 
01/22/2018 22:02:48 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:02:48 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:02:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:02:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:02:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:02:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:02:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:03:31 [INFO] exp_shallowmodel: train time: 42.863s
01/22/2018 22:03:31 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:03:31 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 22:03:31 [INFO] exp_shallowmodel: f1_score:   0.435
01/22/2018 22:03:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:03:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.65      0.62       164
          F       0.71      0.73      0.72       268
          R       0.44      0.38      0.41       125

avg / total       0.60      0.61      0.60       571

01/22/2018 22:03:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:03:31 [INFO] exp_shallowmodel: 
[[  0   2   5   7]
 [  1 106  34  23]
 [  3  39 196  30]
 [  3  33  42  47]]
01/22/2018 22:03:31 [INFO] exp_shallowmodel: ******************** dstc2 - Round 43 
01/22/2018 22:03:31 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:03:31 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:03:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:03:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:03:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:03:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:03:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:04:17 [INFO] exp_shallowmodel: train time: 46.186s
01/22/2018 22:04:17 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:04:17 [INFO] exp_shallowmodel: accuracy:   0.585
01/22/2018 22:04:17 [INFO] exp_shallowmodel: f1_score:   0.405
01/22/2018 22:04:17 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:04:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.53      0.65      0.59       164
          F       0.69      0.72      0.70       268
          R       0.42      0.27      0.33       125

avg / total       0.57      0.58      0.57       571

01/22/2018 22:04:17 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:04:17 [INFO] exp_shallowmodel: 
[[  0   4   6   4]
 [  0 107  34  23]
 [  3  52 193  20]
 [  6  38  47  34]]
01/22/2018 22:04:17 [INFO] exp_shallowmodel: ******************** dstc2 - Round 44 
01/22/2018 22:04:17 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:04:17 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:04:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:04:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:04:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:04:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:04:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:05:01 [INFO] exp_shallowmodel: train time: 44.096s
01/22/2018 22:05:01 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:05:01 [INFO] exp_shallowmodel: accuracy:   0.653
01/22/2018 22:05:01 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 22:05:01 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:05:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.62      0.73      0.67       164
          F       0.72      0.79      0.75       268
          R       0.53      0.34      0.42       125

avg / total       0.63      0.65      0.64       571

01/22/2018 22:05:01 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:05:01 [INFO] exp_shallowmodel: 
[[  0   3   8   3]
 [  0 119  29  16]
 [  0  38 211  19]
 [  4  32  46  43]]
01/22/2018 22:05:02 [INFO] exp_shallowmodel: ******************** dstc2 - Round 45 
01/22/2018 22:05:02 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:05:02 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:05:02 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:05:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:05:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:05:02 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:05:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:05:45 [INFO] exp_shallowmodel: train time: 42.940s
01/22/2018 22:05:45 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:05:45 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:05:45 [INFO] exp_shallowmodel: f1_score:   0.433
01/22/2018 22:05:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:05:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.70      0.64       164
          F       0.70      0.73      0.71       268
          R       0.44      0.33      0.38       125

avg / total       0.59      0.61      0.60       571

01/22/2018 22:05:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:05:45 [INFO] exp_shallowmodel: 
[[  0   5   2   7]
 [  1 114  32  17]
 [  3  41 196  28]
 [  1  32  51  41]]
01/22/2018 22:05:45 [INFO] exp_shallowmodel: ******************** dstc2 - Round 46 
01/22/2018 22:05:45 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:05:45 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:05:45 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:05:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:05:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:05:45 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:05:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:06:27 [INFO] exp_shallowmodel: train time: 42.522s
01/22/2018 22:06:27 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:06:27 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:06:27 [INFO] exp_shallowmodel: f1_score:   0.489
01/22/2018 22:06:27 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:06:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.21      0.29        14
          C       0.56      0.62      0.59       164
          F       0.70      0.79      0.74       268
          R       0.44      0.28      0.34       125

avg / total       0.59      0.61      0.60       571

01/22/2018 22:06:27 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:06:27 [INFO] exp_shallowmodel: 
[[  3   2   6   3]
 [  0 102  39  23]
 [  1  37 211  19]
 [  3  40  47  35]]
01/22/2018 22:06:27 [INFO] exp_shallowmodel: ******************** dstc2 - Round 47 
01/22/2018 22:06:27 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:06:27 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:06:27 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:06:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:06:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:06:27 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:06:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:07:11 [INFO] exp_shallowmodel: train time: 43.158s
01/22/2018 22:07:11 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:07:11 [INFO] exp_shallowmodel: accuracy:   0.620
01/22/2018 22:07:11 [INFO] exp_shallowmodel: f1_score:   0.437
01/22/2018 22:07:11 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:07:11 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.74      0.66       164
          F       0.71      0.72      0.71       268
          R       0.45      0.31      0.37       125

avg / total       0.60      0.62      0.61       571

01/22/2018 22:07:11 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:07:11 [INFO] exp_shallowmodel: 
[[  0   1   7   6]
 [  3 122  23  16]
 [  6  44 193  25]
 [  0  36  50  39]]
01/22/2018 22:07:11 [INFO] exp_shallowmodel: ******************** dstc2 - Round 48 
01/22/2018 22:07:11 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:07:11 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:07:11 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:07:11 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:07:11 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:07:11 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:07:11 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:07:54 [INFO] exp_shallowmodel: train time: 43.174s
01/22/2018 22:07:54 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:07:54 [INFO] exp_shallowmodel: accuracy:   0.625
01/22/2018 22:07:54 [INFO] exp_shallowmodel: f1_score:   0.472
01/22/2018 22:07:54 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:07:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.07      0.11        14
          C       0.62      0.69      0.65       164
          F       0.72      0.72      0.72       268
          R       0.43      0.39      0.41       125

avg / total       0.61      0.63      0.62       571

01/22/2018 22:07:54 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:07:54 [INFO] exp_shallowmodel: 
[[  1   2   4   7]
 [  1 113  23  27]
 [  0  43 194  31]
 [  3  24  49  49]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:07:54 [INFO] exp_shallowmodel: ******************** dstc2 - Round 49 
01/22/2018 22:07:54 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 22:07:54 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:07:54 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:07:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:07:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:07:54 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:07:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:08:35 [INFO] exp_shallowmodel: train time: 41.219s
01/22/2018 22:08:35 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:08:35 [INFO] exp_shallowmodel: accuracy:   0.609
01/22/2018 22:08:35 [INFO] exp_shallowmodel: f1_score:   0.433
01/22/2018 22:08:35 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:08:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.58      0.69      0.63       169
          F       0.69      0.72      0.71       271
          R       0.45      0.35      0.40       130

avg / total       0.59      0.61      0.60       586

01/22/2018 22:08:35 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:08:35 [INFO] exp_shallowmodel: 
[[  0   1   8   7]
 [  2 116  30  21]
 [  1  47 195  28]
 [  1  35  48  46]]
01/22/2018 22:08:38 [INFO] exp_shallowmodel: ******************** dstc3 - Round 0 
01/22/2018 22:08:38 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:08:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:08:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:08:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:08:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:08:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:08:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:09:22 [INFO] exp_shallowmodel: train time: 43.540s
01/22/2018 22:09:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:09:22 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:09:22 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 22:09:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:09:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.05      0.07        20
          C       0.55      0.64      0.59       169
          F       0.75      0.77      0.76       281
          R       0.40      0.31      0.35       122

avg / total       0.60      0.61      0.60       592

01/22/2018 22:09:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:09:22 [INFO] exp_shallowmodel: 
[[  1   2   8   9]
 [  2 109  32  26]
 [  2  40 216  23]
 [  4  47  33  38]]
01/22/2018 22:09:22 [INFO] exp_shallowmodel: ******************** dstc3 - Round 1 
01/22/2018 22:09:22 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:09:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:09:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:09:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:09:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:09:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:09:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:10:08 [INFO] exp_shallowmodel: train time: 46.249s
01/22/2018 22:10:08 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:10:08 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 22:10:08 [INFO] exp_shallowmodel: f1_score:   0.404
01/22/2018 22:10:08 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:10:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.66      0.59       169
          F       0.72      0.74      0.73       281
          R       0.38      0.25      0.30       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:10:08 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:10:08 [INFO] exp_shallowmodel: 
[[  0   3   8   9]
 [  4 112  33  20]
 [  0  52 208  21]
 [  7  46  39  30]]
01/22/2018 22:10:09 [INFO] exp_shallowmodel: ******************** dstc3 - Round 2 
01/22/2018 22:10:09 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:10:09 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:10:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:10:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:10:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:10:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:10:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:10:55 [INFO] exp_shallowmodel: train time: 46.322s
01/22/2018 22:10:55 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:10:55 [INFO] exp_shallowmodel: accuracy:   0.596
01/22/2018 22:10:55 [INFO] exp_shallowmodel: f1_score:   0.449
01/22/2018 22:10:55 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:10:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.51      0.62      0.56       169
          F       0.72      0.74      0.73       281
          R       0.43      0.31      0.36       122

avg / total       0.58      0.60      0.59       592

01/22/2018 22:10:55 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:10:55 [INFO] exp_shallowmodel: 
[[  2   5  10   3]
 [  4 105  32  28]
 [  0  53 208  20]
 [  1  44  39  38]]
01/22/2018 22:10:55 [INFO] exp_shallowmodel: ******************** dstc3 - Round 3 
01/22/2018 22:10:55 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:10:55 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:10:55 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:10:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:10:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:10:55 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:10:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:11:43 [INFO] exp_shallowmodel: train time: 48.145s
01/22/2018 22:11:43 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:11:43 [INFO] exp_shallowmodel: accuracy:   0.584
01/22/2018 22:11:43 [INFO] exp_shallowmodel: f1_score:   0.397
01/22/2018 22:11:43 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:11:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.52      0.64      0.58       169
          F       0.69      0.74      0.71       281
          R       0.39      0.25      0.30       122

avg / total       0.56      0.58      0.56       592

01/22/2018 22:11:43 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:11:43 [INFO] exp_shallowmodel: 
[[  0   7   7   6]
 [  1 109  41  18]
 [  0  52 207  22]
 [  4  41  47  30]]
01/22/2018 22:11:43 [INFO] exp_shallowmodel: ******************** dstc3 - Round 4 
01/22/2018 22:11:43 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:11:43 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:11:43 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:11:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:11:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:11:43 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:11:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:12:29 [INFO] exp_shallowmodel: train time: 46.113s
01/22/2018 22:12:29 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:12:29 [INFO] exp_shallowmodel: accuracy:   0.573
01/22/2018 22:12:29 [INFO] exp_shallowmodel: f1_score:   0.401
01/22/2018 22:12:29 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:12:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.51      0.53      0.52       169
          F       0.68      0.74      0.71       281
          R       0.40      0.34      0.37       122

avg / total       0.55      0.57      0.56       592

01/22/2018 22:12:29 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:12:29 [INFO] exp_shallowmodel: 
[[  0   3   6  11]
 [  3  89  52  25]
 [  4  43 208  26]
 [  3  38  39  42]]
01/22/2018 22:12:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 5 
01/22/2018 22:12:30 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:12:30 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:12:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:12:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:12:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:12:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:12:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:13:18 [INFO] exp_shallowmodel: train time: 48.403s
01/22/2018 22:13:18 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:13:18 [INFO] exp_shallowmodel: accuracy:   0.578
01/22/2018 22:13:18 [INFO] exp_shallowmodel: f1_score:   0.411
01/22/2018 22:13:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:13:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.09      0.05      0.06        20
          C       0.53      0.62      0.57       169
          F       0.66      0.73      0.69       281
          R       0.42      0.25      0.31       122

avg / total       0.55      0.58      0.56       592

01/22/2018 22:13:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:13:18 [INFO] exp_shallowmodel: 
[[  1   3   9   7]
 [  3 105  48  13]
 [  0  54 206  21]
 [  7  35  50  30]]
01/22/2018 22:13:18 [INFO] exp_shallowmodel: ******************** dstc3 - Round 6 
01/22/2018 22:13:18 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:13:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:13:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:13:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:13:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:13:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:13:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:14:04 [INFO] exp_shallowmodel: train time: 46.205s
01/22/2018 22:14:04 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:14:04 [INFO] exp_shallowmodel: accuracy:   0.557
01/22/2018 22:14:04 [INFO] exp_shallowmodel: f1_score:   0.401
01/22/2018 22:14:04 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:14:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.08      0.05      0.06        20
          C       0.49      0.55      0.52       169
          F       0.69      0.71      0.70       281
          R       0.36      0.30      0.32       122

avg / total       0.54      0.56      0.55       592

01/22/2018 22:14:04 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:14:04 [INFO] exp_shallowmodel: 
[[  1   7   5   7]
 [  3  93  43  30]
 [  5  49 200  27]
 [  3  42  41  36]]
01/22/2018 22:14:05 [INFO] exp_shallowmodel: ******************** dstc3 - Round 7 
01/22/2018 22:14:05 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:14:05 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:14:05 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:14:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:14:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:14:05 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:14:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:14:52 [INFO] exp_shallowmodel: train time: 47.562s
01/22/2018 22:14:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:14:52 [INFO] exp_shallowmodel: accuracy:   0.581
01/22/2018 22:14:52 [INFO] exp_shallowmodel: f1_score:   0.407
01/22/2018 22:14:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:14:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.54      0.58      0.56       169
          F       0.68      0.74      0.71       281
          R       0.41      0.32      0.36       122

avg / total       0.56      0.58      0.57       592

01/22/2018 22:14:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:14:52 [INFO] exp_shallowmodel: 
[[  0   5   7   8]
 [  3  98  45  23]
 [  4  46 207  24]
 [  5  31  47  39]]
01/22/2018 22:14:52 [INFO] exp_shallowmodel: ******************** dstc3 - Round 8 
01/22/2018 22:14:52 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:14:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:14:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:14:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:14:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:14:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:14:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:15:40 [INFO] exp_shallowmodel: train time: 47.329s
01/22/2018 22:15:40 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:15:40 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 22:15:40 [INFO] exp_shallowmodel: f1_score:   0.424
01/22/2018 22:15:40 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:15:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.05      0.07        20
          C       0.51      0.64      0.57       169
          F       0.71      0.74      0.72       281
          R       0.42      0.28      0.34       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:15:40 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:15:40 [INFO] exp_shallowmodel: 
[[  1   4   9   6]
 [  2 108  36  23]
 [  2  55 207  17]
 [  3  45  40  34]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:15:40 [INFO] exp_shallowmodel: ******************** dstc3 - Round 9 
01/22/2018 22:15:40 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 22:15:40 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:15:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:15:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:15:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:15:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:15:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:16:28 [INFO] exp_shallowmodel: train time: 48.557s
01/22/2018 22:16:28 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:16:28 [INFO] exp_shallowmodel: accuracy:   0.604
01/22/2018 22:16:28 [INFO] exp_shallowmodel: f1_score:   0.432
01/22/2018 22:16:28 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:16:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.09      0.04      0.05        28
          C       0.54      0.70      0.61       172
          F       0.74      0.74      0.74       283
          R       0.40      0.28      0.33       123

avg / total       0.58      0.60      0.59       606

01/22/2018 22:16:28 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:16:28 [INFO] exp_shallowmodel: 
[[  1   8   6  13]
 [  0 121  29  22]
 [  4  52 210  17]
 [  6  44  39  34]]
01/22/2018 22:16:29 [INFO] exp_shallowmodel: ******************** dstc3 - Round 10 
01/22/2018 22:16:29 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:16:29 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:16:29 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:16:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:16:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:16:29 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:16:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:17:16 [INFO] exp_shallowmodel: train time: 46.946s
01/22/2018 22:17:16 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:17:16 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 22:17:16 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 22:17:16 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:17:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.05      0.06        20
          C       0.54      0.63      0.58       169
          F       0.76      0.76      0.76       281
          R       0.42      0.34      0.37       122

avg / total       0.60      0.61      0.60       592

01/22/2018 22:17:16 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:17:16 [INFO] exp_shallowmodel: 
[[  1   4   6   9]
 [  2 106  32  29]
 [  4  44 214  19]
 [  7  43  31  41]]
01/22/2018 22:17:16 [INFO] exp_shallowmodel: ******************** dstc3 - Round 11 
01/22/2018 22:17:16 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:17:16 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:17:16 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:17:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:17:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:17:16 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:17:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:18:01 [INFO] exp_shallowmodel: train time: 45.007s
01/22/2018 22:18:01 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:18:01 [INFO] exp_shallowmodel: accuracy:   0.569
01/22/2018 22:18:01 [INFO] exp_shallowmodel: f1_score:   0.417
01/22/2018 22:18:01 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:18:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.18      0.10      0.13        20
          C       0.51      0.53      0.52       169
          F       0.67      0.76      0.71       281
          R       0.36      0.26      0.30       122

avg / total       0.55      0.57      0.55       592

01/22/2018 22:18:01 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:18:01 [INFO] exp_shallowmodel: 
[[  2   5   6   7]
 [  3  90  49  27]
 [  1  44 213  23]
 [  5  37  48  32]]
01/22/2018 22:18:01 [INFO] exp_shallowmodel: ******************** dstc3 - Round 12 
01/22/2018 22:18:01 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:18:01 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:18:01 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:18:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:18:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:18:01 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:18:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:18:49 [INFO] exp_shallowmodel: train time: 48.004s
01/22/2018 22:18:49 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:18:49 [INFO] exp_shallowmodel: accuracy:   0.568
01/22/2018 22:18:49 [INFO] exp_shallowmodel: f1_score:   0.400
01/22/2018 22:18:49 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:18:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.05      0.06        20
          C       0.52      0.61      0.56       169
          F       0.69      0.73      0.71       281
          R       0.34      0.23      0.27       122

avg / total       0.55      0.57      0.55       592

01/22/2018 22:18:49 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:18:49 [INFO] exp_shallowmodel: 
[[  1   5   5   9]
 [  4 103  45  17]
 [  3  45 204  29]
 [  7  44  43  28]]
01/22/2018 22:18:49 [INFO] exp_shallowmodel: ******************** dstc3 - Round 13 
01/22/2018 22:18:49 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:18:49 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:18:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:18:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:18:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:18:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:18:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:19:38 [INFO] exp_shallowmodel: train time: 49.307s
01/22/2018 22:19:38 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:19:38 [INFO] exp_shallowmodel: accuracy:   0.588
01/22/2018 22:19:38 [INFO] exp_shallowmodel: f1_score:   0.424
01/22/2018 22:19:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:19:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.08      0.05      0.06        20
          C       0.51      0.59      0.55       169
          F       0.71      0.74      0.73       281
          R       0.42      0.31      0.36       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:19:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:19:38 [INFO] exp_shallowmodel: 
[[  1   3   9   7]
 [  5 100  40  24]
 [  1  49 209  22]
 [  5  44  35  38]]
01/22/2018 22:19:39 [INFO] exp_shallowmodel: ******************** dstc3 - Round 14 
01/22/2018 22:19:39 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:19:39 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:19:39 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:19:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:19:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:19:39 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:19:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:20:28 [INFO] exp_shallowmodel: train time: 49.294s
01/22/2018 22:20:28 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:20:28 [INFO] exp_shallowmodel: accuracy:   0.586
01/22/2018 22:20:28 [INFO] exp_shallowmodel: f1_score:   0.437
01/22/2018 22:20:28 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:20:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.10      0.13        20
          C       0.51      0.57      0.54       169
          F       0.70      0.75      0.72       281
          R       0.41      0.30      0.35       122

avg / total       0.57      0.59      0.57       592

01/22/2018 22:20:28 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:20:28 [INFO] exp_shallowmodel: 
[[  2   4  10   4]
 [  2  97  39  31]
 [  3  48 211  19]
 [  3  40  42  37]]
01/22/2018 22:20:28 [INFO] exp_shallowmodel: ******************** dstc3 - Round 15 
01/22/2018 22:20:28 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:20:28 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:20:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:20:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:20:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:20:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:20:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:21:16 [INFO] exp_shallowmodel: train time: 47.799s
01/22/2018 22:21:16 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:21:16 [INFO] exp_shallowmodel: accuracy:   0.579
01/22/2018 22:21:16 [INFO] exp_shallowmodel: f1_score:   0.400
01/22/2018 22:21:16 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:21:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.51      0.59      0.54       169
          F       0.70      0.73      0.72       281
          R       0.38      0.30      0.34       122

avg / total       0.56      0.58      0.57       592

01/22/2018 22:21:16 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:21:16 [INFO] exp_shallowmodel: 
[[  0   5   6   9]
 [  2 100  42  25]
 [  0  49 206  26]
 [  1  44  40  37]]
01/22/2018 22:21:16 [INFO] exp_shallowmodel: ******************** dstc3 - Round 16 
01/22/2018 22:21:16 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:21:16 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:21:16 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:21:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:21:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:21:16 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:21:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:22:02 [INFO] exp_shallowmodel: train time: 45.577s
01/22/2018 22:22:02 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:22:02 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:22:02 [INFO] exp_shallowmodel: f1_score:   0.431
01/22/2018 22:22:02 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:22:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.54      0.67      0.60       169
          F       0.73      0.75      0.74       281
          R       0.46      0.33      0.38       122

avg / total       0.60      0.61      0.60       592

01/22/2018 22:22:02 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:22:02 [INFO] exp_shallowmodel: 
[[  0   3   7  10]
 [  1 114  34  20]
 [  4  50 210  17]
 [  2  44  36  40]]
01/22/2018 22:22:02 [INFO] exp_shallowmodel: ******************** dstc3 - Round 17 
01/22/2018 22:22:02 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:22:02 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:22:02 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:22:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:22:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:22:02 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:22:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:22:50 [INFO] exp_shallowmodel: train time: 48.590s
01/22/2018 22:22:50 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:22:50 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 22:22:50 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 22:22:50 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:22:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.10      0.13        20
          C       0.54      0.66      0.59       169
          F       0.71      0.72      0.72       281
          R       0.38      0.29      0.33       122

avg / total       0.58      0.59      0.58       592

01/22/2018 22:22:50 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:22:50 [INFO] exp_shallowmodel: 
[[  2   3   5  10]
 [  3 111  33  22]
 [  3  52 202  24]
 [  2  41  44  35]]
01/22/2018 22:22:50 [INFO] exp_shallowmodel: ******************** dstc3 - Round 18 
01/22/2018 22:22:50 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:22:50 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:22:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:22:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:22:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:22:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:22:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:23:35 [INFO] exp_shallowmodel: train time: 44.845s
01/22/2018 22:23:35 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:23:35 [INFO] exp_shallowmodel: accuracy:   0.581
01/22/2018 22:23:35 [INFO] exp_shallowmodel: f1_score:   0.401
01/22/2018 22:23:35 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:23:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.54      0.63      0.58       169
          F       0.68      0.73      0.70       281
          R       0.39      0.28      0.33       122

avg / total       0.56      0.58      0.57       592

01/22/2018 22:23:35 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:23:35 [INFO] exp_shallowmodel: 
[[  0   5   8   7]
 [  1 106  37  25]
 [  2  54 204  21]
 [  4  33  51  34]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:23:35 [INFO] exp_shallowmodel: ******************** dstc3 - Round 19 
01/22/2018 22:23:35 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 22:23:35 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:23:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:23:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:23:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:23:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:23:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:24:22 [INFO] exp_shallowmodel: train time: 46.477s
01/22/2018 22:24:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:24:22 [INFO] exp_shallowmodel: accuracy:   0.592
01/22/2018 22:24:22 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 22:24:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:24:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.11      0.15        28
          C       0.51      0.67      0.58       172
          F       0.71      0.75      0.73       283
          R       0.41      0.24      0.30       123

avg / total       0.57      0.59      0.57       606

01/22/2018 22:24:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:24:22 [INFO] exp_shallowmodel: 
[[  3   9   8   8]
 [  2 115  39  16]
 [  3  49 211  20]
 [  3  52  38  30]]
01/22/2018 22:24:22 [INFO] exp_shallowmodel: ******************** dstc3 - Round 20 
01/22/2018 22:24:22 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:24:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:24:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:24:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:24:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:24:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:24:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:25:09 [INFO] exp_shallowmodel: train time: 46.723s
01/22/2018 22:25:09 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:25:09 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 22:25:09 [INFO] exp_shallowmodel: f1_score:   0.450
01/22/2018 22:25:09 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:25:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.19      0.15      0.17        20
          C       0.53      0.72      0.61       169
          F       0.75      0.75      0.75       281
          R       0.40      0.20      0.27       122

avg / total       0.60      0.61      0.59       592

01/22/2018 22:25:09 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:25:09 [INFO] exp_shallowmodel: 
[[  3   3   7   7]
 [  2 122  28  17]
 [  1  54 212  14]
 [ 10  53  34  25]]
01/22/2018 22:25:09 [INFO] exp_shallowmodel: ******************** dstc3 - Round 21 
01/22/2018 22:25:09 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:25:09 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:25:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:25:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:25:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:25:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:25:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:25:59 [INFO] exp_shallowmodel: train time: 50.387s
01/22/2018 22:25:59 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:25:59 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 22:25:59 [INFO] exp_shallowmodel: f1_score:   0.413
01/22/2018 22:25:59 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:25:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.50      0.63      0.56       169
          F       0.72      0.73      0.72       281
          R       0.46      0.30      0.37       122

avg / total       0.58      0.59      0.58       592

01/22/2018 22:25:59 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:25:59 [INFO] exp_shallowmodel: 
[[  0   6   7   7]
 [  5 107  40  17]
 [  2  54 206  19]
 [  5  45  35  37]]
01/22/2018 22:26:00 [INFO] exp_shallowmodel: ******************** dstc3 - Round 22 
01/22/2018 22:26:00 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:26:00 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:26:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:26:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:26:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:26:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:26:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:26:46 [INFO] exp_shallowmodel: train time: 46.331s
01/22/2018 22:26:46 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:26:46 [INFO] exp_shallowmodel: accuracy:   0.586
01/22/2018 22:26:46 [INFO] exp_shallowmodel: f1_score:   0.403
01/22/2018 22:26:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:26:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.54      0.63      0.58       169
          F       0.69      0.74      0.71       281
          R       0.41      0.26      0.32       122

avg / total       0.56      0.59      0.57       592

01/22/2018 22:26:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:26:46 [INFO] exp_shallowmodel: 
[[  0   4  12   4]
 [  6 107  38  18]
 [  2  47 208  24]
 [  4  41  45  32]]
01/22/2018 22:26:46 [INFO] exp_shallowmodel: ******************** dstc3 - Round 23 
01/22/2018 22:26:46 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:26:46 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:26:46 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:26:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:26:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:26:46 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:26:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:27:29 [INFO] exp_shallowmodel: train time: 43.100s
01/22/2018 22:27:29 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:27:29 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 22:27:29 [INFO] exp_shallowmodel: f1_score:   0.423
01/22/2018 22:27:29 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:27:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.05      0.08        20
          C       0.52      0.62      0.56       169
          F       0.69      0.74      0.72       281
          R       0.42      0.28      0.33       122

avg / total       0.57      0.59      0.57       592

01/22/2018 22:27:29 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:27:29 [INFO] exp_shallowmodel: 
[[  1   5   7   7]
 [  3 105  39  22]
 [  1  53 209  18]
 [  1  40  47  34]]
01/22/2018 22:27:29 [INFO] exp_shallowmodel: ******************** dstc3 - Round 24 
01/22/2018 22:27:29 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:27:29 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:27:29 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:27:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:27:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:27:29 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:27:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:28:16 [INFO] exp_shallowmodel: train time: 46.594s
01/22/2018 22:28:16 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:28:16 [INFO] exp_shallowmodel: accuracy:   0.576
01/22/2018 22:28:16 [INFO] exp_shallowmodel: f1_score:   0.396
01/22/2018 22:28:16 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:28:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.52      0.63      0.57       169
          F       0.72      0.72      0.72       281
          R       0.33      0.27      0.30       122

avg / total       0.56      0.58      0.56       592

01/22/2018 22:28:16 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:28:16 [INFO] exp_shallowmodel: 
[[  0   2   9   9]
 [  1 107  30  31]
 [  3  50 201  27]
 [  3  47  39  33]]
01/22/2018 22:28:16 [INFO] exp_shallowmodel: ******************** dstc3 - Round 25 
01/22/2018 22:28:16 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:28:16 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:28:16 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:28:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:28:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:28:16 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:28:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:29:04 [INFO] exp_shallowmodel: train time: 47.626s
01/22/2018 22:29:04 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:29:04 [INFO] exp_shallowmodel: accuracy:   0.556
01/22/2018 22:29:04 [INFO] exp_shallowmodel: f1_score:   0.372
01/22/2018 22:29:04 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:29:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.48      0.59      0.53       169
          F       0.69      0.72      0.71       281
          R       0.31      0.21      0.25       122

avg / total       0.53      0.56      0.54       592

01/22/2018 22:29:04 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:29:04 [INFO] exp_shallowmodel: 
[[  0   5   7   8]
 [  1 100  43  25]
 [  1  51 203  26]
 [  2  54  40  26]]
01/22/2018 22:29:04 [INFO] exp_shallowmodel: ******************** dstc3 - Round 26 
01/22/2018 22:29:04 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:29:04 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:29:04 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:29:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:29:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:29:04 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:29:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:29:55 [INFO] exp_shallowmodel: train time: 50.636s
01/22/2018 22:29:55 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:29:55 [INFO] exp_shallowmodel: accuracy:   0.600
01/22/2018 22:29:55 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 22:29:55 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:29:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.06      0.05      0.05        20
          C       0.55      0.66      0.60       169
          F       0.73      0.73      0.73       281
          R       0.41      0.32      0.36       122

avg / total       0.59      0.60      0.59       592

01/22/2018 22:29:55 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:29:55 [INFO] exp_shallowmodel: 
[[  1   6   8   5]
 [  3 111  29  26]
 [  5  48 204  24]
 [  8  36  39  39]]
01/22/2018 22:29:55 [INFO] exp_shallowmodel: ******************** dstc3 - Round 27 
01/22/2018 22:29:55 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:29:55 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:29:55 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:29:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:29:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:29:55 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:29:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:30:42 [INFO] exp_shallowmodel: train time: 46.779s
01/22/2018 22:30:42 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:30:42 [INFO] exp_shallowmodel: accuracy:   0.578
01/22/2018 22:30:42 [INFO] exp_shallowmodel: f1_score:   0.399
01/22/2018 22:30:42 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:30:42 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.51      0.56      0.53       169
          F       0.72      0.75      0.73       281
          R       0.37      0.30      0.33       122

avg / total       0.56      0.58      0.57       592

01/22/2018 22:30:42 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:30:42 [INFO] exp_shallowmodel: 
[[  0   8   4   8]
 [  5  94  40  30]
 [  2  44 211  24]
 [  5  40  40  37]]
01/22/2018 22:30:42 [INFO] exp_shallowmodel: ******************** dstc3 - Round 28 
01/22/2018 22:30:42 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:30:42 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:30:42 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:30:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:30:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:30:42 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:30:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:31:31 [INFO] exp_shallowmodel: train time: 49.538s
01/22/2018 22:31:31 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:31:31 [INFO] exp_shallowmodel: accuracy:   0.596
01/22/2018 22:31:31 [INFO] exp_shallowmodel: f1_score:   0.432
01/22/2018 22:31:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:31:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.05      0.07        20
          C       0.52      0.63      0.57       169
          F       0.73      0.74      0.74       281
          R       0.39      0.32      0.35       122

avg / total       0.58      0.60      0.59       592

01/22/2018 22:31:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:31:31 [INFO] exp_shallowmodel: 
[[  1   4   8   7]
 [  2 106  29  32]
 [  1  51 207  22]
 [  4  41  38  39]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:31:31 [INFO] exp_shallowmodel: ******************** dstc3 - Round 29 
01/22/2018 22:31:31 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 22:31:31 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:31:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:31:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:31:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:31:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:31:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:32:18 [INFO] exp_shallowmodel: train time: 46.737s
01/22/2018 22:32:18 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:32:18 [INFO] exp_shallowmodel: accuracy:   0.599
01/22/2018 22:32:18 [INFO] exp_shallowmodel: f1_score:   0.426
01/22/2018 22:32:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:32:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.04      0.06        28
          C       0.55      0.63      0.58       172
          F       0.72      0.77      0.74       283
          R       0.37      0.28      0.32       123

avg / total       0.57      0.60      0.58       606

01/22/2018 22:32:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:32:18 [INFO] exp_shallowmodel: 
[[  1   5   8  14]
 [  3 108  37  24]
 [  2  40 219  22]
 [  2  45  41  35]]
01/22/2018 22:32:18 [INFO] exp_shallowmodel: ******************** dstc3 - Round 30 
01/22/2018 22:32:18 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:32:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:32:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:32:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:32:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:32:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:32:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:33:02 [INFO] exp_shallowmodel: train time: 43.904s
01/22/2018 22:33:02 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:33:02 [INFO] exp_shallowmodel: accuracy:   0.623
01/22/2018 22:33:02 [INFO] exp_shallowmodel: f1_score:   0.457
01/22/2018 22:33:02 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:33:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.18      0.10      0.13        20
          C       0.56      0.67      0.61       169
          F       0.73      0.79      0.76       281
          R       0.45      0.27      0.34       122

avg / total       0.60      0.62      0.61       592

01/22/2018 22:33:02 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:33:02 [INFO] exp_shallowmodel: 
[[  2   6   5   7]
 [  5 113  33  18]
 [  1  43 221  16]
 [  3  41  45  33]]
01/22/2018 22:33:02 [INFO] exp_shallowmodel: ******************** dstc3 - Round 31 
01/22/2018 22:33:02 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:33:02 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:33:02 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:33:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:33:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:33:02 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:33:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:33:48 [INFO] exp_shallowmodel: train time: 45.627s
01/22/2018 22:33:48 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:33:48 [INFO] exp_shallowmodel: accuracy:   0.596
01/22/2018 22:33:48 [INFO] exp_shallowmodel: f1_score:   0.457
01/22/2018 22:33:48 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:33:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.15      0.19        20
          C       0.52      0.62      0.57       169
          F       0.73      0.75      0.74       281
          R       0.39      0.30      0.34       122

avg / total       0.58      0.60      0.59       592

01/22/2018 22:33:48 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:33:48 [INFO] exp_shallowmodel: 
[[  3   4   4   9]
 [  2 104  37  26]
 [  3  47 210  21]
 [  4  44  38  36]]
01/22/2018 22:33:48 [INFO] exp_shallowmodel: ******************** dstc3 - Round 32 
01/22/2018 22:33:48 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:33:48 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:33:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:33:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:33:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:33:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:33:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:34:35 [INFO] exp_shallowmodel: train time: 47.036s
01/22/2018 22:34:35 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:34:35 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 22:34:35 [INFO] exp_shallowmodel: f1_score:   0.410
01/22/2018 22:34:35 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:34:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.62      0.57       169
          F       0.72      0.73      0.73       281
          R       0.36      0.32      0.34       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:34:35 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:34:35 [INFO] exp_shallowmodel: 
[[  0   2   8  10]
 [  0 104  34  31]
 [  2  46 206  27]
 [  3  43  37  39]]
01/22/2018 22:34:35 [INFO] exp_shallowmodel: ******************** dstc3 - Round 33 
01/22/2018 22:34:35 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:34:35 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:34:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:34:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:34:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:34:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:34:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:35:22 [INFO] exp_shallowmodel: train time: 46.580s
01/22/2018 22:35:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:35:22 [INFO] exp_shallowmodel: accuracy:   0.566
01/22/2018 22:35:22 [INFO] exp_shallowmodel: f1_score:   0.418
01/22/2018 22:35:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:35:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.10      0.12        20
          C       0.50      0.64      0.56       169
          F       0.71      0.69      0.70       281
          R       0.34      0.25      0.28       122

avg / total       0.56      0.57      0.56       592

01/22/2018 22:35:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:35:22 [INFO] exp_shallowmodel: 
[[  2   4   7   7]
 [  3 109  29  28]
 [  2  61 194  24]
 [  5  44  43  30]]
01/22/2018 22:35:22 [INFO] exp_shallowmodel: ******************** dstc3 - Round 34 
01/22/2018 22:35:22 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:35:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:35:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:35:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:35:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:35:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:35:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:36:07 [INFO] exp_shallowmodel: train time: 44.741s
01/22/2018 22:36:07 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:36:07 [INFO] exp_shallowmodel: accuracy:   0.584
01/22/2018 22:36:07 [INFO] exp_shallowmodel: f1_score:   0.418
01/22/2018 22:36:07 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:36:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.05      0.07        20
          C       0.51      0.64      0.57       169
          F       0.73      0.73      0.73       281
          R       0.37      0.27      0.31       122

avg / total       0.57      0.58      0.57       592

01/22/2018 22:36:07 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:36:07 [INFO] exp_shallowmodel: 
[[  1   4   7   8]
 [  1 108  32  28]
 [  2  55 204  20]
 [  6  46  37  33]]
01/22/2018 22:36:07 [INFO] exp_shallowmodel: ******************** dstc3 - Round 35 
01/22/2018 22:36:07 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:36:07 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:36:07 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:36:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:36:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:36:07 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:36:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:36:52 [INFO] exp_shallowmodel: train time: 44.932s
01/22/2018 22:36:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:36:52 [INFO] exp_shallowmodel: accuracy:   0.600
01/22/2018 22:36:52 [INFO] exp_shallowmodel: f1_score:   0.415
01/22/2018 22:36:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:36:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.62      0.57       169
          F       0.73      0.76      0.74       281
          R       0.40      0.31      0.35       122

avg / total       0.58      0.60      0.59       592

01/22/2018 22:36:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:36:52 [INFO] exp_shallowmodel: 
[[  0   4   9   7]
 [  1 104  35  29]
 [  2  45 213  21]
 [  3  45  36  38]]
01/22/2018 22:36:52 [INFO] exp_shallowmodel: ******************** dstc3 - Round 36 
01/22/2018 22:36:52 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:36:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:36:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:36:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:36:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:36:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:36:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:37:37 [INFO] exp_shallowmodel: train time: 44.719s
01/22/2018 22:37:37 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:37:37 [INFO] exp_shallowmodel: accuracy:   0.586
01/22/2018 22:37:37 [INFO] exp_shallowmodel: f1_score:   0.402
01/22/2018 22:37:37 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:37:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.50      0.62      0.55       169
          F       0.70      0.75      0.72       281
          R       0.46      0.26      0.33       122

avg / total       0.57      0.59      0.57       592

01/22/2018 22:37:37 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:37:37 [INFO] exp_shallowmodel: 
[[  0   6   8   6]
 [  4 105  48  12]
 [  3  48 210  20]
 [  2  52  36  32]]
01/22/2018 22:37:37 [INFO] exp_shallowmodel: ******************** dstc3 - Round 37 
01/22/2018 22:37:37 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:37:37 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:37:37 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:37:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:37:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:37:37 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:37:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:38:22 [INFO] exp_shallowmodel: train time: 45.270s
01/22/2018 22:38:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:38:22 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 22:38:22 [INFO] exp_shallowmodel: f1_score:   0.406
01/22/2018 22:38:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:38:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.51      0.63      0.56       169
          F       0.73      0.74      0.73       281
          R       0.40      0.28      0.33       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:38:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:38:22 [INFO] exp_shallowmodel: 
[[  0   2   8  10]
 [  5 107  33  24]
 [  2  54 208  17]
 [  3  48  37  34]]
01/22/2018 22:38:22 [INFO] exp_shallowmodel: ******************** dstc3 - Round 38 
01/22/2018 22:38:22 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:38:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:38:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:38:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:38:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:38:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:38:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:39:09 [INFO] exp_shallowmodel: train time: 46.704s
01/22/2018 22:39:09 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:39:09 [INFO] exp_shallowmodel: accuracy:   0.573
01/22/2018 22:39:09 [INFO] exp_shallowmodel: f1_score:   0.390
01/22/2018 22:39:09 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:39:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.50      0.57      0.54       169
          F       0.70      0.75      0.72       281
          R       0.36      0.26      0.30       122

avg / total       0.55      0.57      0.56       592

01/22/2018 22:39:09 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:39:09 [INFO] exp_shallowmodel: 
[[  0   6   6   8]
 [  3  97  40  29]
 [  1  50 210  20]
 [  5  40  45  32]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:39:09 [INFO] exp_shallowmodel: ******************** dstc3 - Round 39 
01/22/2018 22:39:09 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 22:39:09 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:39:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:39:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:39:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:39:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:39:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:39:57 [INFO] exp_shallowmodel: train time: 47.578s
01/22/2018 22:39:57 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:39:57 [INFO] exp_shallowmodel: accuracy:   0.584
01/22/2018 22:39:57 [INFO] exp_shallowmodel: f1_score:   0.413
01/22/2018 22:39:57 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:39:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        28
          C       0.51      0.62      0.56       172
          F       0.72      0.73      0.72       283
          R       0.41      0.34      0.37       123

avg / total       0.56      0.58      0.57       606

01/22/2018 22:39:57 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:39:57 [INFO] exp_shallowmodel: 
[[  0   3  13  12]
 [  4 106  34  28]
 [  0  56 206  21]
 [  4  44  33  42]]
01/22/2018 22:39:57 [INFO] exp_shallowmodel: ******************** dstc3 - Round 40 
01/22/2018 22:39:57 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:39:57 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:39:57 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:39:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:39:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:39:57 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:39:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:40:41 [INFO] exp_shallowmodel: train time: 43.564s
01/22/2018 22:40:41 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:40:41 [INFO] exp_shallowmodel: accuracy:   0.600
01/22/2018 22:40:41 [INFO] exp_shallowmodel: f1_score:   0.417
01/22/2018 22:40:41 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:40:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.63      0.58       169
          F       0.70      0.75      0.73       281
          R       0.46      0.30      0.37       122

avg / total       0.58      0.60      0.58       592

01/22/2018 22:40:41 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:40:41 [INFO] exp_shallowmodel: 
[[  0   6   9   5]
 [  4 107  40  18]
 [  0  50 211  20]
 [  6  38  41  37]]
01/22/2018 22:40:41 [INFO] exp_shallowmodel: ******************** dstc3 - Round 41 
01/22/2018 22:40:41 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:40:41 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:40:41 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:40:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:40:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:40:41 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:40:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:41:30 [INFO] exp_shallowmodel: train time: 48.759s
01/22/2018 22:41:30 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:41:30 [INFO] exp_shallowmodel: accuracy:   0.566
01/22/2018 22:41:30 [INFO] exp_shallowmodel: f1_score:   0.387
01/22/2018 22:41:30 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:41:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.56      0.54       169
          F       0.67      0.74      0.71       281
          R       0.34      0.26      0.30       122

avg / total       0.54      0.57      0.55       592

01/22/2018 22:41:30 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:41:30 [INFO] exp_shallowmodel: 
[[  0   2   5  13]
 [  4  94  46  25]
 [  4  44 209  24]
 [  3  37  50  32]]
01/22/2018 22:41:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 42 
01/22/2018 22:41:30 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:41:30 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:41:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:41:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:41:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:41:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:41:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:42:18 [INFO] exp_shallowmodel: train time: 48.529s
01/22/2018 22:42:18 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:42:18 [INFO] exp_shallowmodel: accuracy:   0.588
01/22/2018 22:42:18 [INFO] exp_shallowmodel: f1_score:   0.411
01/22/2018 22:42:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:42:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.50      0.64      0.56       169
          F       0.72      0.72      0.72       281
          R       0.42      0.31      0.36       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:42:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:42:18 [INFO] exp_shallowmodel: 
[[  0  11   4   5]
 [  1 108  37  23]
 [  2  52 202  25]
 [  5  43  36  38]]
01/22/2018 22:42:18 [INFO] exp_shallowmodel: ******************** dstc3 - Round 43 
01/22/2018 22:42:18 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:42:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:42:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:42:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:42:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:42:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:42:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:43:07 [INFO] exp_shallowmodel: train time: 48.741s
01/22/2018 22:43:07 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:43:07 [INFO] exp_shallowmodel: accuracy:   0.584
01/22/2018 22:43:07 [INFO] exp_shallowmodel: f1_score:   0.418
01/22/2018 22:43:07 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:43:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.05      0.08        20
          C       0.51      0.66      0.58       169
          F       0.72      0.72      0.72       281
          R       0.36      0.25      0.30       122

avg / total       0.57      0.58      0.57       592

01/22/2018 22:43:07 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:43:07 [INFO] exp_shallowmodel: 
[[  1   4   9   6]
 [  2 112  33  22]
 [  1  52 202  26]
 [  2  52  37  31]]
01/22/2018 22:43:07 [INFO] exp_shallowmodel: ******************** dstc3 - Round 44 
01/22/2018 22:43:07 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:43:07 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:43:07 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:43:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:43:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:43:07 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:43:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:43:54 [INFO] exp_shallowmodel: train time: 46.909s
01/22/2018 22:43:54 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:43:54 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 22:43:54 [INFO] exp_shallowmodel: f1_score:   0.409
01/22/2018 22:43:54 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:43:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.48      0.57      0.52       169
          F       0.74      0.76      0.75       281
          R       0.43      0.32      0.37       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:43:54 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:43:54 [INFO] exp_shallowmodel: 
[[  0   4   7   9]
 [  3  97  36  33]
 [  2  56 214   9]
 [  2  47  34  39]]
01/22/2018 22:43:54 [INFO] exp_shallowmodel: ******************** dstc3 - Round 45 
01/22/2018 22:43:54 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:43:54 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:43:54 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:43:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:43:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:43:54 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:43:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:44:41 [INFO] exp_shallowmodel: train time: 46.804s
01/22/2018 22:44:41 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:44:41 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 22:44:41 [INFO] exp_shallowmodel: f1_score:   0.458
01/22/2018 22:44:41 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:44:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.15      0.21        20
          C       0.52      0.61      0.56       169
          F       0.70      0.73      0.72       281
          R       0.41      0.30      0.35       122

avg / total       0.58      0.59      0.58       592

01/22/2018 22:44:41 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:44:41 [INFO] exp_shallowmodel: 
[[  3   6   7   4]
 [  1 103  36  29]
 [  2  52 206  21]
 [  3  38  44  37]]
01/22/2018 22:44:41 [INFO] exp_shallowmodel: ******************** dstc3 - Round 46 
01/22/2018 22:44:41 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:44:41 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:44:41 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:44:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:44:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:44:41 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:44:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:45:30 [INFO] exp_shallowmodel: train time: 48.071s
01/22/2018 22:45:30 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:45:30 [INFO] exp_shallowmodel: accuracy:   0.598
01/22/2018 22:45:30 [INFO] exp_shallowmodel: f1_score:   0.441
01/22/2018 22:45:30 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:45:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.15      0.10      0.12        20
          C       0.55      0.62      0.58       169
          F       0.73      0.76      0.74       281
          R       0.36      0.29      0.32       122

avg / total       0.58      0.60      0.59       592

01/22/2018 22:45:30 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:45:30 [INFO] exp_shallowmodel: 
[[  2   3  10   5]
 [  2 104  32  31]
 [  1  42 213  25]
 [  8  41  38  35]]
01/22/2018 22:45:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 47 
01/22/2018 22:45:30 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:45:30 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:45:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:45:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:45:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:45:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:45:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:46:15 [INFO] exp_shallowmodel: train time: 45.640s
01/22/2018 22:46:15 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:46:15 [INFO] exp_shallowmodel: accuracy:   0.562
01/22/2018 22:46:15 [INFO] exp_shallowmodel: f1_score:   0.380
01/22/2018 22:46:15 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:46:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.50      0.62      0.55       169
          F       0.70      0.72      0.71       281
          R       0.31      0.22      0.26       122

avg / total       0.54      0.56      0.55       592

01/22/2018 22:46:15 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:46:15 [INFO] exp_shallowmodel: 
[[  0   2   7  11]
 [  4 104  35  26]
 [  2  55 202  22]
 [  3  47  45  27]]
01/22/2018 22:46:15 [INFO] exp_shallowmodel: ******************** dstc3 - Round 48 
01/22/2018 22:46:15 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:46:15 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:46:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:46:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:46:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:46:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:46:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:47:04 [INFO] exp_shallowmodel: train time: 48.338s
01/22/2018 22:47:04 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:47:04 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 22:47:04 [INFO] exp_shallowmodel: f1_score:   0.457
01/22/2018 22:47:04 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:47:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.10      0.14        20
          C       0.53      0.63      0.58       169
          F       0.73      0.77      0.75       281
          R       0.44      0.31      0.37       122

avg / total       0.60      0.61      0.60       592

01/22/2018 22:47:04 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:47:04 [INFO] exp_shallowmodel: 
[[  2   6   5   7]
 [  2 107  37  23]
 [  1  47 215  18]
 [  4  43  37  38]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:47:04 [INFO] exp_shallowmodel: ******************** dstc3 - Round 49 
01/22/2018 22:47:04 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 22:47:04 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:47:04 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:47:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:47:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:47:04 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:47:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:47:50 [INFO] exp_shallowmodel: train time: 45.716s
01/22/2018 22:47:50 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:47:50 [INFO] exp_shallowmodel: accuracy:   0.583
01/22/2018 22:47:50 [INFO] exp_shallowmodel: f1_score:   0.395
01/22/2018 22:47:50 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:47:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        28
          C       0.51      0.67      0.58       172
          F       0.70      0.74      0.72       283
          R       0.37      0.23      0.28       123

avg / total       0.55      0.58      0.56       606

01/22/2018 22:47:50 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:47:50 [INFO] exp_shallowmodel: 
[[  0   9   9  10]
 [  3 115  34  20]
 [  2  53 210  18]
 [  0  48  47  28]]
01/22/2018 22:47:54 [INFO] exp_shallowmodel: ******************** family - Round 0 
01/22/2018 22:47:54 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:47:54 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:47:54 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:47:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:47:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:47:54 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:47:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:48:07 [INFO] exp_shallowmodel: train time: 12.701s
01/22/2018 22:48:07 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:48:07 [INFO] exp_shallowmodel: accuracy:   0.636
01/22/2018 22:48:07 [INFO] exp_shallowmodel: f1_score:   0.304
01/22/2018 22:48:07 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:48:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.31      0.19      0.23        27
          F       0.75      0.84      0.79       250
          R       0.21      0.17      0.19        52

avg / total       0.59      0.64      0.61       352

01/22/2018 22:48:07 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:48:07 [INFO] exp_shallowmodel: 
[[  0   0  12  11]
 [  0   5  19   3]
 [ 11   9 210  20]
 [  2   2  39   9]]
01/22/2018 22:48:07 [INFO] exp_shallowmodel: ******************** family - Round 1 
01/22/2018 22:48:07 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:48:07 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:48:07 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:48:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:48:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:48:07 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:48:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:48:20 [INFO] exp_shallowmodel: train time: 12.827s
01/22/2018 22:48:20 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:48:20 [INFO] exp_shallowmodel: accuracy:   0.645
01/22/2018 22:48:20 [INFO] exp_shallowmodel: f1_score:   0.325
01/22/2018 22:48:20 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:48:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.09      0.10        23
          C       0.24      0.15      0.18        27
          F       0.76      0.84      0.80       250
          R       0.26      0.19      0.22        52

avg / total       0.60      0.64      0.62       352

01/22/2018 22:48:20 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:48:20 [INFO] exp_shallowmodel: 
[[  2   1  15   5]
 [  4   4  17   2]
 [ 10   8 211  21]
 [  3   4  35  10]]
01/22/2018 22:48:20 [INFO] exp_shallowmodel: ******************** family - Round 2 
01/22/2018 22:48:20 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:48:20 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:48:20 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:48:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:48:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:48:20 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:48:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:48:32 [INFO] exp_shallowmodel: train time: 12.569s
01/22/2018 22:48:32 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:48:32 [INFO] exp_shallowmodel: accuracy:   0.690
01/22/2018 22:48:32 [INFO] exp_shallowmodel: f1_score:   0.347
01/22/2018 22:48:32 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:48:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.09      0.11        23
          C       0.24      0.15      0.18        27
          F       0.79      0.90      0.84       250
          R       0.32      0.21      0.26        52

avg / total       0.63      0.69      0.66       352

01/22/2018 22:48:33 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:48:33 [INFO] exp_shallowmodel: 
[[  2   2  16   3]
 [  3   4  16   4]
 [  3   5 226  16]
 [  6   6  29  11]]
01/22/2018 22:48:33 [INFO] exp_shallowmodel: ******************** family - Round 3 
01/22/2018 22:48:33 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:48:33 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:48:33 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:48:33 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:48:33 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:48:33 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:48:33 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:48:46 [INFO] exp_shallowmodel: train time: 13.392s
01/22/2018 22:48:46 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:48:46 [INFO] exp_shallowmodel: accuracy:   0.659
01/22/2018 22:48:46 [INFO] exp_shallowmodel: f1_score:   0.359
01/22/2018 22:48:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:48:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.19      0.13      0.15        23
          C       0.26      0.22      0.24        27
          F       0.76      0.85      0.80       250
          R       0.32      0.19      0.24        52

avg / total       0.62      0.66      0.63       352

01/22/2018 22:48:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:48:46 [INFO] exp_shallowmodel: 
[[  3   1  14   5]
 [  1   6  18   2]
 [  9  14 213  14]
 [  3   2  37  10]]
01/22/2018 22:48:46 [INFO] exp_shallowmodel: ******************** family - Round 4 
01/22/2018 22:48:46 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:48:46 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:48:46 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:48:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:48:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:48:46 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:48:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:49:00 [INFO] exp_shallowmodel: train time: 13.475s
01/22/2018 22:49:00 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:49:00 [INFO] exp_shallowmodel: accuracy:   0.597
01/22/2018 22:49:00 [INFO] exp_shallowmodel: f1_score:   0.270
01/22/2018 22:49:00 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:49:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        23
          C       0.11      0.07      0.09        27
          F       0.73      0.79      0.76       250
          R       0.19      0.17      0.18        52

avg / total       0.56      0.60      0.58       352

01/22/2018 22:49:00 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:49:00 [INFO] exp_shallowmodel: 
[[  1   5  14   3]
 [  0   2  23   2]
 [ 10   8 198  34]
 [  3   4  36   9]]
01/22/2018 22:49:00 [INFO] exp_shallowmodel: ******************** family - Round 5 
01/22/2018 22:49:00 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:49:00 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:49:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:49:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:49:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:49:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:49:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:49:13 [INFO] exp_shallowmodel: train time: 13.268s
01/22/2018 22:49:13 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:49:13 [INFO] exp_shallowmodel: accuracy:   0.653
01/22/2018 22:49:13 [INFO] exp_shallowmodel: f1_score:   0.354
01/22/2018 22:49:13 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:49:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.18      0.09      0.12        23
          C       0.30      0.22      0.26        27
          F       0.76      0.84      0.80       250
          R       0.26      0.23      0.24        52

avg / total       0.62      0.65      0.63       352

01/22/2018 22:49:13 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:49:13 [INFO] exp_shallowmodel: 
[[  2   3  12   6]
 [  0   6  18   3]
 [  7   8 210  25]
 [  2   3  35  12]]
01/22/2018 22:49:13 [INFO] exp_shallowmodel: ******************** family - Round 6 
01/22/2018 22:49:13 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:49:13 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:49:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:49:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:49:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:49:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:49:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:49:27 [INFO] exp_shallowmodel: train time: 13.659s
01/22/2018 22:49:27 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:49:27 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:49:27 [INFO] exp_shallowmodel: f1_score:   0.274
01/22/2018 22:49:27 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:49:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.18      0.15      0.16        27
          F       0.75      0.84      0.79       250
          R       0.18      0.12      0.14        52

avg / total       0.57      0.62      0.60       352

01/22/2018 22:49:27 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:49:27 [INFO] exp_shallowmodel: 
[[  0   1  18   4]
 [  1   4  18   4]
 [ 11  11 209  19]
 [  7   6  33   6]]
01/22/2018 22:49:27 [INFO] exp_shallowmodel: ******************** family - Round 7 
01/22/2018 22:49:27 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:49:27 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:49:27 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:49:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:49:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:49:27 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:49:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:49:40 [INFO] exp_shallowmodel: train time: 13.006s
01/22/2018 22:49:40 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:49:40 [INFO] exp_shallowmodel: accuracy:   0.648
01/22/2018 22:49:40 [INFO] exp_shallowmodel: f1_score:   0.328
01/22/2018 22:49:40 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:49:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.09      0.04      0.06        23
          C       0.25      0.22      0.24        27
          F       0.76      0.84      0.80       250
          R       0.24      0.19      0.22        52

avg / total       0.60      0.65      0.62       352

01/22/2018 22:49:40 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:49:40 [INFO] exp_shallowmodel: 
[[  1   2  13   7]
 [  1   6  16   4]
 [  6  13 211  20]
 [  3   3  36  10]]
01/22/2018 22:49:40 [INFO] exp_shallowmodel: ******************** family - Round 8 
01/22/2018 22:49:40 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:49:40 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:49:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:49:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:49:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:49:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:49:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:49:52 [INFO] exp_shallowmodel: train time: 12.134s
01/22/2018 22:49:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:49:52 [INFO] exp_shallowmodel: accuracy:   0.631
01/22/2018 22:49:52 [INFO] exp_shallowmodel: f1_score:   0.315
01/22/2018 22:49:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:49:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.06      0.04      0.05        23
          C       0.26      0.26      0.26        27
          F       0.76      0.83      0.79       250
          R       0.19      0.13      0.16        52

avg / total       0.59      0.63      0.61       352

01/22/2018 22:49:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:49:52 [INFO] exp_shallowmodel: 
[[  1   2  16   4]
 [  1   7  15   4]
 [  6  15 207  22]
 [  8   3  34   7]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:49:52 [INFO] exp_shallowmodel: ******************** family - Round 9 
01/22/2018 22:49:52 [INFO] exp_shallowmodel: #(data) = 2816
01/22/2018 22:49:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:49:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:49:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:49:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:49:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:49:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:50:05 [INFO] exp_shallowmodel: train time: 12.909s
01/22/2018 22:50:05 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:50:05 [INFO] exp_shallowmodel: accuracy:   0.633
01/22/2018 22:50:05 [INFO] exp_shallowmodel: f1_score:   0.303
01/22/2018 22:50:05 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:50:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.08      0.12        25
          C       0.22      0.15      0.18        27
          F       0.73      0.86      0.79       251
          R       0.16      0.10      0.12        59

avg / total       0.56      0.63      0.59       362

01/22/2018 22:50:05 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:50:05 [INFO] exp_shallowmodel: 
[[  2   2  16   5]
 [  0   4  20   3]
 [  5   5 217  24]
 [  1   7  45   6]]
01/22/2018 22:50:05 [INFO] exp_shallowmodel: ******************** family - Round 10 
01/22/2018 22:50:05 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:50:05 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:50:05 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:50:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:50:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:50:05 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:50:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:50:18 [INFO] exp_shallowmodel: train time: 13.244s
01/22/2018 22:50:18 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:50:18 [INFO] exp_shallowmodel: accuracy:   0.642
01/22/2018 22:50:18 [INFO] exp_shallowmodel: f1_score:   0.307
01/22/2018 22:50:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:50:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.27      0.15      0.19        27
          F       0.76      0.84      0.80       250
          R       0.26      0.23      0.24        52

avg / total       0.59      0.64      0.62       352

01/22/2018 22:50:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:50:18 [INFO] exp_shallowmodel: 
[[  0   2  19   2]
 [  0   4  14   9]
 [  8   8 210  24]
 [  4   1  35  12]]
01/22/2018 22:50:18 [INFO] exp_shallowmodel: ******************** family - Round 11 
01/22/2018 22:50:18 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:50:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:50:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:50:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:50:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:50:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:50:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:50:31 [INFO] exp_shallowmodel: train time: 12.771s
01/22/2018 22:50:31 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:50:31 [INFO] exp_shallowmodel: accuracy:   0.642
01/22/2018 22:50:31 [INFO] exp_shallowmodel: f1_score:   0.318
01/22/2018 22:50:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:50:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.04      0.06        23
          C       0.29      0.19      0.23        27
          F       0.75      0.84      0.79       250
          R       0.20      0.17      0.19        52

avg / total       0.59      0.64      0.61       352

01/22/2018 22:50:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:50:31 [INFO] exp_shallowmodel: 
[[  1   1  19   2]
 [  0   5  18   4]
 [  5   5 211  29]
 [  2   6  35   9]]
01/22/2018 22:50:31 [INFO] exp_shallowmodel: ******************** family - Round 12 
01/22/2018 22:50:31 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:50:31 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:50:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:50:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:50:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:50:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:50:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:50:46 [INFO] exp_shallowmodel: train time: 14.265s
01/22/2018 22:50:46 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:50:46 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:50:46 [INFO] exp_shallowmodel: f1_score:   0.301
01/22/2018 22:50:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:50:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.09      0.11        23
          C       0.14      0.11      0.12        27
          F       0.75      0.82      0.78       250
          R       0.21      0.17      0.19        52

avg / total       0.58      0.62      0.60       352

01/22/2018 22:50:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:50:46 [INFO] exp_shallowmodel: 
[[  2   1  13   7]
 [  2   3  18   4]
 [  8  15 205  22]
 [  2   3  38   9]]
01/22/2018 22:50:46 [INFO] exp_shallowmodel: ******************** family - Round 13 
01/22/2018 22:50:46 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:50:46 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:50:46 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:50:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:50:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:50:46 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:50:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:50:59 [INFO] exp_shallowmodel: train time: 13.243s
01/22/2018 22:50:59 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:50:59 [INFO] exp_shallowmodel: accuracy:   0.670
01/22/2018 22:50:59 [INFO] exp_shallowmodel: f1_score:   0.337
01/22/2018 22:50:59 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:50:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.13      0.13      0.13        23
          C       0.33      0.11      0.17        27
          F       0.77      0.88      0.82       250
          R       0.28      0.19      0.23        52

avg / total       0.63      0.67      0.64       352

01/22/2018 22:50:59 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:50:59 [INFO] exp_shallowmodel: 
[[  3   0  15   5]
 [  4   3  16   4]
 [  9   4 220  17]
 [  7   2  33  10]]
01/22/2018 22:50:59 [INFO] exp_shallowmodel: ******************** family - Round 14 
01/22/2018 22:50:59 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:50:59 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:50:59 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:50:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:50:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:50:59 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:50:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:51:13 [INFO] exp_shallowmodel: train time: 14.088s
01/22/2018 22:51:13 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:51:13 [INFO] exp_shallowmodel: accuracy:   0.673
01/22/2018 22:51:13 [INFO] exp_shallowmodel: f1_score:   0.348
01/22/2018 22:51:13 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:51:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.13      0.09      0.11        23
          C       0.24      0.15      0.18        27
          F       0.78      0.87      0.82       250
          R       0.32      0.25      0.28        52

avg / total       0.63      0.67      0.65       352

01/22/2018 22:51:13 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:51:13 [INFO] exp_shallowmodel: 
[[  2   2  13   6]
 [  1   4  17   5]
 [  8   7 218  17]
 [  4   4  31  13]]
01/22/2018 22:51:13 [INFO] exp_shallowmodel: ******************** family - Round 15 
01/22/2018 22:51:13 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:51:13 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:51:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:51:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:51:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:51:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:51:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:51:26 [INFO] exp_shallowmodel: train time: 12.809s
01/22/2018 22:51:26 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:51:26 [INFO] exp_shallowmodel: accuracy:   0.631
01/22/2018 22:51:26 [INFO] exp_shallowmodel: f1_score:   0.324
01/22/2018 22:51:26 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:51:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.09      0.11        23
          C       0.26      0.19      0.22        27
          F       0.75      0.82      0.79       250
          R       0.20      0.17      0.19        52

avg / total       0.59      0.63      0.61       352

01/22/2018 22:51:26 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:51:26 [INFO] exp_shallowmodel: 
[[  2   2  13   6]
 [  1   5  16   5]
 [  8  11 206  25]
 [  3   1  39   9]]
01/22/2018 22:51:26 [INFO] exp_shallowmodel: ******************** family - Round 16 
01/22/2018 22:51:26 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:51:26 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:51:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:51:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:51:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:51:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:51:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:51:39 [INFO] exp_shallowmodel: train time: 12.860s
01/22/2018 22:51:39 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:51:39 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:51:39 [INFO] exp_shallowmodel: f1_score:   0.301
01/22/2018 22:51:39 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:51:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.06      0.04      0.05        23
          C       0.30      0.22      0.26        27
          F       0.76      0.83      0.79       250
          R       0.12      0.10      0.11        52

avg / total       0.58      0.62      0.60       352

01/22/2018 22:51:39 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:51:39 [INFO] exp_shallowmodel: 
[[  1   2  11   9]
 [  0   6  17   4]
 [ 13   6 207  24]
 [  4   6  37   5]]
01/22/2018 22:51:39 [INFO] exp_shallowmodel: ******************** family - Round 17 
01/22/2018 22:51:39 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:51:39 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:51:39 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:51:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:51:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:51:39 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:51:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:51:53 [INFO] exp_shallowmodel: train time: 13.602s
01/22/2018 22:51:53 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:51:53 [INFO] exp_shallowmodel: accuracy:   0.653
01/22/2018 22:51:53 [INFO] exp_shallowmodel: f1_score:   0.339
01/22/2018 22:51:53 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:51:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        23
          C       0.35      0.26      0.30        27
          F       0.76      0.85      0.80       250
          R       0.24      0.17      0.20        52

avg / total       0.61      0.65      0.63       352

01/22/2018 22:51:53 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:51:53 [INFO] exp_shallowmodel: 
[[  1   2  16   4]
 [  1   7  18   1]
 [  7   7 213  23]
 [  6   4  33   9]]
01/22/2018 22:51:53 [INFO] exp_shallowmodel: ******************** family - Round 18 
01/22/2018 22:51:53 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:51:53 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:51:53 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:51:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:51:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:51:53 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:51:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:52:06 [INFO] exp_shallowmodel: train time: 13.356s
01/22/2018 22:52:06 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:52:06 [INFO] exp_shallowmodel: accuracy:   0.634
01/22/2018 22:52:06 [INFO] exp_shallowmodel: f1_score:   0.336
01/22/2018 22:52:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:52:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.15      0.17      0.16        23
          C       0.29      0.15      0.20        27
          F       0.75      0.82      0.79       250
          R       0.23      0.17      0.20        52

avg / total       0.60      0.63      0.61       352

01/22/2018 22:52:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:52:06 [INFO] exp_shallowmodel: 
[[  4   1  14   4]
 [  2   4  17   4]
 [ 18   4 206  22]
 [  2   5  36   9]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:52:06 [INFO] exp_shallowmodel: ******************** family - Round 19 
01/22/2018 22:52:06 [INFO] exp_shallowmodel: #(data) = 2816
01/22/2018 22:52:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:52:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:52:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:52:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:52:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:52:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:52:21 [INFO] exp_shallowmodel: train time: 14.442s
01/22/2018 22:52:21 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:52:21 [INFO] exp_shallowmodel: accuracy:   0.627
01/22/2018 22:52:21 [INFO] exp_shallowmodel: f1_score:   0.303
01/22/2018 22:52:21 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:52:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.08      0.10        25
          C       0.20      0.07      0.11        27
          F       0.74      0.84      0.79       251
          R       0.24      0.20      0.22        59

avg / total       0.58      0.63      0.60       362

01/22/2018 22:52:21 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:52:21 [INFO] exp_shallowmodel: 
[[  2   2  13   8]
 [  1   2  19   5]
 [  9   5 211  26]
 [  5   1  41  12]]
01/22/2018 22:52:21 [INFO] exp_shallowmodel: ******************** family - Round 20 
01/22/2018 22:52:21 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:52:21 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:52:21 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:52:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:52:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:52:21 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:52:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:52:34 [INFO] exp_shallowmodel: train time: 12.847s
01/22/2018 22:52:34 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:52:34 [INFO] exp_shallowmodel: accuracy:   0.653
01/22/2018 22:52:34 [INFO] exp_shallowmodel: f1_score:   0.330
01/22/2018 22:52:34 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:52:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.15      0.09      0.11        23
          C       0.16      0.11      0.13        27
          F       0.76      0.85      0.80       250
          R       0.31      0.25      0.28        52

avg / total       0.61      0.65      0.63       352

01/22/2018 22:52:34 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:52:34 [INFO] exp_shallowmodel: 
[[  2   1  17   3]
 [  1   3  20   3]
 [  8   7 212  23]
 [  2   8  29  13]]
01/22/2018 22:52:34 [INFO] exp_shallowmodel: ******************** family - Round 21 
01/22/2018 22:52:34 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:52:34 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:52:34 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:52:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:52:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:52:34 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:52:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:52:47 [INFO] exp_shallowmodel: train time: 13.835s
01/22/2018 22:52:47 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:52:47 [INFO] exp_shallowmodel: accuracy:   0.653
01/22/2018 22:52:47 [INFO] exp_shallowmodel: f1_score:   0.336
01/22/2018 22:52:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:52:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.13      0.09      0.11        23
          C       0.35      0.22      0.27        27
          F       0.74      0.86      0.80       250
          R       0.23      0.13      0.17        52

avg / total       0.60      0.65      0.62       352

01/22/2018 22:52:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:52:47 [INFO] exp_shallowmodel: 
[[  2   1  15   5]
 [  0   6  19   2]
 [ 11   8 215  16]
 [  2   2  41   7]]
01/22/2018 22:52:48 [INFO] exp_shallowmodel: ******************** family - Round 22 
01/22/2018 22:52:48 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:52:48 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:52:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:52:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:52:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:52:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:52:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:53:00 [INFO] exp_shallowmodel: train time: 12.411s
01/22/2018 22:53:00 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:53:00 [INFO] exp_shallowmodel: accuracy:   0.639
01/22/2018 22:53:00 [INFO] exp_shallowmodel: f1_score:   0.313
01/22/2018 22:53:00 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:53:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.13      0.15        23
          C       0.04      0.04      0.04        27
          F       0.77      0.84      0.80       250
          R       0.32      0.23      0.27        52

avg / total       0.60      0.64      0.62       352

01/22/2018 22:53:00 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:53:00 [INFO] exp_shallowmodel: 
[[  3   1  14   5]
 [  1   1  19   6]
 [  9  17 209  15]
 [  5   4  31  12]]
01/22/2018 22:53:00 [INFO] exp_shallowmodel: ******************** family - Round 23 
01/22/2018 22:53:00 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:53:00 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:53:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:53:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:53:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:53:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:53:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:53:13 [INFO] exp_shallowmodel: train time: 12.548s
01/22/2018 22:53:13 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:53:13 [INFO] exp_shallowmodel: accuracy:   0.631
01/22/2018 22:53:13 [INFO] exp_shallowmodel: f1_score:   0.310
01/22/2018 22:53:13 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:53:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.17      0.19        23
          C       0.05      0.04      0.04        27
          F       0.76      0.83      0.79       250
          R       0.26      0.19      0.22        52

avg / total       0.59      0.63      0.61       352

01/22/2018 22:53:13 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:53:13 [INFO] exp_shallowmodel: 
[[  4   1  12   6]
 [  3   1  20   3]
 [ 11  13 207  19]
 [  2   5  35  10]]
01/22/2018 22:53:13 [INFO] exp_shallowmodel: ******************** family - Round 24 
01/22/2018 22:53:13 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:53:13 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:53:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:53:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:53:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:53:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:53:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:53:25 [INFO] exp_shallowmodel: train time: 11.940s
01/22/2018 22:53:25 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:53:25 [INFO] exp_shallowmodel: accuracy:   0.651
01/22/2018 22:53:25 [INFO] exp_shallowmodel: f1_score:   0.297
01/22/2018 22:53:25 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:53:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        23
          C       0.23      0.11      0.15        27
          F       0.77      0.87      0.82       250
          R       0.19      0.15      0.17        52

avg / total       0.60      0.65      0.62       352

01/22/2018 22:53:25 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:53:25 [INFO] exp_shallowmodel: 
[[  1   3  12   7]
 [  2   3  16   6]
 [  8   4 217  21]
 [  4   3  37   8]]
01/22/2018 22:53:25 [INFO] exp_shallowmodel: ******************** family - Round 25 
01/22/2018 22:53:25 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:53:25 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:53:25 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:53:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:53:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:53:25 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:53:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:53:38 [INFO] exp_shallowmodel: train time: 13.005s
01/22/2018 22:53:38 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:53:38 [INFO] exp_shallowmodel: accuracy:   0.648
01/22/2018 22:53:38 [INFO] exp_shallowmodel: f1_score:   0.319
01/22/2018 22:53:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:53:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.13      0.16        23
          C       0.08      0.04      0.05        27
          F       0.75      0.84      0.79       250
          R       0.30      0.25      0.27        52

avg / total       0.60      0.65      0.62       352

01/22/2018 22:53:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:53:38 [INFO] exp_shallowmodel: 
[[  3   0  15   5]
 [  1   1  21   4]
 [  9   8 211  22]
 [  2   3  34  13]]
01/22/2018 22:53:38 [INFO] exp_shallowmodel: ******************** family - Round 26 
01/22/2018 22:53:38 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:53:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:53:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:53:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:53:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:53:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:53:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:53:51 [INFO] exp_shallowmodel: train time: 13.687s
01/22/2018 22:53:51 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:53:51 [INFO] exp_shallowmodel: accuracy:   0.662
01/22/2018 22:53:51 [INFO] exp_shallowmodel: f1_score:   0.379
01/22/2018 22:53:51 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:53:51 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.09      0.10        23
          C       0.35      0.22      0.27        27
          F       0.77      0.83      0.80       250
          R       0.37      0.33      0.35        52

avg / total       0.63      0.66      0.65       352

01/22/2018 22:53:51 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:53:51 [INFO] exp_shallowmodel: 
[[  2   2  16   3]
 [  2   6  16   3]
 [ 12   7 208  23]
 [  2   2  31  17]]
01/22/2018 22:53:52 [INFO] exp_shallowmodel: ******************** family - Round 27 
01/22/2018 22:53:52 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:53:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:53:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:53:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:53:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:53:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:53:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:54:05 [INFO] exp_shallowmodel: train time: 13.901s
01/22/2018 22:54:05 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:54:05 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 22:54:05 [INFO] exp_shallowmodel: f1_score:   0.319
01/22/2018 22:54:05 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:54:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.16      0.17      0.17        23
          C       0.18      0.11      0.14        27
          F       0.76      0.79      0.78       250
          R       0.20      0.19      0.20        52

avg / total       0.59      0.61      0.60       352

01/22/2018 22:54:05 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:54:05 [INFO] exp_shallowmodel: 
[[  4   2  13   4]
 [  3   3  13   8]
 [ 14  10 198  28]
 [  4   2  36  10]]
01/22/2018 22:54:06 [INFO] exp_shallowmodel: ******************** family - Round 28 
01/22/2018 22:54:06 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:54:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:54:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:54:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:54:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:54:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:54:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:54:20 [INFO] exp_shallowmodel: train time: 14.892s
01/22/2018 22:54:20 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:54:20 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 22:54:20 [INFO] exp_shallowmodel: f1_score:   0.309
01/22/2018 22:54:20 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:54:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        23
          C       0.37      0.26      0.30        27
          F       0.74      0.84      0.79       250
          R       0.11      0.08      0.09        52

avg / total       0.58      0.63      0.60       352

01/22/2018 22:54:20 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:54:20 [INFO] exp_shallowmodel: 
[[  1   0  13   9]
 [  0   7  16   4]
 [ 11  10 209  20]
 [  3   2  43   4]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:54:21 [INFO] exp_shallowmodel: ******************** family - Round 29 
01/22/2018 22:54:21 [INFO] exp_shallowmodel: #(data) = 2816
01/22/2018 22:54:21 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:54:21 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:54:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:54:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:54:21 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:54:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:54:34 [INFO] exp_shallowmodel: train time: 13.731s
01/22/2018 22:54:34 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:54:34 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:54:34 [INFO] exp_shallowmodel: f1_score:   0.337
01/22/2018 22:54:34 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:54:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        25
          C       0.31      0.37      0.34        27
          F       0.76      0.82      0.79       251
          R       0.20      0.15      0.17        59

avg / total       0.59      0.62      0.60       362

01/22/2018 22:54:34 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:54:34 [INFO] exp_shallowmodel: 
[[  1   4  11   9]
 [  0  10  13   4]
 [ 10  13 205  23]
 [  3   5  42   9]]
01/22/2018 22:54:34 [INFO] exp_shallowmodel: ******************** family - Round 30 
01/22/2018 22:54:34 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:54:34 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:54:34 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:54:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:54:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:54:34 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:54:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:54:48 [INFO] exp_shallowmodel: train time: 14.056s
01/22/2018 22:54:48 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:54:48 [INFO] exp_shallowmodel: accuracy:   0.642
01/22/2018 22:54:48 [INFO] exp_shallowmodel: f1_score:   0.349
01/22/2018 22:54:48 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:54:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.08      0.09      0.09        23
          C       0.25      0.22      0.24        27
          F       0.79      0.82      0.80       250
          R       0.30      0.25      0.27        52

avg / total       0.63      0.64      0.63       352

01/22/2018 22:54:48 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:54:48 [INFO] exp_shallowmodel: 
[[  2   1  13   7]
 [  1   6  17   3]
 [ 13  12 205  20]
 [  8   5  26  13]]
01/22/2018 22:54:49 [INFO] exp_shallowmodel: ******************** family - Round 31 
01/22/2018 22:54:49 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:54:49 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:54:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:54:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:54:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:54:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:54:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:55:02 [INFO] exp_shallowmodel: train time: 13.713s
01/22/2018 22:55:02 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:55:02 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:55:02 [INFO] exp_shallowmodel: f1_score:   0.261
01/22/2018 22:55:02 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:55:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.23      0.11      0.15        27
          F       0.73      0.84      0.78       250
          R       0.13      0.10      0.11        52

avg / total       0.56      0.62      0.58       352

01/22/2018 22:55:02 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:55:02 [INFO] exp_shallowmodel: 
[[  0   2  16   5]
 [  0   3  23   1]
 [  6   6 211  27]
 [  6   2  39   5]]
01/22/2018 22:55:02 [INFO] exp_shallowmodel: ******************** family - Round 32 
01/22/2018 22:55:02 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:55:02 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:55:02 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:55:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:55:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:55:02 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:55:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:55:15 [INFO] exp_shallowmodel: train time: 12.269s
01/22/2018 22:55:15 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:55:15 [INFO] exp_shallowmodel: accuracy:   0.670
01/22/2018 22:55:15 [INFO] exp_shallowmodel: f1_score:   0.346
01/22/2018 22:55:15 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:55:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.09      0.12        23
          C       0.44      0.15      0.22        27
          F       0.76      0.88      0.81       250
          R       0.24      0.21      0.23        52

avg / total       0.62      0.67      0.64       352

01/22/2018 22:55:15 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:55:15 [INFO] exp_shallowmodel: 
[[  2   1  14   6]
 [  0   4  17   6]
 [  8   1 219  22]
 [  0   3  38  11]]
01/22/2018 22:55:15 [INFO] exp_shallowmodel: ******************** family - Round 33 
01/22/2018 22:55:15 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:55:15 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:55:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:55:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:55:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:55:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:55:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:55:28 [INFO] exp_shallowmodel: train time: 13.406s
01/22/2018 22:55:28 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:55:28 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:55:28 [INFO] exp_shallowmodel: f1_score:   0.300
01/22/2018 22:55:28 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:55:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.09      0.09        23
          C       0.27      0.15      0.19        27
          F       0.75      0.83      0.79       250
          R       0.15      0.12      0.13        52

avg / total       0.58      0.62      0.60       352

01/22/2018 22:55:28 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:55:28 [INFO] exp_shallowmodel: 
[[  2   1  15   5]
 [  1   4  14   8]
 [ 14   7 207  22]
 [  4   3  39   6]]
01/22/2018 22:55:28 [INFO] exp_shallowmodel: ******************** family - Round 34 
01/22/2018 22:55:28 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:55:28 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:55:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:55:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:55:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:55:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:55:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:55:41 [INFO] exp_shallowmodel: train time: 13.327s
01/22/2018 22:55:41 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:55:41 [INFO] exp_shallowmodel: accuracy:   0.634
01/22/2018 22:55:41 [INFO] exp_shallowmodel: f1_score:   0.275
01/22/2018 22:55:41 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:55:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.07      0.07      0.07        27
          F       0.75      0.84      0.79       250
          R       0.29      0.19      0.23        52

avg / total       0.58      0.63      0.60       352

01/22/2018 22:55:41 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:55:41 [INFO] exp_shallowmodel: 
[[  0   4  16   3]
 [  2   2  18   5]
 [  7  16 211  16]
 [  0   6  36  10]]
01/22/2018 22:55:42 [INFO] exp_shallowmodel: ******************** family - Round 35 
01/22/2018 22:55:42 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:55:42 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:55:42 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:55:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:55:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:55:42 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:55:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:55:56 [INFO] exp_shallowmodel: train time: 14.536s
01/22/2018 22:55:56 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:55:56 [INFO] exp_shallowmodel: accuracy:   0.636
01/22/2018 22:55:56 [INFO] exp_shallowmodel: f1_score:   0.317
01/22/2018 22:55:56 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:55:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.06      0.04      0.05        23
          C       0.22      0.19      0.20        27
          F       0.75      0.83      0.79       250
          R       0.29      0.19      0.23        52

avg / total       0.59      0.64      0.61       352

01/22/2018 22:55:56 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:55:56 [INFO] exp_shallowmodel: 
[[  1   2  15   5]
 [  0   5  19   3]
 [ 11  14 208  17]
 [  4   2  36  10]]
01/22/2018 22:55:56 [INFO] exp_shallowmodel: ******************** family - Round 36 
01/22/2018 22:55:56 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:55:56 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:55:56 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:55:56 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:55:56 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:55:56 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:55:56 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:56:08 [INFO] exp_shallowmodel: train time: 12.160s
01/22/2018 22:56:08 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:56:08 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 22:56:08 [INFO] exp_shallowmodel: f1_score:   0.316
01/22/2018 22:56:08 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:56:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.13      0.09      0.11        23
          C       0.26      0.22      0.24        27
          F       0.75      0.83      0.79       250
          R       0.15      0.12      0.13        52

avg / total       0.59      0.63      0.60       352

01/22/2018 22:56:08 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:56:08 [INFO] exp_shallowmodel: 
[[  2   3  15   3]
 [  2   6  16   3]
 [  8   8 207  27]
 [  3   6  37   6]]
01/22/2018 22:56:08 [INFO] exp_shallowmodel: ******************** family - Round 37 
01/22/2018 22:56:08 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:56:08 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:56:08 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:56:08 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:56:08 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:56:08 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:56:08 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:56:23 [INFO] exp_shallowmodel: train time: 14.318s
01/22/2018 22:56:23 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:56:23 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 22:56:23 [INFO] exp_shallowmodel: f1_score:   0.306
01/22/2018 22:56:23 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:56:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.15      0.09      0.11        23
          C       0.17      0.11      0.13        27
          F       0.72      0.83      0.77       250
          R       0.26      0.17      0.21        52

avg / total       0.57      0.63      0.60       352

01/22/2018 22:56:23 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:56:23 [INFO] exp_shallowmodel: 
[[  2   2  15   4]
 [  1   3  23   0]
 [ 10  11 207  22]
 [  0   2  41   9]]
01/22/2018 22:56:23 [INFO] exp_shallowmodel: ******************** family - Round 38 
01/22/2018 22:56:23 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:56:23 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:56:23 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:56:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:56:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:56:23 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:56:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:56:36 [INFO] exp_shallowmodel: train time: 13.309s
01/22/2018 22:56:36 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:56:36 [INFO] exp_shallowmodel: accuracy:   0.656
01/22/2018 22:56:36 [INFO] exp_shallowmodel: f1_score:   0.387
01/22/2018 22:56:36 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:56:36 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.23      0.22      0.22        23
          C       0.29      0.22      0.25        27
          F       0.78      0.83      0.80       250
          R       0.31      0.25      0.28        52

avg / total       0.63      0.66      0.64       352

01/22/2018 22:56:36 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:56:36 [INFO] exp_shallowmodel: 
[[  5   1  13   4]
 [  2   6  17   2]
 [ 10  10 207  23]
 [  5   4  30  13]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:56:36 [INFO] exp_shallowmodel: ******************** family - Round 39 
01/22/2018 22:56:36 [INFO] exp_shallowmodel: #(data) = 2816
01/22/2018 22:56:36 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:56:36 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:56:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:56:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:56:36 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:56:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:56:50 [INFO] exp_shallowmodel: train time: 13.645s
01/22/2018 22:56:50 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:56:50 [INFO] exp_shallowmodel: accuracy:   0.599
01/22/2018 22:56:50 [INFO] exp_shallowmodel: f1_score:   0.269
01/22/2018 22:56:50 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:56:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        25
          C       0.10      0.07      0.08        27
          F       0.73      0.82      0.77       251
          R       0.19      0.15      0.17        59

avg / total       0.55      0.60      0.57       362

01/22/2018 22:56:50 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:56:50 [INFO] exp_shallowmodel: 
[[  1   0  18   6]
 [  3   2  16   6]
 [  7  13 205  26]
 [  4   6  40   9]]
01/22/2018 22:56:50 [INFO] exp_shallowmodel: ******************** family - Round 40 
01/22/2018 22:56:50 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:56:50 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:56:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:56:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:56:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:56:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:56:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:57:03 [INFO] exp_shallowmodel: train time: 12.548s
01/22/2018 22:57:03 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:57:03 [INFO] exp_shallowmodel: accuracy:   0.636
01/22/2018 22:57:03 [INFO] exp_shallowmodel: f1_score:   0.328
01/22/2018 22:57:03 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:57:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        23
          C       0.38      0.30      0.33        27
          F       0.76      0.84      0.79       250
          R       0.15      0.12      0.13        52

avg / total       0.59      0.64      0.61       352

01/22/2018 22:57:03 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:57:03 [INFO] exp_shallowmodel: 
[[  1   1  17   4]
 [  2   8  13   4]
 [  5  10 209  26]
 [  7   2  37   6]]
01/22/2018 22:57:03 [INFO] exp_shallowmodel: ******************** family - Round 41 
01/22/2018 22:57:03 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:57:03 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:57:03 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:57:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:57:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:57:03 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:57:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:57:17 [INFO] exp_shallowmodel: train time: 14.306s
01/22/2018 22:57:17 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:57:17 [INFO] exp_shallowmodel: accuracy:   0.639
01/22/2018 22:57:17 [INFO] exp_shallowmodel: f1_score:   0.293
01/22/2018 22:57:17 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:57:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.14      0.11      0.12        27
          F       0.75      0.84      0.79       250
          R       0.29      0.23      0.26        52

avg / total       0.59      0.64      0.61       352

01/22/2018 22:57:17 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:57:17 [INFO] exp_shallowmodel: 
[[  0   2  15   6]
 [  1   3  22   1]
 [  7  10 210  23]
 [  1   7  32  12]]
01/22/2018 22:57:17 [INFO] exp_shallowmodel: ******************** family - Round 42 
01/22/2018 22:57:17 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:57:17 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:57:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:57:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:57:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:57:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:57:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:57:31 [INFO] exp_shallowmodel: train time: 14.018s
01/22/2018 22:57:31 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:57:31 [INFO] exp_shallowmodel: accuracy:   0.651
01/22/2018 22:57:31 [INFO] exp_shallowmodel: f1_score:   0.317
01/22/2018 22:57:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:57:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.13      0.13        23
          C       0.19      0.11      0.14        27
          F       0.77      0.86      0.81       250
          R       0.25      0.15      0.19        52

avg / total       0.60      0.65      0.62       352

01/22/2018 22:57:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:57:31 [INFO] exp_shallowmodel: 
[[  3   2  14   4]
 [  2   3  17   5]
 [ 11   9 215  15]
 [  8   2  34   8]]
01/22/2018 22:57:31 [INFO] exp_shallowmodel: ******************** family - Round 43 
01/22/2018 22:57:31 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:57:31 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:57:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:57:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:57:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:57:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:57:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:57:44 [INFO] exp_shallowmodel: train time: 12.776s
01/22/2018 22:57:44 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:57:44 [INFO] exp_shallowmodel: accuracy:   0.651
01/22/2018 22:57:44 [INFO] exp_shallowmodel: f1_score:   0.327
01/22/2018 22:57:44 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:57:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.09      0.10        23
          C       0.24      0.15      0.18        27
          F       0.77      0.85      0.81       250
          R       0.25      0.19      0.22        52

avg / total       0.61      0.65      0.63       352

01/22/2018 22:57:44 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:57:44 [INFO] exp_shallowmodel: 
[[  2   1  15   5]
 [  5   4  13   5]
 [  7  10 213  20]
 [  3   2  37  10]]
01/22/2018 22:57:44 [INFO] exp_shallowmodel: ******************** family - Round 44 
01/22/2018 22:57:44 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:57:44 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:57:44 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:57:44 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:57:44 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:57:44 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:57:44 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:57:57 [INFO] exp_shallowmodel: train time: 12.754s
01/22/2018 22:57:57 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:57:57 [INFO] exp_shallowmodel: accuracy:   0.659
01/22/2018 22:57:57 [INFO] exp_shallowmodel: f1_score:   0.328
01/22/2018 22:57:57 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:57:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.04      0.06        23
          C       0.29      0.19      0.23        27
          F       0.76      0.86      0.81       250
          R       0.25      0.19      0.22        52

avg / total       0.60      0.66      0.63       352

01/22/2018 22:57:57 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:57:57 [INFO] exp_shallowmodel: 
[[  1   1  15   6]
 [  2   5  16   4]
 [  5   9 216  20]
 [  2   2  38  10]]
01/22/2018 22:57:57 [INFO] exp_shallowmodel: ******************** family - Round 45 
01/22/2018 22:57:57 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:57:57 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:57:57 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:57:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:57:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:57:57 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:57:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:58:10 [INFO] exp_shallowmodel: train time: 12.839s
01/22/2018 22:58:10 [INFO] exp_shallowmodel: test time:  0.000s
01/22/2018 22:58:10 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 22:58:10 [INFO] exp_shallowmodel: f1_score:   0.328
01/22/2018 22:58:10 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:58:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.16      0.13      0.14        23
          C       0.15      0.15      0.15        27
          F       0.77      0.81      0.79       250
          R       0.24      0.21      0.23        52

avg / total       0.61      0.63      0.62       352

01/22/2018 22:58:10 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:58:10 [INFO] exp_shallowmodel: 
[[  3   4  13   3]
 [  0   4  17   6]
 [ 10  12 203  25]
 [  6   6  29  11]]
01/22/2018 22:58:10 [INFO] exp_shallowmodel: ******************** family - Round 46 
01/22/2018 22:58:10 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:58:10 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:58:10 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:58:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:58:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:58:10 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:58:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:58:22 [INFO] exp_shallowmodel: train time: 12.235s
01/22/2018 22:58:22 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:58:22 [INFO] exp_shallowmodel: accuracy:   0.631
01/22/2018 22:58:22 [INFO] exp_shallowmodel: f1_score:   0.331
01/22/2018 22:58:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:58:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.13      0.12        23
          C       0.26      0.22      0.24        27
          F       0.74      0.82      0.78       250
          R       0.29      0.13      0.18        52

avg / total       0.60      0.63      0.61       352

01/22/2018 22:58:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:58:22 [INFO] exp_shallowmodel: 
[[  3   1  16   3]
 [  1   6  19   1]
 [ 18  13 206  13]
 [  5   3  37   7]]
01/22/2018 22:58:22 [INFO] exp_shallowmodel: ******************** family - Round 47 
01/22/2018 22:58:22 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:58:22 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:58:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:58:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:58:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:58:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:58:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:58:35 [INFO] exp_shallowmodel: train time: 13.244s
01/22/2018 22:58:35 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:58:35 [INFO] exp_shallowmodel: accuracy:   0.665
01/22/2018 22:58:35 [INFO] exp_shallowmodel: f1_score:   0.337
01/22/2018 22:58:35 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:58:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.09      0.09      0.09        23
          C       0.28      0.19      0.22        27
          F       0.78      0.87      0.82       250
          R       0.28      0.17      0.21        52

avg / total       0.62      0.66      0.64       352

01/22/2018 22:58:35 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:58:35 [INFO] exp_shallowmodel: 
[[  2   2  15   4]
 [  1   5  16   5]
 [ 12   6 218  14]
 [  8   5  30   9]]
01/22/2018 22:58:35 [INFO] exp_shallowmodel: ******************** family - Round 48 
01/22/2018 22:58:35 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 22:58:35 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:58:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:58:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:58:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:58:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:58:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:58:48 [INFO] exp_shallowmodel: train time: 12.734s
01/22/2018 22:58:48 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:58:48 [INFO] exp_shallowmodel: accuracy:   0.653
01/22/2018 22:58:48 [INFO] exp_shallowmodel: f1_score:   0.317
01/22/2018 22:58:48 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:58:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.07      0.04      0.05        23
          C       0.29      0.15      0.20        27
          F       0.77      0.86      0.81       250
          R       0.23      0.19      0.21        52

avg / total       0.60      0.65      0.62       352

01/22/2018 22:58:48 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:58:48 [INFO] exp_shallowmodel: 
[[  1   2  16   4]
 [  0   4  17   6]
 [  8   4 215  23]
 [  5   4  33  10]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:58:48 [INFO] exp_shallowmodel: ******************** family - Round 49 
01/22/2018 22:58:48 [INFO] exp_shallowmodel: #(data) = 2816
01/22/2018 22:58:48 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:58:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:58:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:58:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:58:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:58:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:59:01 [INFO] exp_shallowmodel: train time: 12.659s
01/22/2018 22:59:01 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:59:01 [INFO] exp_shallowmodel: accuracy:   0.627
01/22/2018 22:59:01 [INFO] exp_shallowmodel: f1_score:   0.300
01/22/2018 22:59:01 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:59:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.08      0.04      0.05        25
          C       0.24      0.15      0.18        27
          F       0.74      0.85      0.79       251
          R       0.21      0.15      0.18        59

avg / total       0.57      0.63      0.59       362

01/22/2018 22:59:01 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:59:01 [INFO] exp_shallowmodel: 
[[  1   0  20   4]
 [  2   4  15   6]
 [  8   6 213  24]
 [  2   7  41   9]]
01/22/2018 22:59:06 [INFO] exp_shallowmodel: ******************** ghome - Round 0 
01/22/2018 22:59:06 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 22:59:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:59:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:59:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:59:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:59:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:59:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:59:29 [INFO] exp_shallowmodel: train time: 23.109s
01/22/2018 22:59:29 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:59:29 [INFO] exp_shallowmodel: accuracy:   0.720
01/22/2018 22:59:29 [INFO] exp_shallowmodel: f1_score:   0.376
01/22/2018 22:59:29 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:59:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.29      0.30        59
          C       0.17      0.08      0.11        12
          F       0.83      0.87      0.85       396
          R       0.29      0.22      0.25        55

avg / total       0.70      0.72      0.71       522

01/22/2018 22:59:29 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:59:29 [INFO] exp_shallowmodel: 
[[ 17   0  35   7]
 [  5   1   4   2]
 [ 26   4 346  20]
 [  8   1  34  12]]
01/22/2018 22:59:29 [INFO] exp_shallowmodel: ******************** ghome - Round 1 
01/22/2018 22:59:29 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 22:59:29 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:59:29 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:59:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:59:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:59:29 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:59:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:59:51 [INFO] exp_shallowmodel: train time: 22.323s
01/22/2018 22:59:51 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 22:59:51 [INFO] exp_shallowmodel: accuracy:   0.741
01/22/2018 22:59:51 [INFO] exp_shallowmodel: f1_score:   0.353
01/22/2018 22:59:51 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:59:51 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.27      0.30        59
          C       0.00      0.00      0.00        12
          F       0.83      0.91      0.87       396
          R       0.33      0.20      0.25        55

avg / total       0.70      0.74      0.72       522

01/22/2018 22:59:51 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:59:51 [INFO] exp_shallowmodel: 
[[ 16   1  35   7]
 [  1   0   8   3]
 [ 23   1 360  12]
 [  9   2  33  11]]
01/22/2018 22:59:52 [INFO] exp_shallowmodel: ******************** ghome - Round 2 
01/22/2018 22:59:52 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 22:59:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 22:59:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:59:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:59:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:59:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:59:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:00:14 [INFO] exp_shallowmodel: train time: 22.888s
01/22/2018 23:00:14 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:00:14 [INFO] exp_shallowmodel: accuracy:   0.713
01/22/2018 23:00:14 [INFO] exp_shallowmodel: f1_score:   0.318
01/22/2018 23:00:14 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:00:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.24      0.22      0.23        59
          C       0.00      0.00      0.00        12
          F       0.81      0.89      0.85       396
          R       0.29      0.15      0.19        55

avg / total       0.68      0.71      0.69       522

01/22/2018 23:00:14 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:00:14 [INFO] exp_shallowmodel: 
[[ 13   3  37   6]
 [  4   0   7   1]
 [ 27   5 351  13]
 [ 10   1  36   8]]
01/22/2018 23:00:15 [INFO] exp_shallowmodel: ******************** ghome - Round 3 
01/22/2018 23:00:15 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:00:15 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:00:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:00:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:00:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:00:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:00:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:00:39 [INFO] exp_shallowmodel: train time: 24.160s
01/22/2018 23:00:39 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:00:39 [INFO] exp_shallowmodel: accuracy:   0.701
01/22/2018 23:00:39 [INFO] exp_shallowmodel: f1_score:   0.330
01/22/2018 23:00:39 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:00:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.25      0.27        59
          C       0.00      0.00      0.00        12
          F       0.81      0.86      0.84       396
          R       0.23      0.20      0.22        55

avg / total       0.67      0.70      0.69       522

01/22/2018 23:00:39 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:00:39 [INFO] exp_shallowmodel: 
[[ 15   1  34   9]
 [  1   0   9   2]
 [ 28   3 340  25]
 [  8   1  35  11]]
01/22/2018 23:00:39 [INFO] exp_shallowmodel: ******************** ghome - Round 4 
01/22/2018 23:00:39 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:00:39 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:00:39 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:00:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:00:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:00:39 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:00:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:01:03 [INFO] exp_shallowmodel: train time: 23.901s
01/22/2018 23:01:03 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:01:03 [INFO] exp_shallowmodel: accuracy:   0.709
01/22/2018 23:01:03 [INFO] exp_shallowmodel: f1_score:   0.405
01/22/2018 23:01:03 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:01:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.28      0.24      0.26        59
          C       0.67      0.17      0.27        12
          F       0.82      0.86      0.84       396
          R       0.26      0.25      0.26        55

avg / total       0.70      0.71      0.70       522

01/22/2018 23:01:03 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:01:03 [INFO] exp_shallowmodel: 
[[ 14   0  35  10]
 [  0   2   8   2]
 [ 27   1 340  28]
 [  9   0  32  14]]
01/22/2018 23:01:03 [INFO] exp_shallowmodel: ******************** ghome - Round 5 
01/22/2018 23:01:03 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:01:03 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:01:03 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:01:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:01:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:01:03 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:01:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:01:28 [INFO] exp_shallowmodel: train time: 24.818s
01/22/2018 23:01:28 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:01:28 [INFO] exp_shallowmodel: accuracy:   0.697
01/22/2018 23:01:28 [INFO] exp_shallowmodel: f1_score:   0.339
01/22/2018 23:01:28 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:01:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.26      0.25      0.26        59
          C       0.00      0.00      0.00        12
          F       0.81      0.85      0.83       396
          R       0.31      0.24      0.27        55

avg / total       0.68      0.70      0.69       522

01/22/2018 23:01:28 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:01:28 [INFO] exp_shallowmodel: 
[[ 15   2  36   6]
 [  3   0   8   1]
 [ 33   5 336  22]
 [  7   2  33  13]]
01/22/2018 23:01:28 [INFO] exp_shallowmodel: ******************** ghome - Round 6 
01/22/2018 23:01:28 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:01:28 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:01:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:01:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:01:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:01:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:01:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:01:52 [INFO] exp_shallowmodel: train time: 23.660s
01/22/2018 23:01:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:01:52 [INFO] exp_shallowmodel: accuracy:   0.732
01/22/2018 23:01:52 [INFO] exp_shallowmodel: f1_score:   0.374
01/22/2018 23:01:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:01:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.17      0.21        59
          C       0.25      0.17      0.20        12
          F       0.81      0.91      0.86       396
          R       0.31      0.18      0.23        55

avg / total       0.68      0.73      0.70       522

01/22/2018 23:01:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:01:52 [INFO] exp_shallowmodel: 
[[ 10   2  41   6]
 [  2   2   7   1]
 [ 18   3 360  15]
 [  7   1  37  10]]
01/22/2018 23:01:52 [INFO] exp_shallowmodel: ******************** ghome - Round 7 
01/22/2018 23:01:52 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:01:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:01:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:01:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:01:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:01:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:01:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:02:16 [INFO] exp_shallowmodel: train time: 24.768s
01/22/2018 23:02:16 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:02:16 [INFO] exp_shallowmodel: accuracy:   0.711
01/22/2018 23:02:16 [INFO] exp_shallowmodel: f1_score:   0.398
01/22/2018 23:02:16 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:02:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.17      0.18        59
          C       0.33      0.25      0.29        12
          F       0.81      0.87      0.84       396
          R       0.36      0.24      0.29        55

avg / total       0.68      0.71      0.69       522

01/22/2018 23:02:16 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:02:16 [INFO] exp_shallowmodel: 
[[ 10   1  40   8]
 [  2   3   7   0]
 [ 31   5 345  15]
 [  8   0  34  13]]
01/22/2018 23:02:17 [INFO] exp_shallowmodel: ******************** ghome - Round 8 
01/22/2018 23:02:17 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:02:17 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:02:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:02:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:02:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:02:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:02:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:02:41 [INFO] exp_shallowmodel: train time: 24.416s
01/22/2018 23:02:41 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:02:41 [INFO] exp_shallowmodel: accuracy:   0.703
01/22/2018 23:02:41 [INFO] exp_shallowmodel: f1_score:   0.326
01/22/2018 23:02:41 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:02:41 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.21      0.17      0.19        59
          C       0.10      0.08      0.09        12
          F       0.80      0.88      0.84       396
          R       0.27      0.15      0.19        55

avg / total       0.66      0.70      0.68       522

01/22/2018 23:02:41 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:02:41 [INFO] exp_shallowmodel: 
[[ 10   3  41   5]
 [  2   1   9   0]
 [ 28   3 348  17]
 [  8   3  36   8]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:02:41 [INFO] exp_shallowmodel: ******************** ghome - Round 9 
01/22/2018 23:02:41 [INFO] exp_shallowmodel: #(data) = 4176
01/22/2018 23:02:41 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:02:41 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:02:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:02:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:02:41 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:02:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:03:06 [INFO] exp_shallowmodel: train time: 25.081s
01/22/2018 23:03:06 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:03:06 [INFO] exp_shallowmodel: accuracy:   0.703
01/22/2018 23:03:06 [INFO] exp_shallowmodel: f1_score:   0.346
01/22/2018 23:03:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:03:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.27      0.30        64
          C       0.12      0.07      0.09        14
          F       0.80      0.89      0.84       402
          R       0.21      0.13      0.16        63

avg / total       0.66      0.70      0.68       543

01/22/2018 23:03:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:03:06 [INFO] exp_shallowmodel: 
[[ 17   1  41   5]
 [  3   1   6   4]
 [ 21   4 356  21]
 [ 10   2  43   8]]
01/22/2018 23:03:06 [INFO] exp_shallowmodel: ******************** ghome - Round 10 
01/22/2018 23:03:06 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:03:06 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:03:06 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:03:06 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:03:06 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:03:06 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:03:06 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:03:31 [INFO] exp_shallowmodel: train time: 24.710s
01/22/2018 23:03:31 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:03:31 [INFO] exp_shallowmodel: accuracy:   0.699
01/22/2018 23:03:31 [INFO] exp_shallowmodel: f1_score:   0.313
01/22/2018 23:03:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:03:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.24      0.22      0.23        59
          C       0.00      0.00      0.00        12
          F       0.81      0.87      0.84       396
          R       0.21      0.16      0.18        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:03:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:03:31 [INFO] exp_shallowmodel: 
[[ 13   0  36  10]
 [  2   0   6   4]
 [ 33   0 343  20]
 [  7   3  36   9]]
01/22/2018 23:03:31 [INFO] exp_shallowmodel: ******************** ghome - Round 11 
01/22/2018 23:03:31 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:03:31 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:03:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:03:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:03:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:03:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:03:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:03:54 [INFO] exp_shallowmodel: train time: 23.086s
01/22/2018 23:03:54 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:03:54 [INFO] exp_shallowmodel: accuracy:   0.713
01/22/2018 23:03:54 [INFO] exp_shallowmodel: f1_score:   0.313
01/22/2018 23:03:54 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:03:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.17      0.19        59
          C       0.00      0.00      0.00        12
          F       0.82      0.89      0.85       396
          R       0.29      0.16      0.21        55

avg / total       0.67      0.71      0.69       522

01/22/2018 23:03:54 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:03:54 [INFO] exp_shallowmodel: 
[[ 10   5  39   5]
 [  5   0   6   1]
 [ 21   6 353  16]
 [ 10   1  35   9]]
01/22/2018 23:03:54 [INFO] exp_shallowmodel: ******************** ghome - Round 12 
01/22/2018 23:03:54 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:03:54 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:03:54 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:03:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:03:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:03:54 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:03:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:04:18 [INFO] exp_shallowmodel: train time: 23.538s
01/22/2018 23:04:18 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:04:18 [INFO] exp_shallowmodel: accuracy:   0.724
01/22/2018 23:04:18 [INFO] exp_shallowmodel: f1_score:   0.377
01/22/2018 23:04:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:04:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.27      0.30        59
          C       0.11      0.08      0.10        12
          F       0.83      0.88      0.85       396
          R       0.30      0.24      0.27        55

avg / total       0.70      0.72      0.71       522

01/22/2018 23:04:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:04:18 [INFO] exp_shallowmodel: 
[[ 16   3  33   7]
 [  3   1   7   1]
 [ 24   2 348  22]
 [  6   3  33  13]]
01/22/2018 23:04:18 [INFO] exp_shallowmodel: ******************** ghome - Round 13 
01/22/2018 23:04:18 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:04:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:04:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:04:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:04:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:04:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:04:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:04:43 [INFO] exp_shallowmodel: train time: 24.721s
01/22/2018 23:04:43 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:04:43 [INFO] exp_shallowmodel: accuracy:   0.686
01/22/2018 23:04:43 [INFO] exp_shallowmodel: f1_score:   0.306
01/22/2018 23:04:43 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:04:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.14      0.15        59
          C       0.00      0.00      0.00        12
          F       0.80      0.85      0.82       396
          R       0.29      0.22      0.25        55

avg / total       0.65      0.69      0.67       522

01/22/2018 23:04:43 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:04:43 [INFO] exp_shallowmodel: 
[[  8   1  43   7]
 [  1   0  10   1]
 [ 30   7 338  21]
 [  8   1  34  12]]
01/22/2018 23:04:43 [INFO] exp_shallowmodel: ******************** ghome - Round 14 
01/22/2018 23:04:43 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:04:43 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:04:43 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:04:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:04:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:04:43 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:04:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:05:07 [INFO] exp_shallowmodel: train time: 23.878s
01/22/2018 23:05:07 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:05:07 [INFO] exp_shallowmodel: accuracy:   0.701
01/22/2018 23:05:07 [INFO] exp_shallowmodel: f1_score:   0.333
01/22/2018 23:05:07 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:05:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.19      0.17      0.18        59
          C       0.10      0.08      0.09        12
          F       0.82      0.87      0.84       396
          R       0.29      0.18      0.22        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:05:07 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:05:07 [INFO] exp_shallowmodel: 
[[ 10   3  40   6]
 [  3   1   7   1]
 [ 28   5 345  18]
 [ 13   1  31  10]]
01/22/2018 23:05:07 [INFO] exp_shallowmodel: ******************** ghome - Round 15 
01/22/2018 23:05:07 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:05:07 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:05:07 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:05:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:05:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:05:07 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:05:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:05:31 [INFO] exp_shallowmodel: train time: 23.866s
01/22/2018 23:05:31 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:05:31 [INFO] exp_shallowmodel: accuracy:   0.728
01/22/2018 23:05:31 [INFO] exp_shallowmodel: f1_score:   0.365
01/22/2018 23:05:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:05:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.31      0.34        59
          C       0.17      0.08      0.11        12
          F       0.82      0.89      0.85       396
          R       0.20      0.13      0.16        55

avg / total       0.69      0.73      0.70       522

01/22/2018 23:05:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:05:31 [INFO] exp_shallowmodel: 
[[ 18   1  34   6]
 [  2   1   8   1]
 [ 18   3 354  21]
 [  9   1  38   7]]
01/22/2018 23:05:31 [INFO] exp_shallowmodel: ******************** ghome - Round 16 
01/22/2018 23:05:31 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:05:31 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:05:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:05:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:05:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:05:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:05:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:05:55 [INFO] exp_shallowmodel: train time: 24.132s
01/22/2018 23:05:55 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:05:55 [INFO] exp_shallowmodel: accuracy:   0.692
01/22/2018 23:05:55 [INFO] exp_shallowmodel: f1_score:   0.302
01/22/2018 23:05:55 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:05:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.17      0.19        59
          C       0.00      0.00      0.00        12
          F       0.80      0.86      0.83       396
          R       0.24      0.16      0.19        55

avg / total       0.65      0.69      0.67       522

01/22/2018 23:05:55 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:05:55 [INFO] exp_shallowmodel: 
[[ 10   1  41   7]
 [  2   0   9   1]
 [ 30   3 342  21]
 [  7   1  38   9]]
01/22/2018 23:05:55 [INFO] exp_shallowmodel: ******************** ghome - Round 17 
01/22/2018 23:05:55 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:05:55 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:05:55 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:05:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:05:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:05:55 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:05:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:06:18 [INFO] exp_shallowmodel: train time: 22.562s
01/22/2018 23:06:18 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:06:18 [INFO] exp_shallowmodel: accuracy:   0.705
01/22/2018 23:06:18 [INFO] exp_shallowmodel: f1_score:   0.333
01/22/2018 23:06:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:06:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.15      0.17        59
          C       0.17      0.08      0.11        12
          F       0.82      0.88      0.85       396
          R       0.22      0.18      0.20        55

avg / total       0.67      0.70      0.69       522

01/22/2018 23:06:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:06:18 [INFO] exp_shallowmodel: 
[[  9   2  38  10]
 [  2   1   7   2]
 [ 24   1 348  23]
 [ 10   2  33  10]]
01/22/2018 23:06:18 [INFO] exp_shallowmodel: ******************** ghome - Round 18 
01/22/2018 23:06:18 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:06:18 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:06:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:06:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:06:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:06:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:06:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:06:40 [INFO] exp_shallowmodel: train time: 22.225s
01/22/2018 23:06:40 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:06:40 [INFO] exp_shallowmodel: accuracy:   0.730
01/22/2018 23:06:40 [INFO] exp_shallowmodel: f1_score:   0.368
01/22/2018 23:06:40 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:06:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.31      0.34        59
          C       0.00      0.00      0.00        12
          F       0.82      0.88      0.85       396
          R       0.31      0.25      0.28        55

avg / total       0.70      0.73      0.71       522

01/22/2018 23:06:40 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:06:40 [INFO] exp_shallowmodel: 
[[ 18   1  34   6]
 [  1   0   9   2]
 [ 22   2 349  23]
 [  5   1  35  14]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:06:40 [INFO] exp_shallowmodel: ******************** ghome - Round 19 
01/22/2018 23:06:40 [INFO] exp_shallowmodel: #(data) = 4176
01/22/2018 23:06:40 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:06:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:06:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:06:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:06:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:06:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:07:03 [INFO] exp_shallowmodel: train time: 23.213s
01/22/2018 23:07:03 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:07:03 [INFO] exp_shallowmodel: accuracy:   0.683
01/22/2018 23:07:03 [INFO] exp_shallowmodel: f1_score:   0.340
01/22/2018 23:07:03 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:07:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.18      0.11      0.14        64
          C       0.29      0.14      0.19        14
          F       0.79      0.87      0.83       402
          R       0.23      0.19      0.21        63

avg / total       0.64      0.68      0.66       543

01/22/2018 23:07:03 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:07:03 [INFO] exp_shallowmodel: 
[[  7   1  43  13]
 [  2   2   8   2]
 [ 24   2 350  26]
 [  5   2  44  12]]
01/22/2018 23:07:04 [INFO] exp_shallowmodel: ******************** ghome - Round 20 
01/22/2018 23:07:04 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:07:04 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:07:04 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:07:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:07:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:07:04 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:07:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:07:27 [INFO] exp_shallowmodel: train time: 23.237s
01/22/2018 23:07:27 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:07:27 [INFO] exp_shallowmodel: accuracy:   0.720
01/22/2018 23:07:27 [INFO] exp_shallowmodel: f1_score:   0.352
01/22/2018 23:07:27 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:07:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.24      0.26        59
          C       0.10      0.08      0.09        12
          F       0.81      0.89      0.85       396
          R       0.27      0.16      0.20        55

avg / total       0.68      0.72      0.70       522

01/22/2018 23:07:27 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:07:27 [INFO] exp_shallowmodel: 
[[ 14   3  35   7]
 [  0   1   9   2]
 [ 25   4 352  15]
 [  8   2  36   9]]
01/22/2018 23:07:27 [INFO] exp_shallowmodel: ******************** ghome - Round 21 
01/22/2018 23:07:27 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:07:27 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:07:27 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:07:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:07:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:07:27 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:07:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:07:50 [INFO] exp_shallowmodel: train time: 22.754s
01/22/2018 23:07:50 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:07:50 [INFO] exp_shallowmodel: accuracy:   0.705
01/22/2018 23:07:50 [INFO] exp_shallowmodel: f1_score:   0.341
01/22/2018 23:07:50 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:07:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.28      0.25      0.27        59
          C       0.25      0.08      0.12        12
          F       0.80      0.87      0.84       396
          R       0.18      0.11      0.14        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:07:50 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:07:50 [INFO] exp_shallowmodel: 
[[ 15   0  38   6]
 [  3   1   6   2]
 [ 29   2 346  19]
 [  7   1  41   6]]
01/22/2018 23:07:50 [INFO] exp_shallowmodel: ******************** ghome - Round 22 
01/22/2018 23:07:50 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:07:50 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:07:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:07:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:07:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:07:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:07:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:08:14 [INFO] exp_shallowmodel: train time: 23.752s
01/22/2018 23:08:14 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:08:14 [INFO] exp_shallowmodel: accuracy:   0.720
01/22/2018 23:08:14 [INFO] exp_shallowmodel: f1_score:   0.408
01/22/2018 23:08:14 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:08:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.32      0.29      0.30        59
          C       0.33      0.25      0.29        12
          F       0.82      0.88      0.85       396
          R       0.24      0.16      0.20        55

avg / total       0.69      0.72      0.70       522

01/22/2018 23:08:14 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:08:14 [INFO] exp_shallowmodel: 
[[ 17   2  33   7]
 [  1   3   8   0]
 [ 25   3 347  21]
 [ 10   1  35   9]]
01/22/2018 23:08:14 [INFO] exp_shallowmodel: ******************** ghome - Round 23 
01/22/2018 23:08:14 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:08:14 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:08:14 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:08:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:08:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:08:14 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:08:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:08:38 [INFO] exp_shallowmodel: train time: 24.400s
01/22/2018 23:08:38 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:08:38 [INFO] exp_shallowmodel: accuracy:   0.743
01/22/2018 23:08:38 [INFO] exp_shallowmodel: f1_score:   0.408
01/22/2018 23:08:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:08:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.27      0.28        59
          C       0.25      0.08      0.12        12
          F       0.84      0.89      0.87       396
          R       0.39      0.33      0.36        55

avg / total       0.72      0.74      0.73       522

01/22/2018 23:08:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:08:38 [INFO] exp_shallowmodel: 
[[ 16   1  28  14]
 [  3   1   7   1]
 [ 29   1 353  13]
 [  6   1  30  18]]
01/22/2018 23:08:38 [INFO] exp_shallowmodel: ******************** ghome - Round 24 
01/22/2018 23:08:38 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:08:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:08:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:08:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:08:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:08:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:08:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:09:03 [INFO] exp_shallowmodel: train time: 24.883s
01/22/2018 23:09:03 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:09:03 [INFO] exp_shallowmodel: accuracy:   0.739
01/22/2018 23:09:03 [INFO] exp_shallowmodel: f1_score:   0.360
01/22/2018 23:09:03 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:09:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.31      0.19      0.23        59
          C       0.33      0.08      0.13        12
          F       0.80      0.92      0.86       396
          R       0.31      0.16      0.21        55

avg / total       0.68      0.74      0.70       522

01/22/2018 23:09:03 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:09:03 [INFO] exp_shallowmodel: 
[[ 11   0  45   3]
 [  3   1   6   2]
 [ 15   1 365  15]
 [  7   1  38   9]]
01/22/2018 23:09:03 [INFO] exp_shallowmodel: ******************** ghome - Round 25 
01/22/2018 23:09:03 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:09:03 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:09:03 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:09:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:09:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:09:03 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:09:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:09:26 [INFO] exp_shallowmodel: train time: 22.981s
01/22/2018 23:09:26 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:09:26 [INFO] exp_shallowmodel: accuracy:   0.713
01/22/2018 23:09:26 [INFO] exp_shallowmodel: f1_score:   0.354
01/22/2018 23:09:26 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:09:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.24      0.20      0.22        59
          C       0.11      0.08      0.10        12
          F       0.82      0.88      0.85       396
          R       0.30      0.22      0.25        55

avg / total       0.68      0.71      0.70       522

01/22/2018 23:09:26 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:09:26 [INFO] exp_shallowmodel: 
[[ 12   2  39   6]
 [  4   1   6   1]
 [ 27   1 347  21]
 [  7   5  31  12]]
01/22/2018 23:09:26 [INFO] exp_shallowmodel: ******************** ghome - Round 26 
01/22/2018 23:09:26 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:09:26 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:09:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:09:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:09:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:09:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:09:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:09:49 [INFO] exp_shallowmodel: train time: 22.779s
01/22/2018 23:09:49 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:09:49 [INFO] exp_shallowmodel: accuracy:   0.701
01/22/2018 23:09:49 [INFO] exp_shallowmodel: f1_score:   0.326
01/22/2018 23:09:49 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:09:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.20      0.24        59
          C       0.00      0.00      0.00        12
          F       0.81      0.86      0.84       396
          R       0.24      0.22      0.23        55

avg / total       0.67      0.70      0.69       522

01/22/2018 23:09:49 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:09:49 [INFO] exp_shallowmodel: 
[[ 12   1  33  13]
 [  0   0  11   1]
 [ 24   6 342  24]
 [  6   2  35  12]]
01/22/2018 23:09:49 [INFO] exp_shallowmodel: ******************** ghome - Round 27 
01/22/2018 23:09:49 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:09:49 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:09:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:09:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:09:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:09:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:09:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:10:15 [INFO] exp_shallowmodel: train time: 25.871s
01/22/2018 23:10:15 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:10:15 [INFO] exp_shallowmodel: accuracy:   0.716
01/22/2018 23:10:15 [INFO] exp_shallowmodel: f1_score:   0.332
01/22/2018 23:10:15 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:10:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.26      0.24      0.25        59
          C       0.00      0.00      0.00        12
          F       0.82      0.88      0.85       396
          R       0.31      0.18      0.23        55

avg / total       0.68      0.72      0.70       522

01/22/2018 23:10:15 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:10:15 [INFO] exp_shallowmodel: 
[[ 14   2  39   4]
 [  2   0   8   2]
 [ 25   5 350  16]
 [ 12   1  32  10]]
01/22/2018 23:10:15 [INFO] exp_shallowmodel: ******************** ghome - Round 28 
01/22/2018 23:10:15 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:10:15 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:10:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:10:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:10:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:10:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:10:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:10:40 [INFO] exp_shallowmodel: train time: 24.702s
01/22/2018 23:10:40 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:10:40 [INFO] exp_shallowmodel: accuracy:   0.716
01/22/2018 23:10:40 [INFO] exp_shallowmodel: f1_score:   0.366
01/22/2018 23:10:40 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:10:40 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.22      0.24        59
          C       0.14      0.08      0.11        12
          F       0.82      0.88      0.85       396
          R       0.33      0.24      0.28        55

avg / total       0.69      0.72      0.70       522

01/22/2018 23:10:40 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:10:40 [INFO] exp_shallowmodel: 
[[ 13   2  36   8]
 [  2   1   9   0]
 [ 28   3 347  18]
 [  8   1  33  13]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:10:40 [INFO] exp_shallowmodel: ******************** ghome - Round 29 
01/22/2018 23:10:40 [INFO] exp_shallowmodel: #(data) = 4176
01/22/2018 23:10:40 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:10:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:10:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:10:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:10:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:10:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:11:05 [INFO] exp_shallowmodel: train time: 24.438s
01/22/2018 23:11:05 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:11:05 [INFO] exp_shallowmodel: accuracy:   0.722
01/22/2018 23:11:05 [INFO] exp_shallowmodel: f1_score:   0.351
01/22/2018 23:11:05 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:11:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.25      0.27        64
          C       0.00      0.00      0.00        14
          F       0.83      0.90      0.86       402
          R       0.36      0.22      0.27        63

avg / total       0.69      0.72      0.70       543

01/22/2018 23:11:05 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:11:05 [INFO] exp_shallowmodel: 
[[ 16   4  34  10]
 [  5   0   8   1]
 [ 22   4 362  14]
 [ 13   2  34  14]]
01/22/2018 23:11:05 [INFO] exp_shallowmodel: ******************** ghome - Round 30 
01/22/2018 23:11:05 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:11:05 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:11:05 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:11:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:11:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:11:05 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:11:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:11:30 [INFO] exp_shallowmodel: train time: 25.022s
01/22/2018 23:11:30 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:11:30 [INFO] exp_shallowmodel: accuracy:   0.699
01/22/2018 23:11:30 [INFO] exp_shallowmodel: f1_score:   0.303
01/22/2018 23:11:30 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:11:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.28      0.22      0.25        59
          C       0.00      0.00      0.00        12
          F       0.82      0.87      0.84       396
          R       0.13      0.11      0.12        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:11:30 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:11:30 [INFO] exp_shallowmodel: 
[[ 13   2  30  14]
 [  4   0   6   2]
 [ 23   4 346  23]
 [  6   2  41   6]]
01/22/2018 23:11:30 [INFO] exp_shallowmodel: ******************** ghome - Round 31 
01/22/2018 23:11:30 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:11:30 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:11:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:11:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:11:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:11:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:11:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:11:52 [INFO] exp_shallowmodel: train time: 22.309s
01/22/2018 23:11:52 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:11:52 [INFO] exp_shallowmodel: accuracy:   0.697
01/22/2018 23:11:52 [INFO] exp_shallowmodel: f1_score:   0.305
01/22/2018 23:11:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:11:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.15      0.17        59
          C       0.00      0.00      0.00        12
          F       0.80      0.87      0.83       396
          R       0.25      0.18      0.21        55

avg / total       0.66      0.70      0.67       522

01/22/2018 23:11:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:11:52 [INFO] exp_shallowmodel: 
[[  9   3  38   9]
 [  2   0   9   1]
 [ 28   3 345  20]
 [  5   1  39  10]]
01/22/2018 23:11:52 [INFO] exp_shallowmodel: ******************** ghome - Round 32 
01/22/2018 23:11:52 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:11:52 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:11:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:11:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:11:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:11:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:11:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:12:15 [INFO] exp_shallowmodel: train time: 22.490s
01/22/2018 23:12:15 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:12:15 [INFO] exp_shallowmodel: accuracy:   0.705
01/22/2018 23:12:15 [INFO] exp_shallowmodel: f1_score:   0.330
01/22/2018 23:12:15 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:12:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.24      0.25        59
          C       0.08      0.08      0.08        12
          F       0.81      0.88      0.84       396
          R       0.21      0.11      0.14        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:12:15 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:12:15 [INFO] exp_shallowmodel: 
[[ 14   4  34   7]
 [  1   1   8   2]
 [ 28   7 347  14]
 [  9   0  40   6]]
01/22/2018 23:12:15 [INFO] exp_shallowmodel: ******************** ghome - Round 33 
01/22/2018 23:12:15 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:12:15 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:12:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:12:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:12:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:12:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:12:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:12:38 [INFO] exp_shallowmodel: train time: 22.670s
01/22/2018 23:12:38 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:12:38 [INFO] exp_shallowmodel: accuracy:   0.730
01/22/2018 23:12:38 [INFO] exp_shallowmodel: f1_score:   0.347
01/22/2018 23:12:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:12:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.31      0.34        59
          C       0.00      0.00      0.00        12
          F       0.82      0.89      0.86       396
          R       0.25      0.16      0.20        55

avg / total       0.69      0.73      0.71       522

01/22/2018 23:12:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:12:38 [INFO] exp_shallowmodel: 
[[ 18   0  34   7]
 [  0   0   8   4]
 [ 22   4 354  16]
 [  8   2  36   9]]
01/22/2018 23:12:38 [INFO] exp_shallowmodel: ******************** ghome - Round 34 
01/22/2018 23:12:38 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:12:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:12:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:12:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:12:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:12:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:12:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:13:01 [INFO] exp_shallowmodel: train time: 23.454s
01/22/2018 23:13:01 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:13:01 [INFO] exp_shallowmodel: accuracy:   0.715
01/22/2018 23:13:01 [INFO] exp_shallowmodel: f1_score:   0.357
01/22/2018 23:13:01 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:13:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.28      0.27      0.28        59
          C       0.14      0.08      0.11        12
          F       0.82      0.88      0.85       396
          R       0.25      0.16      0.20        55

avg / total       0.69      0.71      0.70       522

01/22/2018 23:13:01 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:13:01 [INFO] exp_shallowmodel: 
[[ 16   0  35   8]
 [  1   1   8   2]
 [ 27   5 347  17]
 [ 13   1  32   9]]
01/22/2018 23:13:01 [INFO] exp_shallowmodel: ******************** ghome - Round 35 
01/22/2018 23:13:01 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:13:01 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:13:01 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:13:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:13:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:13:01 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:13:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:13:25 [INFO] exp_shallowmodel: train time: 23.536s
01/22/2018 23:13:25 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:13:25 [INFO] exp_shallowmodel: accuracy:   0.703
01/22/2018 23:13:25 [INFO] exp_shallowmodel: f1_score:   0.336
01/22/2018 23:13:25 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:13:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.12      0.14        59
          C       0.20      0.08      0.12        12
          F       0.81      0.87      0.84       396
          R       0.25      0.24      0.25        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:13:25 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:13:25 [INFO] exp_shallowmodel: 
[[  7   0  41  11]
 [  2   1   7   2]
 [ 22   3 346  25]
 [ 10   1  31  13]]
01/22/2018 23:13:25 [INFO] exp_shallowmodel: ******************** ghome - Round 36 
01/22/2018 23:13:25 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:13:25 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:13:25 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:13:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:13:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:13:25 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:13:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:13:49 [INFO] exp_shallowmodel: train time: 24.390s
01/22/2018 23:13:49 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:13:49 [INFO] exp_shallowmodel: accuracy:   0.751
01/22/2018 23:13:49 [INFO] exp_shallowmodel: f1_score:   0.448
01/22/2018 23:13:49 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:13:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.35      0.29      0.31        59
          C       0.30      0.25      0.27        12
          F       0.84      0.90      0.87       396
          R       0.40      0.29      0.34        55

avg / total       0.73      0.75      0.74       522

01/22/2018 23:13:49 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:13:49 [INFO] exp_shallowmodel: 
[[ 17   3  31   8]
 [  2   3   5   2]
 [ 24   2 356  14]
 [  6   2  31  16]]
01/22/2018 23:13:50 [INFO] exp_shallowmodel: ******************** ghome - Round 37 
01/22/2018 23:13:50 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:13:50 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:13:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:13:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:13:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:13:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:13:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:14:13 [INFO] exp_shallowmodel: train time: 23.010s
01/22/2018 23:14:13 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:14:13 [INFO] exp_shallowmodel: accuracy:   0.720
01/22/2018 23:14:13 [INFO] exp_shallowmodel: f1_score:   0.340
01/22/2018 23:14:13 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:14:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.31      0.24      0.27        59
          C       0.00      0.00      0.00        12
          F       0.82      0.88      0.85       396
          R       0.27      0.22      0.24        55

avg / total       0.68      0.72      0.70       522

01/22/2018 23:14:13 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:14:13 [INFO] exp_shallowmodel: 
[[ 14   1  34  10]
 [  1   0  10   1]
 [ 23   2 350  21]
 [  7   1  35  12]]
01/22/2018 23:14:13 [INFO] exp_shallowmodel: ******************** ghome - Round 38 
01/22/2018 23:14:13 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:14:13 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:14:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:14:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:14:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:14:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:14:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:14:36 [INFO] exp_shallowmodel: train time: 22.943s
01/22/2018 23:14:36 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:14:36 [INFO] exp_shallowmodel: accuracy:   0.736
01/22/2018 23:14:36 [INFO] exp_shallowmodel: f1_score:   0.351
01/22/2018 23:14:36 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:14:36 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.29      0.31        59
          C       0.00      0.00      0.00        12
          F       0.83      0.90      0.86       396
          R       0.31      0.18      0.23        55

avg / total       0.70      0.74      0.71       522

01/22/2018 23:14:36 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:14:36 [INFO] exp_shallowmodel: 
[[ 17   2  32   8]
 [  4   0   7   1]
 [ 21   5 357  13]
 [  9   2  34  10]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:14:36 [INFO] exp_shallowmodel: ******************** ghome - Round 39 
01/22/2018 23:14:36 [INFO] exp_shallowmodel: #(data) = 4176
01/22/2018 23:14:36 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:14:36 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:14:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:14:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:14:36 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:14:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:15:01 [INFO] exp_shallowmodel: train time: 24.760s
01/22/2018 23:15:01 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:15:01 [INFO] exp_shallowmodel: accuracy:   0.687
01/22/2018 23:15:01 [INFO] exp_shallowmodel: f1_score:   0.355
01/22/2018 23:15:01 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:15:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.23      0.22      0.23        64
          C       0.14      0.07      0.10        14
          F       0.78      0.86      0.82       402
          R       0.39      0.22      0.28        63

avg / total       0.66      0.69      0.67       543

01/22/2018 23:15:01 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:15:01 [INFO] exp_shallowmodel: 
[[ 14   0  45   5]
 [  4   1   8   1]
 [ 36   6 344  16]
 [  6   0  43  14]]
01/22/2018 23:15:01 [INFO] exp_shallowmodel: ******************** ghome - Round 40 
01/22/2018 23:15:01 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:15:01 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:15:01 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:15:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:15:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:15:01 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:15:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:15:23 [INFO] exp_shallowmodel: train time: 21.878s
01/22/2018 23:15:23 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:15:23 [INFO] exp_shallowmodel: accuracy:   0.705
01/22/2018 23:15:23 [INFO] exp_shallowmodel: f1_score:   0.327
01/22/2018 23:15:23 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:15:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.24      0.26        59
          C       0.00      0.00      0.00        12
          F       0.80      0.87      0.83       396
          R       0.25      0.18      0.21        55

avg / total       0.67      0.70      0.68       522

01/22/2018 23:15:23 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:15:23 [INFO] exp_shallowmodel: 
[[ 14   1  36   8]
 [  1   0   9   2]
 [ 28   4 344  20]
 [  5   0  40  10]]
01/22/2018 23:15:23 [INFO] exp_shallowmodel: ******************** ghome - Round 41 
01/22/2018 23:15:23 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:15:23 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:15:23 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:15:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:15:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:15:23 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:15:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:15:46 [INFO] exp_shallowmodel: train time: 23.454s
01/22/2018 23:15:46 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:15:46 [INFO] exp_shallowmodel: accuracy:   0.724
01/22/2018 23:15:46 [INFO] exp_shallowmodel: f1_score:   0.349
01/22/2018 23:15:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:15:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.19      0.21        59
          C       0.14      0.08      0.11        12
          F       0.82      0.90      0.86       396
          R       0.29      0.18      0.22        55

avg / total       0.68      0.72      0.70       522

01/22/2018 23:15:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:15:46 [INFO] exp_shallowmodel: 
[[ 11   1  37  10]
 [  0   1   9   2]
 [ 24   3 356  13]
 [  9   2  34  10]]
01/22/2018 23:15:46 [INFO] exp_shallowmodel: ******************** ghome - Round 42 
01/22/2018 23:15:46 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:15:46 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:15:46 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:15:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:15:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:15:46 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:15:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:16:10 [INFO] exp_shallowmodel: train time: 23.793s
01/22/2018 23:16:10 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:16:10 [INFO] exp_shallowmodel: accuracy:   0.738
01/22/2018 23:16:10 [INFO] exp_shallowmodel: f1_score:   0.352
01/22/2018 23:16:10 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:16:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.24      0.28        59
          C       0.00      0.00      0.00        12
          F       0.81      0.91      0.86       396
          R       0.38      0.22      0.28        55

avg / total       0.69      0.74      0.71       522

01/22/2018 23:16:10 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:16:10 [INFO] exp_shallowmodel: 
[[ 14   0  39   6]
 [  0   0  10   2]
 [ 22   3 359  12]
 [  6   2  35  12]]
01/22/2018 23:16:10 [INFO] exp_shallowmodel: ******************** ghome - Round 43 
01/22/2018 23:16:10 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:16:10 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:16:10 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:16:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:16:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:16:10 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:16:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:16:34 [INFO] exp_shallowmodel: train time: 23.405s
01/22/2018 23:16:34 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:16:34 [INFO] exp_shallowmodel: accuracy:   0.722
01/22/2018 23:16:34 [INFO] exp_shallowmodel: f1_score:   0.353
01/22/2018 23:16:34 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:16:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.27      0.28        59
          C       0.17      0.08      0.11        12
          F       0.82      0.89      0.85       396
          R       0.25      0.13      0.17        55

avg / total       0.68      0.72      0.70       522

01/22/2018 23:16:34 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:16:34 [INFO] exp_shallowmodel: 
[[ 16   1  38   4]
 [  4   1   7   0]
 [ 23   3 353  17]
 [ 12   1  35   7]]
01/22/2018 23:16:34 [INFO] exp_shallowmodel: ******************** ghome - Round 44 
01/22/2018 23:16:34 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:16:34 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:16:34 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:16:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:16:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:16:34 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:16:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:16:55 [INFO] exp_shallowmodel: train time: 21.534s
01/22/2018 23:16:55 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:16:55 [INFO] exp_shallowmodel: accuracy:   0.715
01/22/2018 23:16:55 [INFO] exp_shallowmodel: f1_score:   0.376
01/22/2018 23:16:55 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:16:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.34      0.34      0.34        59
          C       0.14      0.08      0.11        12
          F       0.81      0.86      0.84       396
          R       0.27      0.18      0.22        55

avg / total       0.69      0.71      0.70       522

01/22/2018 23:16:55 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:16:55 [INFO] exp_shallowmodel: 
[[ 20   3  34   2]
 [  2   1   6   3]
 [ 30   2 342  22]
 [  6   1  38  10]]
01/22/2018 23:16:55 [INFO] exp_shallowmodel: ******************** ghome - Round 45 
01/22/2018 23:16:55 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:16:55 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:16:55 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:16:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:16:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:16:55 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:16:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:17:19 [INFO] exp_shallowmodel: train time: 23.777s
01/22/2018 23:17:19 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:17:19 [INFO] exp_shallowmodel: accuracy:   0.728
01/22/2018 23:17:19 [INFO] exp_shallowmodel: f1_score:   0.344
01/22/2018 23:17:19 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:17:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.26      0.20      0.23        59
          C       0.00      0.00      0.00        12
          F       0.83      0.89      0.86       396
          R       0.33      0.25      0.29        55

avg / total       0.69      0.73      0.71       522

01/22/2018 23:17:19 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:17:19 [INFO] exp_shallowmodel: 
[[ 12   1  35  11]
 [  3   0   3   6]
 [ 24   6 354  12]
 [  7   0  34  14]]
01/22/2018 23:17:19 [INFO] exp_shallowmodel: ******************** ghome - Round 46 
01/22/2018 23:17:19 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:17:19 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:17:19 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:17:19 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:17:19 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:17:19 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:17:19 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:17:45 [INFO] exp_shallowmodel: train time: 25.847s
01/22/2018 23:17:45 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:17:45 [INFO] exp_shallowmodel: accuracy:   0.715
01/22/2018 23:17:45 [INFO] exp_shallowmodel: f1_score:   0.321
01/22/2018 23:17:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:17:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.24      0.25        59
          C       0.00      0.00      0.00        12
          F       0.80      0.89      0.84       396
          R       0.28      0.15      0.19        55

avg / total       0.67      0.71      0.69       522

01/22/2018 23:17:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:17:45 [INFO] exp_shallowmodel: 
[[ 14   0  39   6]
 [  2   0  10   0]
 [ 28   2 351  15]
 [  8   2  37   8]]
01/22/2018 23:17:45 [INFO] exp_shallowmodel: ******************** ghome - Round 47 
01/22/2018 23:17:45 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:17:45 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:17:45 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:17:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:17:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:17:45 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:17:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:18:11 [INFO] exp_shallowmodel: train time: 26.161s
01/22/2018 23:18:11 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:18:11 [INFO] exp_shallowmodel: accuracy:   0.724
01/22/2018 23:18:11 [INFO] exp_shallowmodel: f1_score:   0.326
01/22/2018 23:18:11 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:18:11 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.24      0.17      0.20        59
          C       0.00      0.00      0.00        12
          F       0.82      0.90      0.86       396
          R       0.29      0.22      0.25        55

avg / total       0.68      0.72      0.70       522

01/22/2018 23:18:11 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:18:11 [INFO] exp_shallowmodel: 
[[ 10   2  34  13]
 [  1   0  11   0]
 [ 20   3 356  17]
 [ 11   0  32  12]]
01/22/2018 23:18:12 [INFO] exp_shallowmodel: ******************** ghome - Round 48 
01/22/2018 23:18:12 [INFO] exp_shallowmodel: #(data) = 4197
01/22/2018 23:18:12 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:18:12 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:18:12 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:18:12 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:18:12 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:18:12 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:18:38 [INFO] exp_shallowmodel: train time: 26.265s
01/22/2018 23:18:38 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:18:38 [INFO] exp_shallowmodel: accuracy:   0.695
01/22/2018 23:18:38 [INFO] exp_shallowmodel: f1_score:   0.330
01/22/2018 23:18:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:18:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.21      0.15      0.18        59
          C       0.09      0.08      0.09        12
          F       0.80      0.86      0.83       396
          R       0.25      0.20      0.22        55

avg / total       0.66      0.70      0.68       522

01/22/2018 23:18:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:18:38 [INFO] exp_shallowmodel: 
[[  9   2  38  10]
 [  3   1   8   0]
 [ 24   7 342  23]
 [  6   1  37  11]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:18:38 [INFO] exp_shallowmodel: ******************** ghome - Round 49 
01/22/2018 23:18:38 [INFO] exp_shallowmodel: #(data) = 4176
01/22/2018 23:18:38 [INFO] exp_shallowmodel: #(feature) = 902
01/22/2018 23:18:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:18:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:18:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:18:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:18:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:19:02 [INFO] exp_shallowmodel: train time: 23.904s
01/22/2018 23:19:02 [INFO] exp_shallowmodel: test time:  0.001s
01/22/2018 23:19:02 [INFO] exp_shallowmodel: accuracy:   0.707
01/22/2018 23:19:02 [INFO] exp_shallowmodel: f1_score:   0.353
01/22/2018 23:19:02 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:19:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.26      0.19      0.22        64
          C       0.09      0.07      0.08        14
          F       0.80      0.89      0.84       402
          R       0.35      0.22      0.27        63

avg / total       0.67      0.71      0.68       543

01/22/2018 23:19:02 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:19:02 [INFO] exp_shallowmodel: 
[[ 12   2  43   7]
 [  2   1   7   4]
 [ 23   7 357  15]
 [  9   1  39  14]]
Done: 20180122-231903
