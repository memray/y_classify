/ihome/pbrusilosky/rum20/packages/anaconda3/bin/python -m dialogue.classify.task_runner -experiment_mode cross_validation -selected_feature_set_id 0 -add_similarity_feature -selected_context_id 2
No. of param settings = 1
[('experiment_mode', 'cross_validation'), ('deep_model', False), ('selected_context_id', 2), ('selected_feature_set_id', 0), ('similarity_feature', True), ('k_feature_to_keep', -1), ('k_component_for_pca', -1)]
01/22/2018 21:32:36 [INFO] configuration: experiment_mode  :   cross_validation
01/22/2018 21:32:36 [INFO] configuration: deep_model  :   False
01/22/2018 21:32:36 [INFO] configuration: selected_context_id  :   2
01/22/2018 21:32:36 [INFO] configuration: selected_feature_set_id  :   0
01/22/2018 21:32:36 [INFO] configuration: similarity_feature  :   True
01/22/2018 21:32:36 [INFO] configuration: k_feature_to_keep  :   -1
01/22/2018 21:32:36 [INFO] configuration: k_component_for_pca  :   -1
01/22/2018 21:32:36 [INFO] configuration: seed  :   154316847
01/22/2018 21:32:36 [INFO] configuration: root_path  :   /ihome/pbrusilosky/rum20/y_classify
01/22/2018 21:32:36 [INFO] configuration: task_name  :   utterance_type
01/22/2018 21:32:36 [INFO] configuration: timemark  :   20180122-213236
01/22/2018 21:32:36 [INFO] configuration: context_set  :   last
01/22/2018 21:32:36 [INFO] configuration: utterance_names  :   ['last_user_utterance', 'last_system_utterance', 'current_user_utterance', 'next_system_utterance', 'next_user_utterance']
01/22/2018 21:32:36 [INFO] configuration: utterance_range  :   ['current_user_utterance', 'last_system_utterance', 'current_user_utterance']
01/22/2018 21:32:36 [INFO] configuration: feature_set  :   0-all
01/22/2018 21:32:36 [INFO] configuration: feature_set_number  :   ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
01/22/2018 21:32:36 [INFO] configuration: experiment_name  :   20180122-213236.cross_validation.context=last.feature=0-all.similarity=true
01/22/2018 21:32:36 [INFO] configuration: experiment_path  :   /ihome/pbrusilosky/rum20/y_classify/output/20180122-213236.cross_validation.context=last.feature=0-all.similarity=true
01/22/2018 21:32:36 [INFO] configuration: log_path  :   /ihome/pbrusilosky/rum20/y_classify/output/20180122-213236.cross_validation.context=last.feature=0-all.similarity=true/output.log
01/22/2018 21:32:36 [INFO] configuration: valid_type  :   {'R', 'F', 'C', 'A'}
01/22/2018 21:32:36 [INFO] configuration: data_name  :   
01/22/2018 21:32:36 [INFO] configuration: data_names  :   ['dstc2', 'dstc3', 'family', 'ghome']
01/22/2018 21:32:36 [INFO] configuration: raw_feature_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.raw_feature.pkl
01/22/2018 21:32:36 [INFO] configuration: extracted_feature_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.extracted_feature.pkl
01/22/2018 21:32:36 [INFO] configuration: pipeline_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/%s.pipeline.pkl
01/22/2018 21:32:36 [INFO] configuration: metrics  :   ['accuracy', 'precision', 'recall', 'f1_score', 'training_time', 'test_time']
01/22/2018 21:32:36 [INFO] configuration: do_cross_validation  :   True
01/22/2018 21:32:36 [INFO] configuration: #division  :   5
01/22/2018 21:32:36 [INFO] configuration: #cross_validation  :   10
01/22/2018 21:32:36 [INFO] configuration: cv_index_cache_path  :   
01/22/2018 21:32:36 [INFO] configuration: action_words  :   {'expensive', 'cheap', 'weather', 'food', 'remov', 'snooze', 'findcare', 'room', 'else', 'tell', 'turn', 'light', 'add', 'time', 'any', 'post', 'timer', 'area', 'expens', 'share', 'shuffl', 'start', 'telephon', 'show', 'els', 'temperatur', 'volum', 'south', 'skip', 'watch', 'snooz', 'delete', 'member', 'items', 'list', 'volume', 'temperature', 'price', 'centre', 'number', 'remove', 'discard', 'reminds', 'part', 'centr', 'shuffle', 'moder', 'alarm', 'item', 'play', 'remind', 'song', 'stop', 'reminder', 'help', 'clear', 'reminders', 'music', 'matter', 'phone', 'cast', 'next', 'video', 'moderate', 'telephone', 'findcar', 'ani', 'north', 'delet', 'address'}
01/22/2018 21:32:36 [INFO] configuration: corenlp_jars  :   ('/Users/memray/Project/stanford/stanford-corenlp-full-3.8.0/*', '/Users/memray/Project/stanford/stanford-corenlp-full-3.8.0/stanford-english-kbp-corenlp-2017-06-09-models.jar')
01/22/2018 21:32:36 [INFO] configuration: lda_topic_number  :   50
01/22/2018 21:32:36 [INFO] configuration: lda_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.topic=50.lda.pkl
01/22/2018 21:32:36 [INFO] configuration: gensim_corpus_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.corpus.pkl
01/22/2018 21:32:36 [INFO] configuration: gensim_dict_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.dict
01/22/2018 21:32:36 [INFO] configuration: w2v_path  :   /home/memray/Data/glove/GoogleNews-vectors-negative300.bin
01/22/2018 21:32:36 [INFO] configuration: w2v_vector_length  :   300
01/22/2018 21:32:36 [INFO] configuration: d2v_vector_length  :   300
01/22/2018 21:32:36 [INFO] configuration: d2v_window_size  :   5
01/22/2018 21:32:36 [INFO] configuration: d2v_min_count  :   2
01/22/2018 21:32:36 [INFO] configuration: d2v_model_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.doc2vec.dim=300.window=5.min_count=2.model
01/22/2018 21:32:36 [INFO] configuration: d2v_vector_path  :   /ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.doc2vec.dim=300.window=5.min_count=2.vector
01/22/2018 21:32:36 [INFO] configuration: num_word_keep  :   {'dstc2': 300, 'dstc3': 300, 'family': 1000, 'ghome': 1000}
01/22/2018 21:32:36 [INFO] configuration: batch_size  :   128
01/22/2018 21:32:36 [INFO] configuration: max_epoch  :   50
01/22/2018 21:32:36 [INFO] configuration: early_stop_tolerance  :   2
01/22/2018 21:32:36 [INFO] configuration: concat_sents  :   False
01/22/2018 21:32:36 [INFO] configuration: cnn_setting  :   {'model': 'multichannel', 'early_stopping': True, 'word_dim': 300, 'filters': [3, 4, 5], 'filter_num': [100, 100, 100], 'class_size': 4, 'batch_size': 128, 'learning_rate': 0.001, 'norm_limit': 10, 'dropout_prob': 0.0, 'sentence_num': 3}
01/22/2018 21:32:36 [INFO] configuration: skipthought_setting  :   {'skipthought_model_path': '/Users/memray/Data/skip-thought', 'skipthought_data_path': '/ihome/pbrusilosky/rum20/y_classify/dataset/feature/gensim/%s.skip-thought.biskip.vector', 'fixed_emb': True, 'sentence_num': 3, 'hidden_size': 2400, 'class_size': 4, 'learning_rate': 0.0001, 'norm_limit': 3, 'dropout_prob': 0.5}
01/22/2018 21:32:36 [INFO] configuration: lstm_setting  :   {'model': 'non-static', 'hidden_size': 32, 'embedding_size': 300, 'num_layers': 1, 'bidirectional': False, 'learning_rate': 0.001, 'class_size': 4, 'norm_limit': 2, 'clip_grad_norm': 2, 'dropout_prob': 0.1}
01/22/2018 21:32:42 [INFO] exp_shallowmodel: ******************** dstc2 - Round 0 
01/22/2018 21:32:42 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:32:42 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:32:42 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:32:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:32:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:32:42 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:32:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:34:10 [INFO] exp_shallowmodel: train time: 87.484s
01/22/2018 21:34:10 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:34:10 [INFO] exp_shallowmodel: accuracy:   0.658
01/22/2018 21:34:10 [INFO] exp_shallowmodel: f1_score:   0.477
01/22/2018 21:34:10 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:34:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.62      0.66      0.64       164
          F       0.74      0.78      0.76       268
          R       0.55      0.48      0.51       125

avg / total       0.65      0.66      0.65       571

01/22/2018 21:34:10 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:34:10 [INFO] exp_shallowmodel: 
[[  0   2   8   4]
 [  2 108  28  26]
 [  4  36 208  20]
 [  0  28  37  60]]
01/22/2018 21:34:13 [INFO] exp_shallowmodel: ******************** dstc2 - Round 1 
01/22/2018 21:34:13 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:34:13 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:34:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:34:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:34:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:34:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:34:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:35:30 [INFO] exp_shallowmodel: train time: 77.083s
01/22/2018 21:35:30 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:35:30 [INFO] exp_shallowmodel: accuracy:   0.608
01/22/2018 21:35:30 [INFO] exp_shallowmodel: f1_score:   0.476
01/22/2018 21:35:30 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:35:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.14      0.18        14
          C       0.60      0.61      0.60       164
          F       0.70      0.74      0.72       268
          R       0.42      0.38      0.40       125

avg / total       0.60      0.61      0.60       571

01/22/2018 21:35:30 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:35:30 [INFO] exp_shallowmodel: 
[[  2   1   9   2]
 [  1 100  34  29]
 [  2  35 197  34]
 [  3  31  43  48]]
01/22/2018 21:35:34 [INFO] exp_shallowmodel: ******************** dstc2 - Round 2 
01/22/2018 21:35:34 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:35:34 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:35:34 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:35:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:35:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:35:34 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:35:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:36:57 [INFO] exp_shallowmodel: train time: 82.710s
01/22/2018 21:36:57 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:36:57 [INFO] exp_shallowmodel: accuracy:   0.644
01/22/2018 21:36:57 [INFO] exp_shallowmodel: f1_score:   0.484
01/22/2018 21:36:57 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:36:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.57      0.66      0.61       164
          F       0.77      0.77      0.77       268
          R       0.50      0.42      0.46       125

avg / total       0.64      0.64      0.64       571

01/22/2018 21:36:57 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:36:57 [INFO] exp_shallowmodel: 
[[  1   2   4   7]
 [  1 108  32  23]
 [  2  36 206  24]
 [  2  43  27  53]]
01/22/2018 21:37:00 [INFO] exp_shallowmodel: ******************** dstc2 - Round 3 
01/22/2018 21:37:00 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:37:00 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:37:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:37:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:37:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:37:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:37:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:38:22 [INFO] exp_shallowmodel: train time: 82.208s
01/22/2018 21:38:22 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:38:22 [INFO] exp_shallowmodel: accuracy:   0.634
01/22/2018 21:38:22 [INFO] exp_shallowmodel: f1_score:   0.476
01/22/2018 21:38:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:38:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.07      0.11        14
          C       0.59      0.64      0.62       164
          F       0.73      0.76      0.75       268
          R       0.47      0.41      0.44       125

avg / total       0.62      0.63      0.63       571

01/22/2018 21:38:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:38:22 [INFO] exp_shallowmodel: 
[[  1   2   8   3]
 [  0 105  31  28]
 [  2  34 205  27]
 [  2  36  36  51]]
01/22/2018 21:38:26 [INFO] exp_shallowmodel: ******************** dstc2 - Round 4 
01/22/2018 21:38:26 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:38:26 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:38:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:38:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:38:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:38:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:38:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:39:47 [INFO] exp_shallowmodel: train time: 81.126s
01/22/2018 21:39:47 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:39:47 [INFO] exp_shallowmodel: accuracy:   0.637
01/22/2018 21:39:47 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 21:39:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:39:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.63      0.61       164
          F       0.72      0.76      0.74       268
          R       0.52      0.46      0.49       125

avg / total       0.62      0.64      0.63       571

01/22/2018 21:39:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:39:47 [INFO] exp_shallowmodel: 
[[  0   1   9   4]
 [  1 103  39  21]
 [  0  36 203  29]
 [  1  34  32  58]]
01/22/2018 21:39:51 [INFO] exp_shallowmodel: ******************** dstc2 - Round 5 
01/22/2018 21:39:51 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:39:51 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:39:51 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:39:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:39:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:39:51 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:39:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:41:15 [INFO] exp_shallowmodel: train time: 83.842s
01/22/2018 21:41:15 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:41:15 [INFO] exp_shallowmodel: accuracy:   0.594
01/22/2018 21:41:15 [INFO] exp_shallowmodel: f1_score:   0.417
01/22/2018 21:41:15 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:41:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.55      0.59      0.57       164
          F       0.71      0.74      0.72       268
          R       0.41      0.34      0.37       125

avg / total       0.58      0.59      0.59       571

01/22/2018 21:41:15 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:41:15 [INFO] exp_shallowmodel: 
[[  0   2  10   2]
 [  1  97  33  33]
 [  2  39 199  28]
 [  5  37  40  43]]
01/22/2018 21:41:18 [INFO] exp_shallowmodel: ******************** dstc2 - Round 6 
01/22/2018 21:41:18 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:41:18 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:41:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:41:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:41:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:41:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:41:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:42:39 [INFO] exp_shallowmodel: train time: 81.258s
01/22/2018 21:42:39 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:42:39 [INFO] exp_shallowmodel: accuracy:   0.641
01/22/2018 21:42:39 [INFO] exp_shallowmodel: f1_score:   0.458
01/22/2018 21:42:39 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:42:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.66      0.64       164
          F       0.74      0.76      0.75       268
          R       0.45      0.43      0.44       125

avg / total       0.63      0.64      0.63       571

01/22/2018 21:42:39 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:42:39 [INFO] exp_shallowmodel: 
[[  0   2   8   4]
 [  0 108  25  31]
 [  2  32 204  30]
 [  0  34  37  54]]
01/22/2018 21:42:43 [INFO] exp_shallowmodel: ******************** dstc2 - Round 7 
01/22/2018 21:42:43 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:42:43 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:42:43 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:42:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:42:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:42:43 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:42:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:44:07 [INFO] exp_shallowmodel: train time: 84.022s
01/22/2018 21:44:07 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:44:07 [INFO] exp_shallowmodel: accuracy:   0.609
01/22/2018 21:44:07 [INFO] exp_shallowmodel: f1_score:   0.453
01/22/2018 21:44:07 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:44:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.07      0.08        14
          C       0.59      0.60      0.60       164
          F       0.73      0.75      0.74       268
          R       0.40      0.38      0.39       125

avg / total       0.60      0.61      0.61       571

01/22/2018 21:44:07 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:44:07 [INFO] exp_shallowmodel: 
[[  1   2   7   4]
 [  0  99  31  34]
 [  5  30 200  33]
 [  4  37  36  48]]
01/22/2018 21:44:11 [INFO] exp_shallowmodel: ******************** dstc2 - Round 8 
01/22/2018 21:44:11 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:44:11 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:44:11 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:44:11 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:44:11 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:44:11 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:44:11 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:45:37 [INFO] exp_shallowmodel: train time: 86.118s
01/22/2018 21:45:37 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:45:37 [INFO] exp_shallowmodel: accuracy:   0.646
01/22/2018 21:45:37 [INFO] exp_shallowmodel: f1_score:   0.509
01/22/2018 21:45:37 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:45:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.21        14
          C       0.64      0.65      0.65       164
          F       0.73      0.78      0.76       268
          R       0.44      0.40      0.42       125

avg / total       0.64      0.65      0.64       571

01/22/2018 21:45:37 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:45:37 [INFO] exp_shallowmodel: 
[[  2   1   7   4]
 [  0 107  24  33]
 [  2  29 210  27]
 [  1  29  45  50]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 21:45:40 [INFO] exp_shallowmodel: ******************** dstc2 - Round 9 
01/22/2018 21:45:40 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 21:45:40 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:45:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:45:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:45:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:45:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:45:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:47:06 [INFO] exp_shallowmodel: train time: 85.829s
01/22/2018 21:47:06 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:47:06 [INFO] exp_shallowmodel: accuracy:   0.616
01/22/2018 21:47:06 [INFO] exp_shallowmodel: f1_score:   0.439
01/22/2018 21:47:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:47:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.57      0.61      0.59       169
          F       0.71      0.76      0.74       271
          R       0.47      0.40      0.43       130

avg / total       0.60      0.62      0.61       586

01/22/2018 21:47:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:47:06 [INFO] exp_shallowmodel: 
[[  0   1   7   8]
 [  1 103  40  25]
 [  2  37 206  26]
 [  1  41  36  52]]
01/22/2018 21:47:10 [INFO] exp_shallowmodel: ******************** dstc2 - Round 10 
01/22/2018 21:47:10 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:47:10 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:47:10 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:47:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:47:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:47:10 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:47:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:48:36 [INFO] exp_shallowmodel: train time: 86.740s
01/22/2018 21:48:36 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:48:36 [INFO] exp_shallowmodel: accuracy:   0.625
01/22/2018 21:48:36 [INFO] exp_shallowmodel: f1_score:   0.445
01/22/2018 21:48:36 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:48:36 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.62      0.60       164
          F       0.72      0.76      0.74       268
          R       0.47      0.41      0.44       125

avg / total       0.61      0.63      0.62       571

01/22/2018 21:48:36 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:48:36 [INFO] exp_shallowmodel: 
[[  0   2   8   4]
 [  1 102  32  29]
 [  4  36 204  24]
 [  0  36  38  51]]
01/22/2018 21:48:40 [INFO] exp_shallowmodel: ******************** dstc2 - Round 11 
01/22/2018 21:48:40 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:48:40 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:48:40 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:48:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:48:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:48:40 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:48:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:50:04 [INFO] exp_shallowmodel: train time: 83.710s
01/22/2018 21:50:04 [INFO] exp_shallowmodel: test time:  0.014s
01/22/2018 21:50:04 [INFO] exp_shallowmodel: accuracy:   0.608
01/22/2018 21:50:04 [INFO] exp_shallowmodel: f1_score:   0.427
01/22/2018 21:50:04 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:50:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.57      0.55      0.56       164
          F       0.71      0.78      0.74       268
          R       0.42      0.38      0.40       125

avg / total       0.59      0.61      0.60       571

01/22/2018 21:50:04 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:50:04 [INFO] exp_shallowmodel: 
[[  0   0  12   2]
 [  0  91  35  38]
 [  3  31 208  26]
 [  4  37  36  48]]
01/22/2018 21:50:07 [INFO] exp_shallowmodel: ******************** dstc2 - Round 12 
01/22/2018 21:50:07 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:50:07 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:50:07 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:50:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:50:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:50:07 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:50:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:51:31 [INFO] exp_shallowmodel: train time: 84.070s
01/22/2018 21:51:31 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:51:31 [INFO] exp_shallowmodel: accuracy:   0.571
01/22/2018 21:51:31 [INFO] exp_shallowmodel: f1_score:   0.423
01/22/2018 21:51:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:51:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.53      0.53      0.53       164
          F       0.68      0.74      0.71       268
          R       0.35      0.33      0.34       125

avg / total       0.56      0.57      0.56       571

01/22/2018 21:51:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:51:31 [INFO] exp_shallowmodel: 
[[  1   2   6   5]
 [  2  87  42  33]
 [  0  34 197  37]
 [  1  40  43  41]]
01/22/2018 21:51:35 [INFO] exp_shallowmodel: ******************** dstc2 - Round 13 
01/22/2018 21:51:35 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:51:35 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:51:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:51:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:51:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:51:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:51:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:53:00 [INFO] exp_shallowmodel: train time: 84.627s
01/22/2018 21:53:00 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:53:00 [INFO] exp_shallowmodel: accuracy:   0.643
01/22/2018 21:53:00 [INFO] exp_shallowmodel: f1_score:   0.462
01/22/2018 21:53:00 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:53:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.65      0.62       164
          F       0.74      0.76      0.75       268
          R       0.50      0.46      0.48       125

avg / total       0.63      0.64      0.64       571

01/22/2018 21:53:00 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:53:00 [INFO] exp_shallowmodel: 
[[  0   1   6   7]
 [  1 106  34  23]
 [  2  34 204  28]
 [  2  35  31  57]]
01/22/2018 21:53:03 [INFO] exp_shallowmodel: ******************** dstc2 - Round 14 
01/22/2018 21:53:03 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:53:03 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:53:03 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:53:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:53:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:53:03 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:53:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:54:30 [INFO] exp_shallowmodel: train time: 86.508s
01/22/2018 21:54:30 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:54:30 [INFO] exp_shallowmodel: accuracy:   0.634
01/22/2018 21:54:30 [INFO] exp_shallowmodel: f1_score:   0.503
01/22/2018 21:54:30 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:54:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.21        14
          C       0.59      0.62      0.60       164
          F       0.72      0.76      0.74       268
          R       0.49      0.43      0.46       125

avg / total       0.62      0.63      0.63       571

01/22/2018 21:54:30 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:54:30 [INFO] exp_shallowmodel: 
[[  2   3   7   2]
 [  0 101  36  27]
 [  2  33 205  28]
 [  1  34  36  54]]
01/22/2018 21:54:33 [INFO] exp_shallowmodel: ******************** dstc2 - Round 15 
01/22/2018 21:54:33 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:54:33 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:54:33 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:54:33 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:54:33 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:54:33 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:54:33 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:55:57 [INFO] exp_shallowmodel: train time: 83.556s
01/22/2018 21:55:57 [INFO] exp_shallowmodel: test time:  0.014s
01/22/2018 21:55:57 [INFO] exp_shallowmodel: accuracy:   0.632
01/22/2018 21:55:57 [INFO] exp_shallowmodel: f1_score:   0.450
01/22/2018 21:55:57 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:55:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.65      0.61       164
          F       0.73      0.75      0.74       268
          R       0.48      0.42      0.44       125

avg / total       0.61      0.63      0.62       571

01/22/2018 21:55:57 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:55:57 [INFO] exp_shallowmodel: 
[[  0   2   6   6]
 [  0 107  32  25]
 [  1  39 202  26]
 [  0  38  35  52]]
01/22/2018 21:56:01 [INFO] exp_shallowmodel: ******************** dstc2 - Round 16 
01/22/2018 21:56:01 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:56:01 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:56:01 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:56:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:56:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:56:01 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:56:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:57:21 [INFO] exp_shallowmodel: train time: 80.077s
01/22/2018 21:57:21 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:57:21 [INFO] exp_shallowmodel: accuracy:   0.655
01/22/2018 21:57:21 [INFO] exp_shallowmodel: f1_score:   0.465
01/22/2018 21:57:21 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:57:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.63      0.74      0.68       164
          F       0.77      0.76      0.77       268
          R       0.45      0.38      0.41       125

avg / total       0.64      0.65      0.64       571

01/22/2018 21:57:21 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:57:21 [INFO] exp_shallowmodel: 
[[  0   1   8   5]
 [  0 121  15  28]
 [  3  34 205  26]
 [  1  37  39  48]]
01/22/2018 21:57:24 [INFO] exp_shallowmodel: ******************** dstc2 - Round 17 
01/22/2018 21:57:24 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:57:24 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:57:24 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:57:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:57:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:57:24 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:57:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 21:58:47 [INFO] exp_shallowmodel: train time: 82.775s
01/22/2018 21:58:47 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 21:58:47 [INFO] exp_shallowmodel: accuracy:   0.629
01/22/2018 21:58:47 [INFO] exp_shallowmodel: f1_score:   0.476
01/22/2018 21:58:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 21:58:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.08      0.07      0.08        14
          C       0.61      0.65      0.63       164
          F       0.71      0.73      0.72       268
          R       0.51      0.45      0.48       125

avg / total       0.62      0.63      0.62       571

01/22/2018 21:58:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 21:58:47 [INFO] exp_shallowmodel: 
[[  1   1   9   3]
 [  0 106  35  23]
 [  7  38 196  27]
 [  4  28  37  56]]
01/22/2018 21:58:51 [INFO] exp_shallowmodel: ******************** dstc2 - Round 18 
01/22/2018 21:58:51 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 21:58:51 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 21:58:51 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 21:58:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 21:58:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 21:58:51 [INFO] exp_shallowmodel: Training: 
01/22/2018 21:58:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:00:14 [INFO] exp_shallowmodel: train time: 83.001s
01/22/2018 22:00:14 [INFO] exp_shallowmodel: test time:  0.014s
01/22/2018 22:00:14 [INFO] exp_shallowmodel: accuracy:   0.637
01/22/2018 22:00:14 [INFO] exp_shallowmodel: f1_score:   0.479
01/22/2018 22:00:14 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:00:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.60      0.62      0.61       164
          F       0.75      0.78      0.76       268
          R       0.45      0.42      0.43       125

avg / total       0.63      0.64      0.63       571

01/22/2018 22:00:14 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:00:14 [INFO] exp_shallowmodel: 
[[  1   1   6   6]
 [  1 101  29  33]
 [  1  31 209  27]
 [  1  36  35  53]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:00:17 [INFO] exp_shallowmodel: ******************** dstc2 - Round 19 
01/22/2018 22:00:17 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 22:00:17 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:00:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:00:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:00:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:00:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:00:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:01:38 [INFO] exp_shallowmodel: train time: 80.664s
01/22/2018 22:01:38 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:01:38 [INFO] exp_shallowmodel: accuracy:   0.602
01/22/2018 22:01:38 [INFO] exp_shallowmodel: f1_score:   0.434
01/22/2018 22:01:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:01:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.62      0.64      0.63       169
          F       0.66      0.72      0.69       271
          R       0.45      0.38      0.41       130

avg / total       0.59      0.60      0.59       586

01/22/2018 22:01:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:01:38 [INFO] exp_shallowmodel: 
[[  0   1   8   7]
 [  0 109  40  20]
 [  4  39 194  34]
 [  3  26  51  50]]
01/22/2018 22:01:41 [INFO] exp_shallowmodel: ******************** dstc2 - Round 20 
01/22/2018 22:01:41 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:01:41 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:01:41 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:01:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:01:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:01:41 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:01:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:03:05 [INFO] exp_shallowmodel: train time: 83.965s
01/22/2018 22:03:05 [INFO] exp_shallowmodel: test time:  0.014s
01/22/2018 22:03:05 [INFO] exp_shallowmodel: accuracy:   0.599
01/22/2018 22:03:05 [INFO] exp_shallowmodel: f1_score:   0.452
01/22/2018 22:03:05 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:03:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.57      0.59      0.58       164
          F       0.70      0.73      0.71       268
          R       0.42      0.39      0.41       125

avg / total       0.59      0.60      0.59       571

01/22/2018 22:03:05 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:03:05 [INFO] exp_shallowmodel: 
[[  1   2   7   4]
 [  2  96  39  27]
 [  0  36 196  36]
 [  1  35  40  49]]
01/22/2018 22:03:09 [INFO] exp_shallowmodel: ******************** dstc2 - Round 21 
01/22/2018 22:03:09 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:03:09 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:03:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:03:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:03:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:03:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:03:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:04:33 [INFO] exp_shallowmodel: train time: 84.302s
01/22/2018 22:04:33 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:04:33 [INFO] exp_shallowmodel: accuracy:   0.632
01/22/2018 22:04:33 [INFO] exp_shallowmodel: f1_score:   0.447
01/22/2018 22:04:33 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:04:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.62      0.60       164
          F       0.73      0.78      0.76       268
          R       0.49      0.39      0.43       125

avg / total       0.61      0.63      0.62       571

01/22/2018 22:04:33 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:04:33 [INFO] exp_shallowmodel: 
[[  0   3  10   1]
 [  1 102  36  25]
 [  3  29 210  26]
 [  2  42  32  49]]
01/22/2018 22:04:37 [INFO] exp_shallowmodel: ******************** dstc2 - Round 22 
01/22/2018 22:04:37 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:04:37 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:04:37 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:04:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:04:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:04:37 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:04:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:06:00 [INFO] exp_shallowmodel: train time: 82.560s
01/22/2018 22:06:00 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:06:00 [INFO] exp_shallowmodel: accuracy:   0.632
01/22/2018 22:06:00 [INFO] exp_shallowmodel: f1_score:   0.474
01/22/2018 22:06:00 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:06:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.11      0.07      0.09        14
          C       0.60      0.66      0.63       164
          F       0.73      0.75      0.74       268
          R       0.49      0.41      0.44       125

avg / total       0.62      0.63      0.63       571

01/22/2018 22:06:00 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:06:00 [INFO] exp_shallowmodel: 
[[  1   1   5   7]
 [  2 109  27  26]
 [  3  44 200  21]
 [  3  28  43  51]]
01/22/2018 22:06:03 [INFO] exp_shallowmodel: ******************** dstc2 - Round 23 
01/22/2018 22:06:03 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:06:03 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:06:03 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:06:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:06:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:06:03 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:06:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:07:24 [INFO] exp_shallowmodel: train time: 81.040s
01/22/2018 22:07:24 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:07:24 [INFO] exp_shallowmodel: accuracy:   0.625
01/22/2018 22:07:24 [INFO] exp_shallowmodel: f1_score:   0.498
01/22/2018 22:07:24 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:07:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       1.00      0.14      0.25        14
          C       0.59      0.63      0.61       164
          F       0.72      0.77      0.74       268
          R       0.42      0.36      0.39       125

avg / total       0.62      0.63      0.62       571

01/22/2018 22:07:24 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:07:24 [INFO] exp_shallowmodel: 
[[  2   0   6   6]
 [  0 103  34  27]
 [  0  33 207  28]
 [  0  38  42  45]]
01/22/2018 22:07:28 [INFO] exp_shallowmodel: ******************** dstc2 - Round 24 
01/22/2018 22:07:28 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:07:28 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:07:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:07:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:07:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:07:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:07:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:08:55 [INFO] exp_shallowmodel: train time: 86.828s
01/22/2018 22:08:55 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:08:55 [INFO] exp_shallowmodel: accuracy:   0.613
01/22/2018 22:08:55 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 22:08:55 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:08:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.56      0.66      0.61       164
          F       0.73      0.72      0.73       268
          R       0.44      0.38      0.41       125

avg / total       0.60      0.61      0.61       571

01/22/2018 22:08:55 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:08:55 [INFO] exp_shallowmodel: 
[[  0   4   4   6]
 [  0 108  30  26]
 [  3  42 194  29]
 [  1  38  38  48]]
01/22/2018 22:08:58 [INFO] exp_shallowmodel: ******************** dstc2 - Round 25 
01/22/2018 22:08:58 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:08:58 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:08:58 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:08:58 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:08:58 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:08:58 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:08:58 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:10:23 [INFO] exp_shallowmodel: train time: 84.439s
01/22/2018 22:10:23 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:10:23 [INFO] exp_shallowmodel: accuracy:   0.648
01/22/2018 22:10:23 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 22:10:23 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:10:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.64      0.66      0.65       164
          F       0.75      0.79      0.77       268
          R       0.44      0.41      0.42       125

avg / total       0.63      0.65      0.64       571

01/22/2018 22:10:23 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:10:23 [INFO] exp_shallowmodel: 
[[  0   5   5   4]
 [  0 108  28  28]
 [  2  23 211  32]
 [  1  34  39  51]]
01/22/2018 22:10:26 [INFO] exp_shallowmodel: ******************** dstc2 - Round 26 
01/22/2018 22:10:26 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:10:26 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:10:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:10:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:10:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:10:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:10:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:11:51 [INFO] exp_shallowmodel: train time: 84.527s
01/22/2018 22:11:51 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:11:51 [INFO] exp_shallowmodel: accuracy:   0.606
01/22/2018 22:11:51 [INFO] exp_shallowmodel: f1_score:   0.479
01/22/2018 22:11:51 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:11:51 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.14      0.22        14
          C       0.54      0.55      0.55       164
          F       0.71      0.76      0.74       268
          R       0.44      0.38      0.41       125

avg / total       0.60      0.61      0.60       571

01/22/2018 22:11:51 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:11:51 [INFO] exp_shallowmodel: 
[[  2   1   8   3]
 [  0  91  37  36]
 [  1  39 205  23]
 [  1  38  38  48]]
01/22/2018 22:11:54 [INFO] exp_shallowmodel: ******************** dstc2 - Round 27 
01/22/2018 22:11:54 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:11:54 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:11:54 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:11:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:11:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:11:54 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:11:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:13:24 [INFO] exp_shallowmodel: train time: 89.552s
01/22/2018 22:13:24 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:13:24 [INFO] exp_shallowmodel: accuracy:   0.641
01/22/2018 22:13:24 [INFO] exp_shallowmodel: f1_score:   0.479
01/22/2018 22:13:24 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:13:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.07      0.08        14
          C       0.61      0.66      0.63       164
          F       0.74      0.76      0.75       268
          R       0.49      0.42      0.45       125

avg / total       0.63      0.64      0.64       571

01/22/2018 22:13:24 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:13:24 [INFO] exp_shallowmodel: 
[[  1   3   8   2]
 [  0 108  29  27]
 [  5  33 205  25]
 [  4  34  35  52]]
01/22/2018 22:13:27 [INFO] exp_shallowmodel: ******************** dstc2 - Round 28 
01/22/2018 22:13:27 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:13:27 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:13:27 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:13:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:13:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:13:27 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:13:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:14:55 [INFO] exp_shallowmodel: train time: 87.634s
01/22/2018 22:14:55 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:14:55 [INFO] exp_shallowmodel: accuracy:   0.602
01/22/2018 22:14:55 [INFO] exp_shallowmodel: f1_score:   0.425
01/22/2018 22:14:55 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:14:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.52      0.56      0.54       164
          F       0.74      0.75      0.75       268
          R       0.43      0.40      0.41       125

avg / total       0.59      0.60      0.60       571

01/22/2018 22:14:55 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:14:55 [INFO] exp_shallowmodel: 
[[  0   2   7   5]
 [  1  92  34  37]
 [  3  39 202  24]
 [  1  43  31  50]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:14:59 [INFO] exp_shallowmodel: ******************** dstc2 - Round 29 
01/22/2018 22:14:59 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 22:14:59 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:14:59 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:14:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:14:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:14:59 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:14:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:16:25 [INFO] exp_shallowmodel: train time: 85.966s
01/22/2018 22:16:25 [INFO] exp_shallowmodel: test time:  0.016s
01/22/2018 22:16:25 [INFO] exp_shallowmodel: accuracy:   0.608
01/22/2018 22:16:25 [INFO] exp_shallowmodel: f1_score:   0.434
01/22/2018 22:16:25 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:16:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.60      0.65      0.63       169
          F       0.68      0.73      0.70       271
          R       0.44      0.38      0.40       130

avg / total       0.59      0.61      0.60       586

01/22/2018 22:16:25 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:16:25 [INFO] exp_shallowmodel: 
[[  0   2  11   3]
 [  0 110  35  24]
 [  3  35 197  36]
 [  1  35  45  49]]
01/22/2018 22:16:28 [INFO] exp_shallowmodel: ******************** dstc2 - Round 30 
01/22/2018 22:16:28 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:16:28 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:16:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:16:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:16:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:16:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:16:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:17:52 [INFO] exp_shallowmodel: train time: 83.876s
01/22/2018 22:17:52 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:17:52 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:17:52 [INFO] exp_shallowmodel: f1_score:   0.460
01/22/2018 22:17:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:17:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.07      0.09        14
          C       0.61      0.70      0.65       164
          F       0.75      0.71      0.73       268
          R       0.37      0.37      0.37       125

avg / total       0.61      0.61      0.61       571

01/22/2018 22:17:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:17:52 [INFO] exp_shallowmodel: 
[[  1   0   6   7]
 [  0 114  17  33]
 [  4  37 190  37]
 [  3  37  39  46]]
01/22/2018 22:17:56 [INFO] exp_shallowmodel: ******************** dstc2 - Round 31 
01/22/2018 22:17:56 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:17:56 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:17:56 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:17:56 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:17:56 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:17:56 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:17:56 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:19:19 [INFO] exp_shallowmodel: train time: 83.380s
01/22/2018 22:19:19 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:19:19 [INFO] exp_shallowmodel: accuracy:   0.637
01/22/2018 22:19:19 [INFO] exp_shallowmodel: f1_score:   0.480
01/22/2018 22:19:19 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:19:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.61      0.66      0.64       164
          F       0.73      0.75      0.74       268
          R       0.47      0.42      0.44       125

avg / total       0.63      0.64      0.63       571

01/22/2018 22:19:19 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:19:19 [INFO] exp_shallowmodel: 
[[  1   4   5   4]
 [  0 109  30  25]
 [  3  33 202  30]
 [  2  32  39  52]]
01/22/2018 22:19:23 [INFO] exp_shallowmodel: ******************** dstc2 - Round 32 
01/22/2018 22:19:23 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:19:23 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:19:23 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:19:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:19:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:19:23 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:19:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:20:47 [INFO] exp_shallowmodel: train time: 84.184s
01/22/2018 22:20:47 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:20:47 [INFO] exp_shallowmodel: accuracy:   0.637
01/22/2018 22:20:47 [INFO] exp_shallowmodel: f1_score:   0.478
01/22/2018 22:20:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:20:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.07      0.11        14
          C       0.61      0.60      0.60       164
          F       0.74      0.79      0.77       268
          R       0.44      0.42      0.43       125

avg / total       0.63      0.64      0.63       571

01/22/2018 22:20:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:20:47 [INFO] exp_shallowmodel: 
[[  1   2   8   3]
 [  0  98  27  39]
 [  2  28 212  26]
 [  1  32  39  53]]
01/22/2018 22:20:50 [INFO] exp_shallowmodel: ******************** dstc2 - Round 33 
01/22/2018 22:20:50 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:20:50 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:20:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:20:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:20:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:20:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:20:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:22:17 [INFO] exp_shallowmodel: train time: 86.381s
01/22/2018 22:22:17 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:22:17 [INFO] exp_shallowmodel: accuracy:   0.625
01/22/2018 22:22:17 [INFO] exp_shallowmodel: f1_score:   0.441
01/22/2018 22:22:17 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:22:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.62      0.60       164
          F       0.72      0.77      0.75       268
          R       0.44      0.39      0.42       125

avg / total       0.60      0.63      0.61       571

01/22/2018 22:22:17 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:22:17 [INFO] exp_shallowmodel: 
[[  0   3   9   2]
 [  0 101  29  34]
 [  0  35 207  26]
 [  2  33  41  49]]
01/22/2018 22:22:20 [INFO] exp_shallowmodel: ******************** dstc2 - Round 34 
01/22/2018 22:22:20 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:22:20 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:22:20 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:22:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:22:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:22:20 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:22:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:23:45 [INFO] exp_shallowmodel: train time: 84.245s
01/22/2018 22:23:45 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:23:45 [INFO] exp_shallowmodel: accuracy:   0.636
01/22/2018 22:23:45 [INFO] exp_shallowmodel: f1_score:   0.499
01/22/2018 22:23:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:23:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.21        14
          C       0.56      0.64      0.59       164
          F       0.75      0.78      0.76       268
          R       0.48      0.38      0.42       125

avg / total       0.63      0.64      0.63       571

01/22/2018 22:23:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:23:45 [INFO] exp_shallowmodel: 
[[  2   2   7   3]
 [  0 105  34  25]
 [  0  35 208  25]
 [  3  47  27  48]]
01/22/2018 22:23:48 [INFO] exp_shallowmodel: ******************** dstc2 - Round 35 
01/22/2018 22:23:48 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:23:48 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:23:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:23:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:23:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:23:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:23:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:25:11 [INFO] exp_shallowmodel: train time: 83.051s
01/22/2018 22:25:11 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:25:11 [INFO] exp_shallowmodel: accuracy:   0.630
01/22/2018 22:25:11 [INFO] exp_shallowmodel: f1_score:   0.445
01/22/2018 22:25:11 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:25:11 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.62      0.68      0.65       164
          F       0.74      0.75      0.75       268
          R       0.40      0.37      0.38       125

avg / total       0.61      0.63      0.62       571

01/22/2018 22:25:11 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:25:11 [INFO] exp_shallowmodel: 
[[  0   1   7   6]
 [  0 112  27  25]
 [  2  27 202  37]
 [  1  42  36  46]]
01/22/2018 22:25:15 [INFO] exp_shallowmodel: ******************** dstc2 - Round 36 
01/22/2018 22:25:15 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:25:15 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:25:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:25:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:25:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:25:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:25:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:26:37 [INFO] exp_shallowmodel: train time: 82.531s
01/22/2018 22:26:37 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:26:38 [INFO] exp_shallowmodel: accuracy:   0.629
01/22/2018 22:26:38 [INFO] exp_shallowmodel: f1_score:   0.451
01/22/2018 22:26:38 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:26:38 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.60      0.71      0.65       164
          F       0.71      0.72      0.72       268
          R       0.48      0.40      0.44       125

avg / total       0.61      0.63      0.62       571

01/22/2018 22:26:38 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:26:38 [INFO] exp_shallowmodel: 
[[  0   1   9   4]
 [  1 116  28  19]
 [  2  42 193  31]
 [  1  33  41  50]]
01/22/2018 22:26:41 [INFO] exp_shallowmodel: ******************** dstc2 - Round 37 
01/22/2018 22:26:41 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:26:41 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:26:41 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:26:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:26:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:26:41 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:26:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:28:01 [INFO] exp_shallowmodel: train time: 80.173s
01/22/2018 22:28:01 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:28:01 [INFO] exp_shallowmodel: accuracy:   0.616
01/22/2018 22:28:01 [INFO] exp_shallowmodel: f1_score:   0.481
01/22/2018 22:28:01 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:28:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.14      0.20        14
          C       0.57      0.57      0.57       164
          F       0.71      0.78      0.74       268
          R       0.46      0.38      0.41       125

avg / total       0.60      0.62      0.61       571

01/22/2018 22:28:01 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:28:01 [INFO] exp_shallowmodel: 
[[  2   1   9   2]
 [  0  94  39  31]
 [  1  35 209  23]
 [  3  36  39  47]]
01/22/2018 22:28:05 [INFO] exp_shallowmodel: ******************** dstc2 - Round 38 
01/22/2018 22:28:05 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:28:05 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:28:05 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:28:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:28:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:28:05 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:28:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:29:23 [INFO] exp_shallowmodel: train time: 77.862s
01/22/2018 22:29:23 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:29:23 [INFO] exp_shallowmodel: accuracy:   0.620
01/22/2018 22:29:23 [INFO] exp_shallowmodel: f1_score:   0.441
01/22/2018 22:29:23 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:29:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.62      0.66      0.64       164
          F       0.72      0.74      0.73       268
          R       0.41      0.38      0.39       125

avg / total       0.61      0.62      0.61       571

01/22/2018 22:29:23 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:29:23 [INFO] exp_shallowmodel: 
[[  0   1   7   6]
 [  0 108  25  31]
 [  5  34 199  30]
 [  2  30  46  47]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:29:26 [INFO] exp_shallowmodel: ******************** dstc2 - Round 39 
01/22/2018 22:29:26 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 22:29:26 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:29:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:29:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:29:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:29:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:29:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:30:47 [INFO] exp_shallowmodel: train time: 80.278s
01/22/2018 22:30:47 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:30:47 [INFO] exp_shallowmodel: accuracy:   0.608
01/22/2018 22:30:47 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 22:30:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:30:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.06      0.10        16
          C       0.56      0.57      0.57       169
          F       0.72      0.74      0.73       271
          R       0.45      0.45      0.45       130

avg / total       0.60      0.61      0.60       586

01/22/2018 22:30:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:30:47 [INFO] exp_shallowmodel: 
[[  1   0  10   5]
 [  1  96  33  39]
 [  2  40 201  28]
 [  1  34  37  58]]
01/22/2018 22:30:50 [INFO] exp_shallowmodel: ******************** dstc2 - Round 40 
01/22/2018 22:30:50 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:30:50 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:30:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:30:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:30:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:30:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:30:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:32:10 [INFO] exp_shallowmodel: train time: 80.224s
01/22/2018 22:32:10 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:32:10 [INFO] exp_shallowmodel: accuracy:   0.646
01/22/2018 22:32:10 [INFO] exp_shallowmodel: f1_score:   0.467
01/22/2018 22:32:10 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:32:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.66      0.62       164
          F       0.75      0.75      0.75       268
          R       0.53      0.48      0.50       125

avg / total       0.63      0.65      0.64       571

01/22/2018 22:32:10 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:32:10 [INFO] exp_shallowmodel: 
[[  0   5   6   3]
 [  1 108  28  27]
 [  0  43 201  24]
 [  2  30  33  60]]
01/22/2018 22:32:14 [INFO] exp_shallowmodel: ******************** dstc2 - Round 41 
01/22/2018 22:32:14 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:32:14 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:32:14 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:32:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:32:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:32:14 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:32:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:33:35 [INFO] exp_shallowmodel: train time: 80.647s
01/22/2018 22:33:35 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:33:35 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 22:33:35 [INFO] exp_shallowmodel: f1_score:   0.444
01/22/2018 22:33:35 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:33:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.58      0.63      0.60       164
          F       0.72      0.74      0.73       268
          R       0.47      0.42      0.44       125

avg / total       0.61      0.62      0.61       571

01/22/2018 22:33:35 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:33:35 [INFO] exp_shallowmodel: 
[[  0   0  10   4]
 [  0 104  33  27]
 [  3  38 199  28]
 [  1  38  34  52]]
01/22/2018 22:33:38 [INFO] exp_shallowmodel: ******************** dstc2 - Round 42 
01/22/2018 22:33:38 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:33:38 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:33:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:33:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:33:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:33:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:33:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:34:58 [INFO] exp_shallowmodel: train time: 79.353s
01/22/2018 22:34:58 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:34:58 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 22:34:58 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 22:34:58 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:34:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.56      0.62      0.59       164
          F       0.74      0.75      0.74       268
          R       0.43      0.40      0.41       125

avg / total       0.60      0.61      0.61       571

01/22/2018 22:34:58 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:34:58 [INFO] exp_shallowmodel: 
[[  0   3   5   6]
 [  0 101  29  34]
 [  2  39 200  27]
 [  1  36  38  50]]
01/22/2018 22:35:01 [INFO] exp_shallowmodel: ******************** dstc2 - Round 43 
01/22/2018 22:35:01 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:35:01 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:35:01 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:35:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:35:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:35:01 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:35:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:36:26 [INFO] exp_shallowmodel: train time: 85.205s
01/22/2018 22:36:26 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:36:26 [INFO] exp_shallowmodel: accuracy:   0.613
01/22/2018 22:36:26 [INFO] exp_shallowmodel: f1_score:   0.439
01/22/2018 22:36:26 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:36:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.59      0.62      0.60       164
          F       0.75      0.73      0.74       268
          R       0.41      0.42      0.42       125

avg / total       0.61      0.61      0.61       571

01/22/2018 22:36:26 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:36:26 [INFO] exp_shallowmodel: 
[[  0   1   8   5]
 [  0 101  30  33]
 [  4  29 196  39]
 [  3  40  29  53]]
01/22/2018 22:36:30 [INFO] exp_shallowmodel: ******************** dstc2 - Round 44 
01/22/2018 22:36:30 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:36:30 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:36:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:36:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:36:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:36:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:36:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:37:56 [INFO] exp_shallowmodel: train time: 85.786s
01/22/2018 22:37:56 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:37:56 [INFO] exp_shallowmodel: accuracy:   0.616
01/22/2018 22:37:56 [INFO] exp_shallowmodel: f1_score:   0.462
01/22/2018 22:37:56 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:37:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.60      0.64      0.62       164
          F       0.71      0.74      0.73       268
          R       0.42      0.38      0.40       125

avg / total       0.61      0.62      0.61       571

01/22/2018 22:37:56 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:37:56 [INFO] exp_shallowmodel: 
[[  1   3   6   4]
 [  0 105  31  28]
 [  4  32 198  34]
 [  1  34  42  48]]
01/22/2018 22:37:59 [INFO] exp_shallowmodel: ******************** dstc2 - Round 45 
01/22/2018 22:37:59 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:37:59 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:37:59 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:37:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:37:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:37:59 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:37:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:39:22 [INFO] exp_shallowmodel: train time: 83.018s
01/22/2018 22:39:22 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:39:22 [INFO] exp_shallowmodel: accuracy:   0.609
01/22/2018 22:39:22 [INFO] exp_shallowmodel: f1_score:   0.453
01/22/2018 22:39:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:39:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.07      0.10        14
          C       0.59      0.60      0.60       164
          F       0.70      0.75      0.72       268
          R       0.43      0.37      0.40       125

avg / total       0.59      0.61      0.60       571

01/22/2018 22:39:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:39:22 [INFO] exp_shallowmodel: 
[[  1   5   5   3]
 [  0  99  35  30]
 [  4  35 202  27]
 [  2  29  48  46]]
01/22/2018 22:39:26 [INFO] exp_shallowmodel: ******************** dstc2 - Round 46 
01/22/2018 22:39:26 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:39:26 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:39:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:39:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:39:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:39:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:39:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:40:49 [INFO] exp_shallowmodel: train time: 83.136s
01/22/2018 22:40:49 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:40:49 [INFO] exp_shallowmodel: accuracy:   0.602
01/22/2018 22:40:49 [INFO] exp_shallowmodel: f1_score:   0.477
01/22/2018 22:40:49 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:40:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.14      0.22        14
          C       0.53      0.59      0.55       164
          F       0.71      0.75      0.73       268
          R       0.44      0.37      0.40       125

avg / total       0.59      0.60      0.59       571

01/22/2018 22:40:49 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:40:49 [INFO] exp_shallowmodel: 
[[  2   1   7   4]
 [  0  96  37  31]
 [  1  44 200  23]
 [  1  41  37  46]]
01/22/2018 22:40:53 [INFO] exp_shallowmodel: ******************** dstc2 - Round 47 
01/22/2018 22:40:53 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:40:53 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:40:53 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:40:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:40:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:40:53 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:40:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:42:18 [INFO] exp_shallowmodel: train time: 85.582s
01/22/2018 22:42:18 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:42:18 [INFO] exp_shallowmodel: accuracy:   0.648
01/22/2018 22:42:18 [INFO] exp_shallowmodel: f1_score:   0.463
01/22/2018 22:42:18 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:42:18 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        14
          C       0.61      0.65      0.63       164
          F       0.74      0.77      0.76       268
          R       0.48      0.45      0.46       125

avg / total       0.63      0.65      0.64       571

01/22/2018 22:42:18 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:42:18 [INFO] exp_shallowmodel: 
[[  0   2   6   6]
 [  0 107  27  30]
 [  1  35 207  25]
 [  0  30  39  56]]
01/22/2018 22:42:22 [INFO] exp_shallowmodel: ******************** dstc2 - Round 48 
01/22/2018 22:42:22 [INFO] exp_shallowmodel: #(data) = 4583
01/22/2018 22:42:22 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:42:22 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:42:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:42:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:42:22 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:42:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:43:46 [INFO] exp_shallowmodel: train time: 83.919s
01/22/2018 22:43:46 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:43:46 [INFO] exp_shallowmodel: accuracy:   0.643
01/22/2018 22:43:46 [INFO] exp_shallowmodel: f1_score:   0.487
01/22/2018 22:43:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:43:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.07      0.10        14
          C       0.65      0.69      0.67       164
          F       0.73      0.74      0.74       268
          R       0.45      0.43      0.44       125

avg / total       0.63      0.64      0.64       571

01/22/2018 22:43:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:43:46 [INFO] exp_shallowmodel: 
[[  1   0   9   4]
 [  0 113  25  26]
 [  2  32 199  35]
 [  3  30  38  54]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:43:50 [INFO] exp_shallowmodel: ******************** dstc2 - Round 49 
01/22/2018 22:43:50 [INFO] exp_shallowmodel: #(data) = 4568
01/22/2018 22:43:50 [INFO] exp_shallowmodel: #(feature) = 17612
01/22/2018 22:43:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:43:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:43:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:43:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:43:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:45:11 [INFO] exp_shallowmodel: train time: 81.243s
01/22/2018 22:45:11 [INFO] exp_shallowmodel: test time:  0.015s
01/22/2018 22:45:11 [INFO] exp_shallowmodel: accuracy:   0.630
01/22/2018 22:45:11 [INFO] exp_shallowmodel: f1_score:   0.450
01/22/2018 22:45:11 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:45:11 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        16
          C       0.61      0.66      0.63       169
          F       0.73      0.76      0.74       271
          R       0.44      0.41      0.42       130

avg / total       0.61      0.63      0.62       586

01/22/2018 22:45:11 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:45:11 [INFO] exp_shallowmodel: 
[[  0   1  10   5]
 [  0 111  26  32]
 [  2  34 205  30]
 [  1  35  41  53]]
01/22/2018 22:45:23 [INFO] exp_shallowmodel: ******************** dstc3 - Round 0 
01/22/2018 22:45:23 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:45:23 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:45:23 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:45:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:45:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:45:23 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:45:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:46:45 [INFO] exp_shallowmodel: train time: 81.922s
01/22/2018 22:46:45 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:46:45 [INFO] exp_shallowmodel: accuracy:   0.613
01/22/2018 22:46:45 [INFO] exp_shallowmodel: f1_score:   0.497
01/22/2018 22:46:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:46:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.57      0.20      0.30        20
          C       0.53      0.56      0.54       169
          F       0.74      0.78      0.76       281
          R       0.42      0.37      0.39       122

avg / total       0.61      0.61      0.61       592

01/22/2018 22:46:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:46:45 [INFO] exp_shallowmodel: 
[[  4   1   6   9]
 [  2  94  40  33]
 [  0  41 220  20]
 [  1  43  33  45]]
01/22/2018 22:46:49 [INFO] exp_shallowmodel: ******************** dstc3 - Round 1 
01/22/2018 22:46:49 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:46:49 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:46:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:46:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:46:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:46:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:46:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:48:16 [INFO] exp_shallowmodel: train time: 86.479s
01/22/2018 22:48:16 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:48:16 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 22:48:16 [INFO] exp_shallowmodel: f1_score:   0.480
01/22/2018 22:48:16 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:48:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.23      0.15      0.18        20
          C       0.57      0.64      0.61       169
          F       0.76      0.79      0.77       281
          R       0.40      0.33      0.36       122

avg / total       0.62      0.63      0.62       592

01/22/2018 22:48:16 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:48:16 [INFO] exp_shallowmodel: 
[[  3   5   8   4]
 [  2 108  30  29]
 [  2  30 221  28]
 [  6  45  31  40]]
01/22/2018 22:48:20 [INFO] exp_shallowmodel: ******************** dstc3 - Round 2 
01/22/2018 22:48:20 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:48:20 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:48:20 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:48:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:48:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:48:20 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:48:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:49:46 [INFO] exp_shallowmodel: train time: 85.808s
01/22/2018 22:49:46 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:49:46 [INFO] exp_shallowmodel: accuracy:   0.610
01/22/2018 22:49:46 [INFO] exp_shallowmodel: f1_score:   0.460
01/22/2018 22:49:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:49:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.53      0.59      0.56       169
          F       0.76      0.77      0.76       281
          R       0.39      0.35      0.37       122

avg / total       0.60      0.61      0.60       592

01/22/2018 22:49:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:49:46 [INFO] exp_shallowmodel: 
[[  2   5  10   3]
 [  0 100  29  40]
 [  0  41 216  24]
 [  5  44  30  43]]
01/22/2018 22:49:50 [INFO] exp_shallowmodel: ******************** dstc3 - Round 3 
01/22/2018 22:49:50 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:49:50 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:49:50 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:49:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:49:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:49:50 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:49:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:51:15 [INFO] exp_shallowmodel: train time: 85.010s
01/22/2018 22:51:15 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:51:15 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 22:51:15 [INFO] exp_shallowmodel: f1_score:   0.482
01/22/2018 22:51:15 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:51:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.10      0.15        20
          C       0.61      0.68      0.64       169
          F       0.71      0.75      0.73       281
          R       0.45      0.37      0.40       122

avg / total       0.61      0.63      0.62       592

01/22/2018 22:51:15 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:51:15 [INFO] exp_shallowmodel: 
[[  2   2  12   4]
 [  1 115  31  22]
 [  2  39 210  30]
 [  1  33  43  45]]
01/22/2018 22:51:20 [INFO] exp_shallowmodel: ******************** dstc3 - Round 4 
01/22/2018 22:51:20 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:51:20 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:51:20 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:51:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:51:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:51:20 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:51:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:52:43 [INFO] exp_shallowmodel: train time: 83.835s
01/22/2018 22:52:43 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:52:43 [INFO] exp_shallowmodel: accuracy:   0.586
01/22/2018 22:52:43 [INFO] exp_shallowmodel: f1_score:   0.412
01/22/2018 22:52:43 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:52:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.52      0.53      0.53       169
          F       0.73      0.74      0.73       281
          R       0.38      0.39      0.39       122

avg / total       0.57      0.59      0.58       592

01/22/2018 22:52:43 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:52:43 [INFO] exp_shallowmodel: 
[[  0   4   8   8]
 [  1  90  39  39]
 [  2  39 209  31]
 [  2  40  32  48]]
01/22/2018 22:52:48 [INFO] exp_shallowmodel: ******************** dstc3 - Round 5 
01/22/2018 22:52:48 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:52:48 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:52:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:52:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:52:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:52:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:52:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:54:14 [INFO] exp_shallowmodel: train time: 86.499s
01/22/2018 22:54:14 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:54:14 [INFO] exp_shallowmodel: accuracy:   0.584
01/22/2018 22:54:14 [INFO] exp_shallowmodel: f1_score:   0.408
01/22/2018 22:54:14 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:54:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.51      0.54      0.53       169
          F       0.70      0.75      0.73       281
          R       0.41      0.35      0.38       122

avg / total       0.56      0.58      0.57       592

01/22/2018 22:54:14 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:54:14 [INFO] exp_shallowmodel: 
[[  0   3  12   5]
 [  2  91  41  35]
 [  3  44 212  22]
 [  3  39  37  43]]
01/22/2018 22:54:19 [INFO] exp_shallowmodel: ******************** dstc3 - Round 6 
01/22/2018 22:54:19 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:54:19 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:54:19 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:54:19 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:54:19 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:54:19 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:54:19 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:55:47 [INFO] exp_shallowmodel: train time: 88.392s
01/22/2018 22:55:47 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:55:47 [INFO] exp_shallowmodel: accuracy:   0.617
01/22/2018 22:55:47 [INFO] exp_shallowmodel: f1_score:   0.443
01/22/2018 22:55:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:55:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.05      0.08        20
          C       0.56      0.64      0.60       169
          F       0.76      0.78      0.77       281
          R       0.35      0.30      0.32       122

avg / total       0.60      0.62      0.61       592

01/22/2018 22:55:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:55:47 [INFO] exp_shallowmodel: 
[[  1   4   7   8]
 [  0 108  23  38]
 [  1  37 219  24]
 [  3  43  39  37]]
01/22/2018 22:55:52 [INFO] exp_shallowmodel: ******************** dstc3 - Round 7 
01/22/2018 22:55:52 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:55:52 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:55:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:55:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:55:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:55:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:55:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:57:19 [INFO] exp_shallowmodel: train time: 87.628s
01/22/2018 22:57:19 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:57:19 [INFO] exp_shallowmodel: accuracy:   0.579
01/22/2018 22:57:19 [INFO] exp_shallowmodel: f1_score:   0.426
01/22/2018 22:57:19 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:57:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.10      0.13        20
          C       0.53      0.55      0.54       169
          F       0.70      0.77      0.73       281
          R       0.33      0.27      0.30       122

avg / total       0.56      0.58      0.57       592

01/22/2018 22:57:19 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:57:19 [INFO] exp_shallowmodel: 
[[  2   2  11   5]
 [  2  93  41  33]
 [  1  35 215  30]
 [  5  45  39  33]]
01/22/2018 22:57:24 [INFO] exp_shallowmodel: ******************** dstc3 - Round 8 
01/22/2018 22:57:24 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 22:57:24 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:57:24 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:57:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:57:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:57:24 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:57:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 22:58:51 [INFO] exp_shallowmodel: train time: 86.751s
01/22/2018 22:58:51 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 22:58:51 [INFO] exp_shallowmodel: accuracy:   0.581
01/22/2018 22:58:51 [INFO] exp_shallowmodel: f1_score:   0.440
01/22/2018 22:58:51 [INFO] exp_shallowmodel: classification report:
01/22/2018 22:58:51 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.22      0.10      0.14        20
          C       0.51      0.50      0.51       169
          F       0.72      0.75      0.73       281
          R       0.38      0.39      0.38       122

avg / total       0.57      0.58      0.58       592

01/22/2018 22:58:51 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 22:58:51 [INFO] exp_shallowmodel: 
[[  2   3  11   4]
 [  2  85  36  46]
 [  2  42 210  27]
 [  3  36  36  47]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 22:58:55 [INFO] exp_shallowmodel: ******************** dstc3 - Round 9 
01/22/2018 22:58:55 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 22:58:55 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 22:58:55 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 22:58:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 22:58:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 22:58:55 [INFO] exp_shallowmodel: Training: 
01/22/2018 22:58:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:00:17 [INFO] exp_shallowmodel: train time: 81.786s
01/22/2018 23:00:17 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:00:17 [INFO] exp_shallowmodel: accuracy:   0.597
01/22/2018 23:00:17 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 23:00:17 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:00:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.07      0.12        28
          C       0.54      0.59      0.56       172
          F       0.71      0.78      0.74       283
          R       0.37      0.32      0.34       123

avg / total       0.58      0.60      0.58       606

01/22/2018 23:00:17 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:00:17 [INFO] exp_shallowmodel: 
[[  2  10  11   5]
 [  0 101  37  34]
 [  2  33 220  28]
 [  1  43  40  39]]
01/22/2018 23:00:21 [INFO] exp_shallowmodel: ******************** dstc3 - Round 10 
01/22/2018 23:00:21 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:00:21 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:00:21 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:00:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:00:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:00:21 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:00:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:01:47 [INFO] exp_shallowmodel: train time: 85.486s
01/22/2018 23:01:47 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:01:47 [INFO] exp_shallowmodel: accuracy:   0.610
01/22/2018 23:01:47 [INFO] exp_shallowmodel: f1_score:   0.469
01/22/2018 23:01:47 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:01:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.10      0.17        20
          C       0.56      0.59      0.57       169
          F       0.73      0.75      0.74       281
          R       0.39      0.39      0.39       122

avg / total       0.61      0.61      0.60       592

01/22/2018 23:01:47 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:01:47 [INFO] exp_shallowmodel: 
[[  2   4   7   7]
 [  0  99  35  35]
 [  1  36 212  32]
 [  1  37  36  48]]
01/22/2018 23:01:51 [INFO] exp_shallowmodel: ******************** dstc3 - Round 11 
01/22/2018 23:01:51 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:01:51 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:01:51 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:01:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:01:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:01:51 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:01:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:03:20 [INFO] exp_shallowmodel: train time: 88.931s
01/22/2018 23:03:20 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:03:20 [INFO] exp_shallowmodel: accuracy:   0.645
01/22/2018 23:03:20 [INFO] exp_shallowmodel: f1_score:   0.474
01/22/2018 23:03:20 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:03:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.05      0.07        20
          C       0.61      0.65      0.63       169
          F       0.74      0.79      0.77       281
          R       0.48      0.39      0.43       122

avg / total       0.63      0.65      0.63       592

01/22/2018 23:03:20 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:03:20 [INFO] exp_shallowmodel: 
[[  1   6   9   4]
 [  1 110  36  22]
 [  3  28 223  27]
 [  3  37  34  48]]
01/22/2018 23:03:24 [INFO] exp_shallowmodel: ******************** dstc3 - Round 12 
01/22/2018 23:03:24 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:03:24 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:03:24 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:03:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:03:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:03:24 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:03:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:04:50 [INFO] exp_shallowmodel: train time: 85.171s
01/22/2018 23:04:50 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:04:50 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 23:04:50 [INFO] exp_shallowmodel: f1_score:   0.429
01/22/2018 23:04:50 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:04:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.05      0.08        20
          C       0.52      0.58      0.55       169
          F       0.73      0.74      0.74       281
          R       0.37      0.34      0.35       122

avg / total       0.58      0.59      0.58       592

01/22/2018 23:04:50 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:04:50 [INFO] exp_shallowmodel: 
[[  1   5   9   5]
 [  2  98  35  34]
 [  0  41 209  31]
 [  2  46  33  41]]
01/22/2018 23:04:54 [INFO] exp_shallowmodel: ******************** dstc3 - Round 13 
01/22/2018 23:04:54 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:04:54 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:04:54 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:04:54 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:04:54 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:04:54 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:04:54 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:06:21 [INFO] exp_shallowmodel: train time: 86.901s
01/22/2018 23:06:21 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:06:21 [INFO] exp_shallowmodel: accuracy:   0.625
01/22/2018 23:06:21 [INFO] exp_shallowmodel: f1_score:   0.496
01/22/2018 23:06:21 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:06:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       1.00      0.15      0.26        20
          C       0.52      0.58      0.55       169
          F       0.75      0.80      0.77       281
          R       0.44      0.37      0.40       122

avg / total       0.63      0.62      0.62       592

01/22/2018 23:06:21 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:06:21 [INFO] exp_shallowmodel: 
[[  3   2  11   4]
 [  0  98  35  36]
 [  0  39 224  18]
 [  0  48  29  45]]
01/22/2018 23:06:25 [INFO] exp_shallowmodel: ******************** dstc3 - Round 14 
01/22/2018 23:06:25 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:06:25 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:06:25 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:06:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:06:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:06:25 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:06:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:07:53 [INFO] exp_shallowmodel: train time: 87.741s
01/22/2018 23:07:53 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:07:53 [INFO] exp_shallowmodel: accuracy:   0.603
01/22/2018 23:07:53 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 23:07:53 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:07:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.05      0.08        20
          C       0.52      0.58      0.55       169
          F       0.74      0.75      0.75       281
          R       0.40      0.38      0.39       122

avg / total       0.59      0.60      0.60       592

01/22/2018 23:07:53 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:07:53 [INFO] exp_shallowmodel: 
[[  1   2  11   6]
 [  0  98  33  38]
 [  1  44 212  24]
 [  4  43  29  46]]
01/22/2018 23:07:58 [INFO] exp_shallowmodel: ******************** dstc3 - Round 15 
01/22/2018 23:07:58 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:07:58 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:07:58 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:07:58 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:07:58 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:07:58 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:07:58 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:09:26 [INFO] exp_shallowmodel: train time: 88.404s
01/22/2018 23:09:26 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:09:26 [INFO] exp_shallowmodel: accuracy:   0.583
01/22/2018 23:09:26 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 23:09:26 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:09:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.54      0.56      0.55       169
          F       0.71      0.75      0.73       281
          R       0.33      0.30      0.31       122

avg / total       0.57      0.58      0.57       592

01/22/2018 23:09:26 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:09:26 [INFO] exp_shallowmodel: 
[[  2   3   7   8]
 [  0  95  36  38]
 [  2  38 211  30]
 [  3  40  42  37]]
01/22/2018 23:09:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 16 
01/22/2018 23:09:30 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:09:30 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:09:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:09:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:09:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:09:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:09:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:10:56 [INFO] exp_shallowmodel: train time: 85.108s
01/22/2018 23:10:56 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:10:56 [INFO] exp_shallowmodel: accuracy:   0.620
01/22/2018 23:10:56 [INFO] exp_shallowmodel: f1_score:   0.472
01/22/2018 23:10:56 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:10:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.53      0.62      0.57       169
          F       0.74      0.77      0.75       281
          R       0.46      0.38      0.41       122

avg / total       0.61      0.62      0.61       592

01/22/2018 23:10:56 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:10:56 [INFO] exp_shallowmodel: 
[[  2   3  11   4]
 [  1 104  32  32]
 [  0  47 215  19]
 [  4  41  31  46]]
01/22/2018 23:11:00 [INFO] exp_shallowmodel: ******************** dstc3 - Round 17 
01/22/2018 23:11:00 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:11:00 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:11:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:11:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:11:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:11:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:11:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:12:24 [INFO] exp_shallowmodel: train time: 84.208s
01/22/2018 23:12:24 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:12:24 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 23:12:24 [INFO] exp_shallowmodel: f1_score:   0.473
01/22/2018 23:12:24 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:12:24 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.10      0.17        20
          C       0.55      0.56      0.55       169
          F       0.76      0.79      0.77       281
          R       0.40      0.39      0.40       122

avg / total       0.61      0.62      0.61       592

01/22/2018 23:12:24 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:12:24 [INFO] exp_shallowmodel: 
[[  2   3   9   6]
 [  1  95  27  46]
 [  0  39 223  19]
 [  1  37  36  48]]
01/22/2018 23:12:28 [INFO] exp_shallowmodel: ******************** dstc3 - Round 18 
01/22/2018 23:12:28 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:12:28 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:12:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:12:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:12:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:12:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:12:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:13:53 [INFO] exp_shallowmodel: train time: 84.319s
01/22/2018 23:13:53 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:13:53 [INFO] exp_shallowmodel: accuracy:   0.603
01/22/2018 23:13:53 [INFO] exp_shallowmodel: f1_score:   0.426
01/22/2018 23:13:53 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:13:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.57      0.63      0.60       169
          F       0.71      0.74      0.72       281
          R       0.42      0.35      0.38       122

avg / total       0.58      0.60      0.59       592

01/22/2018 23:13:53 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:13:53 [INFO] exp_shallowmodel: 
[[  0   5  11   4]
 [  3 106  32  28]
 [  1  45 208  27]
 [  5  30  44  43]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:13:57 [INFO] exp_shallowmodel: ******************** dstc3 - Round 19 
01/22/2018 23:13:57 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 23:13:57 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:13:57 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:13:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:13:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:13:57 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:13:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:15:19 [INFO] exp_shallowmodel: train time: 82.007s
01/22/2018 23:15:19 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:15:19 [INFO] exp_shallowmodel: accuracy:   0.604
01/22/2018 23:15:19 [INFO] exp_shallowmodel: f1_score:   0.437
01/22/2018 23:15:19 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:15:19 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.04      0.06        28
          C       0.54      0.56      0.55       172
          F       0.72      0.79      0.75       283
          R       0.41      0.37      0.39       123

avg / total       0.58      0.60      0.59       606

01/22/2018 23:15:19 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:15:19 [INFO] exp_shallowmodel: 
[[  1  10  13   4]
 [  0  97  40  35]
 [  2  33 223  25]
 [  3  41  34  45]]
01/22/2018 23:15:24 [INFO] exp_shallowmodel: ******************** dstc3 - Round 20 
01/22/2018 23:15:24 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:15:24 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:15:24 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:15:24 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:15:24 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:15:24 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:15:24 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:16:43 [INFO] exp_shallowmodel: train time: 79.124s
01/22/2018 23:16:43 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:16:43 [INFO] exp_shallowmodel: accuracy:   0.593
01/22/2018 23:16:43 [INFO] exp_shallowmodel: f1_score:   0.465
01/22/2018 23:16:43 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:16:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.20      0.27        20
          C       0.54      0.54      0.54       169
          F       0.72      0.79      0.75       281
          R       0.33      0.28      0.30       122

avg / total       0.58      0.59      0.58       592

01/22/2018 23:16:43 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:16:43 [INFO] exp_shallowmodel: 
[[  4   3  11   2]
 [  0  92  40  37]
 [  2  28 221  30]
 [  4  47  37  34]]
01/22/2018 23:16:47 [INFO] exp_shallowmodel: ******************** dstc3 - Round 21 
01/22/2018 23:16:47 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:16:47 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:16:47 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:16:47 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:16:47 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:16:47 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:16:47 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:18:10 [INFO] exp_shallowmodel: train time: 83.247s
01/22/2018 23:18:10 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:18:10 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 23:18:10 [INFO] exp_shallowmodel: f1_score:   0.456
01/22/2018 23:18:10 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:18:10 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.05      0.07        20
          C       0.54      0.57      0.56       169
          F       0.73      0.76      0.75       281
          R       0.46      0.43      0.45       122

avg / total       0.60      0.61      0.61       592

01/22/2018 23:18:10 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:18:10 [INFO] exp_shallowmodel: 
[[  1   6   8   5]
 [  3  97  36  33]
 [  1  43 213  24]
 [  2  34  33  53]]
01/22/2018 23:18:15 [INFO] exp_shallowmodel: ******************** dstc3 - Round 22 
01/22/2018 23:18:15 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:18:15 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:18:15 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:18:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:18:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:18:15 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:18:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:19:34 [INFO] exp_shallowmodel: train time: 79.186s
01/22/2018 23:19:34 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:19:34 [INFO] exp_shallowmodel: accuracy:   0.622
01/22/2018 23:19:34 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 23:19:34 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:19:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.55      0.58      0.57       169
          F       0.75      0.79      0.77       281
          R       0.42      0.39      0.41       122

avg / total       0.60      0.62      0.61       592

01/22/2018 23:19:34 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:19:34 [INFO] exp_shallowmodel: 
[[  0   5  11   4]
 [  2  98  30  39]
 [  1  36 222  22]
 [  4  38  32  48]]
01/22/2018 23:19:38 [INFO] exp_shallowmodel: ******************** dstc3 - Round 23 
01/22/2018 23:19:38 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:19:38 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:19:38 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:19:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:19:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:19:38 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:19:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:21:03 [INFO] exp_shallowmodel: train time: 84.669s
01/22/2018 23:21:03 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:21:03 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 23:21:03 [INFO] exp_shallowmodel: f1_score:   0.428
01/22/2018 23:21:03 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:21:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.05      0.08        20
          C       0.56      0.62      0.59       169
          F       0.69      0.74      0.71       281
          R       0.37      0.30      0.33       122

avg / total       0.57      0.59      0.58       592

01/22/2018 23:21:03 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:21:03 [INFO] exp_shallowmodel: 
[[  1   3  12   4]
 [  3 105  28  33]
 [  1  47 207  26]
 [  1  32  52  37]]
01/22/2018 23:21:07 [INFO] exp_shallowmodel: ******************** dstc3 - Round 24 
01/22/2018 23:21:07 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:21:07 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:21:07 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:21:07 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:21:07 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:21:07 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:21:07 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:22:29 [INFO] exp_shallowmodel: train time: 81.941s
01/22/2018 23:22:29 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:22:29 [INFO] exp_shallowmodel: accuracy:   0.601
01/22/2018 23:22:29 [INFO] exp_shallowmodel: f1_score:   0.467
01/22/2018 23:22:29 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:22:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.15      0.19        20
          C       0.53      0.55      0.54       169
          F       0.73      0.76      0.74       281
          R       0.42      0.38      0.40       122

avg / total       0.59      0.60      0.60       592

01/22/2018 23:22:29 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:22:29 [INFO] exp_shallowmodel: 
[[  3   1   7   9]
 [  3  93  39  34]
 [  2  44 214  21]
 [  4  37  35  46]]
01/22/2018 23:22:34 [INFO] exp_shallowmodel: ******************** dstc3 - Round 25 
01/22/2018 23:22:34 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:22:34 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:22:34 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:22:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:22:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:22:34 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:22:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:23:56 [INFO] exp_shallowmodel: train time: 82.391s
01/22/2018 23:23:56 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:23:56 [INFO] exp_shallowmodel: accuracy:   0.601
01/22/2018 23:23:56 [INFO] exp_shallowmodel: f1_score:   0.423
01/22/2018 23:23:56 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:23:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.55      0.57      0.56       169
          F       0.71      0.75      0.73       281
          R       0.40      0.39      0.40       122

avg / total       0.58      0.60      0.59       592

01/22/2018 23:23:56 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:23:56 [INFO] exp_shallowmodel: 
[[  0   2  13   5]
 [  0  96  35  38]
 [  1  40 212  28]
 [  0  37  37  48]]
01/22/2018 23:24:00 [INFO] exp_shallowmodel: ******************** dstc3 - Round 26 
01/22/2018 23:24:00 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:24:00 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:24:00 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:24:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:24:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:24:00 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:24:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:25:23 [INFO] exp_shallowmodel: train time: 82.966s
01/22/2018 23:25:23 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:25:23 [INFO] exp_shallowmodel: accuracy:   0.615
01/22/2018 23:25:23 [INFO] exp_shallowmodel: f1_score:   0.456
01/22/2018 23:25:23 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:25:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.10      0.14        20
          C       0.58      0.57      0.57       169
          F       0.73      0.81      0.77       281
          R       0.36      0.32      0.34       122

avg / total       0.60      0.61      0.60       592

01/22/2018 23:25:23 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:25:23 [INFO] exp_shallowmodel: 
[[  2   4   9   5]
 [  0  96  32  41]
 [  1  31 227  22]
 [  5  34  44  39]]
01/22/2018 23:25:28 [INFO] exp_shallowmodel: ******************** dstc3 - Round 27 
01/22/2018 23:25:28 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:25:28 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:25:28 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:25:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:25:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:25:28 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:25:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:26:48 [INFO] exp_shallowmodel: train time: 80.817s
01/22/2018 23:26:48 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:26:48 [INFO] exp_shallowmodel: accuracy:   0.601
01/22/2018 23:26:48 [INFO] exp_shallowmodel: f1_score:   0.473
01/22/2018 23:26:48 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:26:48 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.15      0.21        20
          C       0.52      0.54      0.53       169
          F       0.74      0.76      0.75       281
          R       0.40      0.39      0.40       122

avg / total       0.60      0.60      0.60       592

01/22/2018 23:26:48 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:26:48 [INFO] exp_shallowmodel: 
[[  3   6   8   3]
 [  2  92  31  44]
 [  0  43 213  25]
 [  3  35  36  48]]
01/22/2018 23:26:53 [INFO] exp_shallowmodel: ******************** dstc3 - Round 28 
01/22/2018 23:26:53 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:26:53 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:26:53 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:26:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:26:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:26:53 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:26:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:28:06 [INFO] exp_shallowmodel: train time: 72.845s
01/22/2018 23:28:06 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:28:06 [INFO] exp_shallowmodel: accuracy:   0.637
01/22/2018 23:28:06 [INFO] exp_shallowmodel: f1_score:   0.496
01/22/2018 23:28:06 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:28:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.15      0.21        20
          C       0.61      0.57      0.59       169
          F       0.75      0.81      0.78       281
          R       0.40      0.40      0.40       122

avg / total       0.63      0.64      0.63       592

01/22/2018 23:28:06 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:28:06 [INFO] exp_shallowmodel: 
[[  3   1   8   8]
 [  0  97  32  40]
 [  1  28 228  24]
 [  5  34  34  49]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:28:10 [INFO] exp_shallowmodel: ******************** dstc3 - Round 29 
01/22/2018 23:28:10 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 23:28:10 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:28:10 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:28:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:28:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:28:10 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:28:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:29:13 [INFO] exp_shallowmodel: train time: 62.827s
01/22/2018 23:29:13 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:29:13 [INFO] exp_shallowmodel: accuracy:   0.617
01/22/2018 23:29:13 [INFO] exp_shallowmodel: f1_score:   0.451
01/22/2018 23:29:13 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:29:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.04      0.06        28
          C       0.57      0.57      0.57       172
          F       0.73      0.79      0.76       283
          R       0.41      0.41      0.41       123

avg / total       0.60      0.62      0.60       606

01/22/2018 23:29:13 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:29:13 [INFO] exp_shallowmodel: 
[[  1   7  10  10]
 [  1  98  36  37]
 [  3  30 224  26]
 [  1  36  35  51]]
01/22/2018 23:29:17 [INFO] exp_shallowmodel: ******************** dstc3 - Round 30 
01/22/2018 23:29:17 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:29:17 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:29:17 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:29:17 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:29:17 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:29:17 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:29:17 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:30:22 [INFO] exp_shallowmodel: train time: 64.658s
01/22/2018 23:30:22 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:30:22 [INFO] exp_shallowmodel: accuracy:   0.617
01/22/2018 23:30:22 [INFO] exp_shallowmodel: f1_score:   0.454
01/22/2018 23:30:22 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:30:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.05      0.08        20
          C       0.57      0.60      0.59       169
          F       0.72      0.77      0.74       281
          R       0.44      0.39      0.41       122

avg / total       0.60      0.62      0.61       592

01/22/2018 23:30:22 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:30:22 [INFO] exp_shallowmodel: 
[[  1   4  11   4]
 [  2 101  31  35]
 [  1  42 215  23]
 [  2  29  43  48]]
01/22/2018 23:30:26 [INFO] exp_shallowmodel: ******************** dstc3 - Round 31 
01/22/2018 23:30:26 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:30:26 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:30:26 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:30:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:30:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:30:26 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:30:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:31:31 [INFO] exp_shallowmodel: train time: 64.943s
01/22/2018 23:31:31 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:31:31 [INFO] exp_shallowmodel: accuracy:   0.630
01/22/2018 23:31:31 [INFO] exp_shallowmodel: f1_score:   0.484
01/22/2018 23:31:31 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:31:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.10      0.16        20
          C       0.58      0.61      0.59       169
          F       0.77      0.77      0.77       281
          R       0.40      0.42      0.41       122

avg / total       0.63      0.63      0.63       592

01/22/2018 23:31:31 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:31:31 [INFO] exp_shallowmodel: 
[[  2   6   4   8]
 [  1 103  26  39]
 [  0  35 217  29]
 [  2  34  35  51]]
01/22/2018 23:31:35 [INFO] exp_shallowmodel: ******************** dstc3 - Round 32 
01/22/2018 23:31:35 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:31:35 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:31:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:31:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:31:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:31:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:31:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:32:37 [INFO] exp_shallowmodel: train time: 61.454s
01/22/2018 23:32:37 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:32:37 [INFO] exp_shallowmodel: accuracy:   0.611
01/22/2018 23:32:37 [INFO] exp_shallowmodel: f1_score:   0.453
01/22/2018 23:32:37 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:32:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.05      0.08        20
          C       0.56      0.59      0.57       169
          F       0.74      0.75      0.75       281
          R       0.40      0.42      0.41       122

avg / total       0.60      0.61      0.61       592

01/22/2018 23:32:37 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:32:37 [INFO] exp_shallowmodel: 
[[  1   3   8   8]
 [  1 100  31  37]
 [  1  40 210  30]
 [  2  36  33  51]]
01/22/2018 23:32:41 [INFO] exp_shallowmodel: ******************** dstc3 - Round 33 
01/22/2018 23:32:41 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:32:41 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:32:41 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:32:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:32:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:32:41 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:32:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:33:44 [INFO] exp_shallowmodel: train time: 63.177s
01/22/2018 23:33:44 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:33:44 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 23:33:44 [INFO] exp_shallowmodel: f1_score:   0.428
01/22/2018 23:33:44 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:33:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.05      0.08        20
          C       0.55      0.57      0.56       169
          F       0.72      0.75      0.74       281
          R       0.34      0.33      0.33       122

avg / total       0.58      0.59      0.58       592

01/22/2018 23:33:44 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:33:44 [INFO] exp_shallowmodel: 
[[  1   5  11   3]
 [  1  96  28  44]
 [  0  39 212  30]
 [  2  36  44  40]]
01/22/2018 23:33:48 [INFO] exp_shallowmodel: ******************** dstc3 - Round 34 
01/22/2018 23:33:48 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:33:48 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:33:48 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:33:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:33:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:33:48 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:33:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:34:52 [INFO] exp_shallowmodel: train time: 64.065s
01/22/2018 23:34:52 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:34:52 [INFO] exp_shallowmodel: accuracy:   0.598
01/22/2018 23:34:52 [INFO] exp_shallowmodel: f1_score:   0.425
01/22/2018 23:34:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:34:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.57      0.64      0.60       169
          F       0.71      0.72      0.71       281
          R       0.41      0.36      0.38       122

avg / total       0.58      0.60      0.59       592

01/22/2018 23:34:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:34:52 [INFO] exp_shallowmodel: 
[[  0   5  12   3]
 [  1 108  30  30]
 [  4  45 202  30]
 [  4  33  41  44]]
01/22/2018 23:34:57 [INFO] exp_shallowmodel: ******************** dstc3 - Round 35 
01/22/2018 23:34:57 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:34:57 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:34:57 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:34:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:34:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:34:57 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:34:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:36:00 [INFO] exp_shallowmodel: train time: 63.523s
01/22/2018 23:36:00 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:36:00 [INFO] exp_shallowmodel: accuracy:   0.610
01/22/2018 23:36:00 [INFO] exp_shallowmodel: f1_score:   0.474
01/22/2018 23:36:00 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:36:00 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.15      0.20        20
          C       0.56      0.52      0.54       169
          F       0.73      0.79      0.76       281
          R       0.41      0.39      0.40       122

avg / total       0.60      0.61      0.60       592

01/22/2018 23:36:00 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:36:00 [INFO] exp_shallowmodel: 
[[  3   2   9   6]
 [  3  88  37  41]
 [  2  34 222  23]
 [  2  34  38  48]]
01/22/2018 23:36:04 [INFO] exp_shallowmodel: ******************** dstc3 - Round 36 
01/22/2018 23:36:04 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:36:04 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:36:04 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:36:04 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:36:04 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:36:04 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:36:04 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:37:08 [INFO] exp_shallowmodel: train time: 63.861s
01/22/2018 23:37:08 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:37:08 [INFO] exp_shallowmodel: accuracy:   0.610
01/22/2018 23:37:08 [INFO] exp_shallowmodel: f1_score:   0.428
01/22/2018 23:37:08 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:37:08 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.53      0.56      0.54       169
          F       0.72      0.78      0.75       281
          R       0.45      0.39      0.42       122

avg / total       0.59      0.61      0.60       592

01/22/2018 23:37:08 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:37:08 [INFO] exp_shallowmodel: 
[[  0   5   9   6]
 [  2  94  42  31]
 [  0  41 219  21]
 [  2  38  34  48]]
01/22/2018 23:37:13 [INFO] exp_shallowmodel: ******************** dstc3 - Round 37 
01/22/2018 23:37:13 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:37:13 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:37:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:37:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:37:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:37:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:37:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:38:17 [INFO] exp_shallowmodel: train time: 64.449s
01/22/2018 23:38:17 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:38:17 [INFO] exp_shallowmodel: accuracy:   0.630
01/22/2018 23:38:17 [INFO] exp_shallowmodel: f1_score:   0.463
01/22/2018 23:38:17 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:38:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.05      0.08        20
          C       0.57      0.64      0.61       169
          F       0.75      0.77      0.76       281
          R       0.43      0.38      0.40       122

avg / total       0.61      0.63      0.62       592

01/22/2018 23:38:17 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:38:17 [INFO] exp_shallowmodel: 
[[  1   2  11   6]
 [  1 109  30  29]
 [  0  39 217  25]
 [  2  41  33  46]]
01/22/2018 23:38:21 [INFO] exp_shallowmodel: ******************** dstc3 - Round 38 
01/22/2018 23:38:21 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:38:21 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:38:21 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:38:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:38:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:38:21 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:38:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:39:25 [INFO] exp_shallowmodel: train time: 64.191s
01/22/2018 23:39:25 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:39:25 [INFO] exp_shallowmodel: accuracy:   0.590
01/22/2018 23:39:25 [INFO] exp_shallowmodel: f1_score:   0.459
01/22/2018 23:39:25 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:39:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.15      0.23        20
          C       0.53      0.57      0.55       169
          F       0.72      0.76      0.74       281
          R       0.34      0.30      0.32       122

avg / total       0.58      0.59      0.58       592

01/22/2018 23:39:25 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:39:25 [INFO] exp_shallowmodel: 
[[  3   5  10   2]
 [  0  96  33  40]
 [  0  39 213  29]
 [  3  42  40  37]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:39:30 [INFO] exp_shallowmodel: ******************** dstc3 - Round 39 
01/22/2018 23:39:30 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 23:39:30 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:39:30 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:39:30 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:39:30 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:39:30 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:39:30 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:40:33 [INFO] exp_shallowmodel: train time: 63.294s
01/22/2018 23:40:33 [INFO] exp_shallowmodel: test time:  0.018s
01/22/2018 23:40:33 [INFO] exp_shallowmodel: accuracy:   0.591
01/22/2018 23:40:33 [INFO] exp_shallowmodel: f1_score:   0.443
01/22/2018 23:40:33 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:40:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.07      0.12        28
          C       0.54      0.56      0.55       172
          F       0.71      0.76      0.73       283
          R       0.38      0.37      0.37       123

avg / total       0.58      0.59      0.58       606

01/22/2018 23:40:33 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:40:33 [INFO] exp_shallowmodel: 
[[  2   2  16   8]
 [  0  97  38  37]
 [  1  41 214  27]
 [  3  41  34  45]]
01/22/2018 23:40:37 [INFO] exp_shallowmodel: ******************** dstc3 - Round 40 
01/22/2018 23:40:37 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:40:37 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:40:37 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:40:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:40:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:40:37 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:40:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:41:42 [INFO] exp_shallowmodel: train time: 64.408s
01/22/2018 23:41:42 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:41:42 [INFO] exp_shallowmodel: accuracy:   0.600
01/22/2018 23:41:42 [INFO] exp_shallowmodel: f1_score:   0.436
01/22/2018 23:41:42 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:41:42 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.05      0.08        20
          C       0.54      0.54      0.54       169
          F       0.76      0.78      0.77       281
          R       0.34      0.37      0.35       122

avg / total       0.59      0.60      0.59       592

01/22/2018 23:41:42 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:41:42 [INFO] exp_shallowmodel: 
[[  1   2  13   4]
 [  0  91  25  53]
 [  0  32 218  31]
 [  3  43  31  45]]
01/22/2018 23:41:46 [INFO] exp_shallowmodel: ******************** dstc3 - Round 41 
01/22/2018 23:41:46 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:41:46 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:41:46 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:41:46 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:41:46 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:41:46 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:41:46 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:42:49 [INFO] exp_shallowmodel: train time: 63.217s
01/22/2018 23:42:49 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:42:49 [INFO] exp_shallowmodel: accuracy:   0.606
01/22/2018 23:42:49 [INFO] exp_shallowmodel: f1_score:   0.467
01/22/2018 23:42:49 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:42:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.15      0.20        20
          C       0.53      0.56      0.54       169
          F       0.74      0.78      0.76       281
          R       0.39      0.34      0.37       122

avg / total       0.59      0.61      0.60       592

01/22/2018 23:42:49 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:42:49 [INFO] exp_shallowmodel: 
[[  3   4   8   5]
 [  2  94  36  37]
 [  1  37 220  23]
 [  4  44  32  42]]
01/22/2018 23:42:53 [INFO] exp_shallowmodel: ******************** dstc3 - Round 42 
01/22/2018 23:42:53 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:42:53 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:42:53 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:42:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:42:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:42:53 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:42:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:43:57 [INFO] exp_shallowmodel: train time: 63.243s
01/22/2018 23:43:57 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:43:57 [INFO] exp_shallowmodel: accuracy:   0.598
01/22/2018 23:43:57 [INFO] exp_shallowmodel: f1_score:   0.452
01/22/2018 23:43:57 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:43:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.10      0.14        20
          C       0.53      0.56      0.55       169
          F       0.72      0.76      0.74       281
          R       0.41      0.36      0.38       122

avg / total       0.58      0.60      0.59       592

01/22/2018 23:43:57 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:43:57 [INFO] exp_shallowmodel: 
[[  2   5   9   4]
 [  2  95  40  32]
 [  1  39 213  28]
 [  3  40  35  44]]
01/22/2018 23:44:01 [INFO] exp_shallowmodel: ******************** dstc3 - Round 43 
01/22/2018 23:44:01 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:44:01 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:44:01 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:44:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:44:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:44:01 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:44:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:45:05 [INFO] exp_shallowmodel: train time: 63.960s
01/22/2018 23:45:05 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:45:05 [INFO] exp_shallowmodel: accuracy:   0.649
01/22/2018 23:45:05 [INFO] exp_shallowmodel: f1_score:   0.498
01/22/2018 23:45:05 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:45:05 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.10      0.15        20
          C       0.60      0.65      0.63       169
          F       0.75      0.78      0.77       281
          R       0.46      0.43      0.44       122

avg / total       0.64      0.65      0.64       592

01/22/2018 23:45:05 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:45:05 [INFO] exp_shallowmodel: 
[[  2   1  13   4]
 [  1 110  29  29]
 [  2  32 220  27]
 [  1  39  30  52]]
01/22/2018 23:45:09 [INFO] exp_shallowmodel: ******************** dstc3 - Round 44 
01/22/2018 23:45:09 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:45:09 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:45:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:45:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:45:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:45:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:45:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:46:14 [INFO] exp_shallowmodel: train time: 64.563s
01/22/2018 23:46:14 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:46:14 [INFO] exp_shallowmodel: accuracy:   0.598
01/22/2018 23:46:14 [INFO] exp_shallowmodel: f1_score:   0.468
01/22/2018 23:46:14 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:46:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.15      0.22        20
          C       0.52      0.59      0.55       169
          F       0.74      0.75      0.75       281
          R       0.38      0.34      0.35       122

avg / total       0.59      0.60      0.59       592

01/22/2018 23:46:14 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:46:14 [INFO] exp_shallowmodel: 
[[  3   3  10   4]
 [  1  99  29  40]
 [  2  44 211  24]
 [  1  45  35  41]]
01/22/2018 23:46:18 [INFO] exp_shallowmodel: ******************** dstc3 - Round 45 
01/22/2018 23:46:18 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:46:18 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:46:18 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:46:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:46:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:46:18 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:46:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:47:21 [INFO] exp_shallowmodel: train time: 62.578s
01/22/2018 23:47:21 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:47:21 [INFO] exp_shallowmodel: accuracy:   0.628
01/22/2018 23:47:21 [INFO] exp_shallowmodel: f1_score:   0.442
01/22/2018 23:47:21 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:47:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        20
          C       0.59      0.59      0.59       169
          F       0.73      0.80      0.76       281
          R       0.43      0.39      0.41       122

avg / total       0.61      0.63      0.62       592

01/22/2018 23:47:21 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:47:21 [INFO] exp_shallowmodel: 
[[  0   3  12   5]
 [  1 100  33  35]
 [  1  33 224  23]
 [  4  34  36  48]]
01/22/2018 23:47:25 [INFO] exp_shallowmodel: ******************** dstc3 - Round 46 
01/22/2018 23:47:25 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:47:25 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:47:25 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:47:25 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:47:25 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:47:25 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:47:25 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:48:27 [INFO] exp_shallowmodel: train time: 62.325s
01/22/2018 23:48:27 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:48:27 [INFO] exp_shallowmodel: accuracy:   0.606
01/22/2018 23:48:27 [INFO] exp_shallowmodel: f1_score:   0.444
01/22/2018 23:48:27 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:48:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.10      0.05      0.07        20
          C       0.56      0.56      0.56       169
          F       0.73      0.77      0.75       281
          R       0.42      0.39      0.41       122

avg / total       0.59      0.61      0.60       592

01/22/2018 23:48:27 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:48:27 [INFO] exp_shallowmodel: 
[[  1   4  11   4]
 [  3  95  33  38]
 [  1  40 215  25]
 [  5  32  37  48]]
01/22/2018 23:48:31 [INFO] exp_shallowmodel: ******************** dstc3 - Round 47 
01/22/2018 23:48:31 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:48:31 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:48:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:48:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:48:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:48:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:48:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:49:33 [INFO] exp_shallowmodel: train time: 61.649s
01/22/2018 23:49:33 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:49:33 [INFO] exp_shallowmodel: accuracy:   0.600
01/22/2018 23:49:33 [INFO] exp_shallowmodel: f1_score:   0.443
01/22/2018 23:49:33 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:49:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.57      0.56      0.56       169
          F       0.72      0.79      0.76       281
          R       0.32      0.29      0.30       122

avg / total       0.58      0.60      0.59       592

01/22/2018 23:49:33 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:49:33 [INFO] exp_shallowmodel: 
[[  2   1  10   7]
 [  1  95  32  41]
 [  0  32 223  26]
 [  4  40  43  35]]
01/22/2018 23:49:37 [INFO] exp_shallowmodel: ******************** dstc3 - Round 48 
01/22/2018 23:49:37 [INFO] exp_shallowmodel: #(data) = 4750
01/22/2018 23:49:37 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:49:37 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:49:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:49:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:49:37 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:49:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:50:45 [INFO] exp_shallowmodel: train time: 67.744s
01/22/2018 23:50:45 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:50:45 [INFO] exp_shallowmodel: accuracy:   0.613
01/22/2018 23:50:45 [INFO] exp_shallowmodel: f1_score:   0.470
01/22/2018 23:50:45 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:50:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.10      0.15        20
          C       0.53      0.57      0.55       169
          F       0.71      0.77      0.74       281
          R       0.48      0.41      0.44       122

avg / total       0.60      0.61      0.60       592

01/22/2018 23:50:45 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:50:45 [INFO] exp_shallowmodel: 
[[  2   7   9   2]
 [  0  96  43  30]
 [  5  39 215  22]
 [  0  38  34  50]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/22/2018 23:50:49 [INFO] exp_shallowmodel: ******************** dstc3 - Round 49 
01/22/2018 23:50:49 [INFO] exp_shallowmodel: #(data) = 4736
01/22/2018 23:50:49 [INFO] exp_shallowmodel: #(feature) = 20699
01/22/2018 23:50:49 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:50:49 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:50:49 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:50:49 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:50:49 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:51:54 [INFO] exp_shallowmodel: train time: 64.618s
01/22/2018 23:51:54 [INFO] exp_shallowmodel: test time:  0.017s
01/22/2018 23:51:54 [INFO] exp_shallowmodel: accuracy:   0.589
01/22/2018 23:51:54 [INFO] exp_shallowmodel: f1_score:   0.420
01/22/2018 23:51:54 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:51:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.07        28
          C       0.55      0.59      0.57       172
          F       0.70      0.77      0.74       283
          R       0.32      0.28      0.30       123

avg / total       0.57      0.59      0.57       606

01/22/2018 23:51:54 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:51:54 [INFO] exp_shallowmodel: 
[[  1  10   8   9]
 [  0 102  29  41]
 [  1  40 219  23]
 [  0  33  55  35]]
01/22/2018 23:52:09 [INFO] exp_shallowmodel: ******************** family - Round 0 
01/22/2018 23:52:09 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 23:52:09 [INFO] exp_shallowmodel: #(feature) = 49505
01/22/2018 23:52:09 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:52:09 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:52:09 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:52:09 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:52:09 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:53:25 [INFO] exp_shallowmodel: train time: 75.746s
01/22/2018 23:53:25 [INFO] exp_shallowmodel: test time:  0.025s
01/22/2018 23:53:25 [INFO] exp_shallowmodel: accuracy:   0.756
01/22/2018 23:53:25 [INFO] exp_shallowmodel: f1_score:   0.448
01/22/2018 23:53:25 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:53:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.13      0.20        23
          C       0.46      0.22      0.30        27
          F       0.79      0.96      0.87       250
          R       0.61      0.33      0.42        52

avg / total       0.71      0.76      0.71       352

01/22/2018 23:53:25 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:53:25 [INFO] exp_shallowmodel: 
[[  3   0  18   2]
 [  0   6  17   4]
 [  3   2 240   5]
 [  1   5  29  17]]
01/22/2018 23:53:31 [INFO] exp_shallowmodel: ******************** family - Round 1 
01/22/2018 23:53:31 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 23:53:31 [INFO] exp_shallowmodel: #(feature) = 49505
01/22/2018 23:53:31 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:53:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:53:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:53:31 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:53:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:54:46 [INFO] exp_shallowmodel: train time: 74.798s
01/22/2018 23:54:46 [INFO] exp_shallowmodel: test time:  0.025s
01/22/2018 23:54:46 [INFO] exp_shallowmodel: accuracy:   0.761
01/22/2018 23:54:46 [INFO] exp_shallowmodel: f1_score:   0.407
01/22/2018 23:54:46 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:54:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.50      0.22      0.31        27
          F       0.78      0.98      0.87       250
          R       0.64      0.35      0.45        52

avg / total       0.69      0.76      0.71       352

01/22/2018 23:54:46 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:54:46 [INFO] exp_shallowmodel: 
[[  0   1  19   3]
 [  0   6  18   3]
 [  0   2 244   4]
 [  0   3  31  18]]
01/22/2018 23:54:52 [INFO] exp_shallowmodel: ******************** family - Round 2 
01/22/2018 23:54:52 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 23:54:52 [INFO] exp_shallowmodel: #(feature) = 49505
01/22/2018 23:54:52 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:54:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:54:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:54:52 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:54:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:56:07 [INFO] exp_shallowmodel: train time: 75.224s
01/22/2018 23:56:07 [INFO] exp_shallowmodel: test time:  0.024s
01/22/2018 23:56:07 [INFO] exp_shallowmodel: accuracy:   0.741
01/22/2018 23:56:07 [INFO] exp_shallowmodel: f1_score:   0.417
01/22/2018 23:56:07 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:56:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.09      0.14        23
          C       0.38      0.22      0.28        27
          F       0.78      0.95      0.86       250
          R       0.60      0.29      0.39        52

avg / total       0.70      0.74      0.70       352

01/22/2018 23:56:07 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:56:07 [INFO] exp_shallowmodel: 
[[  2   1  18   2]
 [  0   6  18   3]
 [  1   6 238   5]
 [  2   3  32  15]]
01/22/2018 23:56:13 [INFO] exp_shallowmodel: ******************** family - Round 3 
01/22/2018 23:56:13 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 23:56:13 [INFO] exp_shallowmodel: #(feature) = 49505
01/22/2018 23:56:13 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:56:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:56:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:56:13 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:56:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:57:29 [INFO] exp_shallowmodel: train time: 75.639s
01/22/2018 23:57:29 [INFO] exp_shallowmodel: test time:  0.025s
01/22/2018 23:57:29 [INFO] exp_shallowmodel: accuracy:   0.747
01/22/2018 23:57:29 [INFO] exp_shallowmodel: f1_score:   0.390
01/22/2018 23:57:29 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:57:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.55      0.22      0.32        27
          F       0.78      0.97      0.86       250
          R       0.56      0.29      0.38        52

avg / total       0.68      0.75      0.69       352

01/22/2018 23:57:29 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:57:29 [INFO] exp_shallowmodel: 
[[  0   0  19   4]
 [  0   6  19   2]
 [  1   1 242   6]
 [  2   4  31  15]]
01/22/2018 23:57:35 [INFO] exp_shallowmodel: ******************** family - Round 4 
01/22/2018 23:57:35 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 23:57:35 [INFO] exp_shallowmodel: #(feature) = 49505
01/22/2018 23:57:35 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:57:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:57:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:57:35 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:57:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/22/2018 23:58:52 [INFO] exp_shallowmodel: train time: 76.646s
01/22/2018 23:58:52 [INFO] exp_shallowmodel: test time:  0.025s
01/22/2018 23:58:52 [INFO] exp_shallowmodel: accuracy:   0.744
01/22/2018 23:58:52 [INFO] exp_shallowmodel: f1_score:   0.419
01/22/2018 23:58:52 [INFO] exp_shallowmodel: classification report:
01/22/2018 23:58:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.09      0.13        23
          C       0.57      0.15      0.24        27
          F       0.77      0.95      0.85       250
          R       0.61      0.37      0.46        52

avg / total       0.70      0.74      0.70       352

01/22/2018 23:58:52 [INFO] exp_shallowmodel: confusion matrix:
01/22/2018 23:58:52 [INFO] exp_shallowmodel: 
[[  2   0  18   3]
 [  1   4  20   2]
 [  4   2 237   7]
 [  1   1  31  19]]
01/22/2018 23:58:58 [INFO] exp_shallowmodel: ******************** family - Round 5 
01/22/2018 23:58:58 [INFO] exp_shallowmodel: #(data) = 2826
01/22/2018 23:58:58 [INFO] exp_shallowmodel: #(feature) = 49505
01/22/2018 23:58:58 [INFO] exp_shallowmodel: ================================================================================
01/22/2018 23:58:58 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/22/2018 23:58:58 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/22/2018 23:58:58 [INFO] exp_shallowmodel: Training: 
01/22/2018 23:58:58 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:00:14 [INFO] exp_shallowmodel: train time: 76.457s
01/23/2018 00:00:14 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:00:14 [INFO] exp_shallowmodel: accuracy:   0.750
01/23/2018 00:00:14 [INFO] exp_shallowmodel: f1_score:   0.416
01/23/2018 00:00:14 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:00:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.75      0.33      0.46        27
          F       0.79      0.97      0.87       250
          R       0.50      0.25      0.33        52

avg / total       0.69      0.75      0.70       352

01/23/2018 00:00:14 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:00:14 [INFO] exp_shallowmodel: 
[[  0   0  21   2]
 [  1   9  11   6]
 [  2   1 242   5]
 [  4   2  33  13]]
01/23/2018 00:00:20 [INFO] exp_shallowmodel: ******************** family - Round 6 
01/23/2018 00:00:20 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:00:20 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:00:20 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:00:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:00:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:00:20 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:00:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:01:37 [INFO] exp_shallowmodel: train time: 76.392s
01/23/2018 00:01:37 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:01:37 [INFO] exp_shallowmodel: accuracy:   0.733
01/23/2018 00:01:37 [INFO] exp_shallowmodel: f1_score:   0.405
01/23/2018 00:01:37 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:01:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.62      0.37      0.47        27
          F       0.78      0.94      0.86       250
          R       0.43      0.23      0.30        52

avg / total       0.67      0.73      0.69       352

01/23/2018 00:01:37 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:01:37 [INFO] exp_shallowmodel: 
[[  0   1  18   4]
 [  1  10  12   4]
 [  2   4 236   8]
 [  3   1  36  12]]
01/23/2018 00:01:43 [INFO] exp_shallowmodel: ******************** family - Round 7 
01/23/2018 00:01:43 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:01:43 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:01:43 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:01:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:01:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:01:43 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:01:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:02:59 [INFO] exp_shallowmodel: train time: 75.828s
01/23/2018 00:02:59 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:02:59 [INFO] exp_shallowmodel: accuracy:   0.724
01/23/2018 00:02:59 [INFO] exp_shallowmodel: f1_score:   0.404
01/23/2018 00:02:59 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:02:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.80      0.17      0.29        23
          C       0.36      0.15      0.21        27
          F       0.77      0.94      0.85       250
          R       0.38      0.21      0.27        52

avg / total       0.68      0.72      0.68       352

01/23/2018 00:02:59 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:02:59 [INFO] exp_shallowmodel: 
[[  4   1  16   2]
 [  1   4  17   5]
 [  0   3 236  11]
 [  0   3  38  11]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 00:03:05 [INFO] exp_shallowmodel: ******************** family - Round 8 
01/23/2018 00:03:05 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:03:05 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:03:05 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:03:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:03:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:03:05 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:03:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:04:21 [INFO] exp_shallowmodel: train time: 76.464s
01/23/2018 00:04:21 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:04:21 [INFO] exp_shallowmodel: accuracy:   0.741
01/23/2018 00:04:21 [INFO] exp_shallowmodel: f1_score:   0.366
01/23/2018 00:04:21 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:04:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.50      0.22      0.31        27
          F       0.79      0.98      0.87       250
          R       0.44      0.21      0.29        52

avg / total       0.66      0.74      0.68       352

01/23/2018 00:04:21 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:04:21 [INFO] exp_shallowmodel: 
[[  0   0  21   2]
 [  0   6  14   7]
 [  1   0 244   5]
 [  4   6  31  11]]
01/23/2018 00:04:27 [INFO] exp_shallowmodel: ******************** family - Round 9 
01/23/2018 00:04:27 [INFO] exp_shallowmodel: #(data) = 2816
01/23/2018 00:04:27 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:04:27 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:04:27 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:04:27 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:04:27 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:04:27 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:05:45 [INFO] exp_shallowmodel: train time: 77.383s
01/23/2018 00:05:45 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:05:45 [INFO] exp_shallowmodel: accuracy:   0.715
01/23/2018 00:05:45 [INFO] exp_shallowmodel: f1_score:   0.347
01/23/2018 00:05:45 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:05:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        25
          C       0.44      0.15      0.22        27
          F       0.74      0.96      0.84       251
          R       0.52      0.24      0.33        59

avg / total       0.63      0.72      0.65       362

01/23/2018 00:05:45 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:05:45 [INFO] exp_shallowmodel: 
[[  0   1  22   2]
 [  1   4  18   4]
 [  1   2 241   7]
 [  0   2  43  14]]
01/23/2018 00:05:51 [INFO] exp_shallowmodel: ******************** family - Round 10 
01/23/2018 00:05:51 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:05:51 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:05:51 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:05:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:05:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:05:51 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:05:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:07:07 [INFO] exp_shallowmodel: train time: 76.374s
01/23/2018 00:07:07 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:07:07 [INFO] exp_shallowmodel: accuracy:   0.764
01/23/2018 00:07:07 [INFO] exp_shallowmodel: f1_score:   0.432
01/23/2018 00:07:07 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:07:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.67      0.30      0.41        27
          F       0.79      0.98      0.87       250
          R       0.68      0.33      0.44        52

avg / total       0.71      0.76      0.72       352

01/23/2018 00:07:07 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:07:07 [INFO] exp_shallowmodel: 
[[  0   1  21   1]
 [  1   8  14   4]
 [  3   0 244   3]
 [  3   3  29  17]]
01/23/2018 00:07:13 [INFO] exp_shallowmodel: ******************** family - Round 11 
01/23/2018 00:07:13 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:07:13 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:07:13 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:07:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:07:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:07:13 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:07:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:08:29 [INFO] exp_shallowmodel: train time: 75.688s
01/23/2018 00:08:29 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:08:29 [INFO] exp_shallowmodel: accuracy:   0.716
01/23/2018 00:08:29 [INFO] exp_shallowmodel: f1_score:   0.333
01/23/2018 00:08:29 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:08:29 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.36      0.15      0.21        27
          F       0.77      0.95      0.85       250
          R       0.38      0.21      0.27        52

avg / total       0.63      0.72      0.66       352

01/23/2018 00:08:29 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:08:29 [INFO] exp_shallowmodel: 
[[  0   0  20   3]
 [  0   4  17   6]
 [  2   2 237   9]
 [  1   5  35  11]]
01/23/2018 00:08:35 [INFO] exp_shallowmodel: ******************** family - Round 12 
01/23/2018 00:08:35 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:08:35 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:08:35 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:08:35 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:08:35 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:08:35 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:08:35 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:09:50 [INFO] exp_shallowmodel: train time: 75.158s
01/23/2018 00:09:50 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:09:50 [INFO] exp_shallowmodel: accuracy:   0.713
01/23/2018 00:09:50 [INFO] exp_shallowmodel: f1_score:   0.373
01/23/2018 00:09:50 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:09:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.14      0.04      0.07        23
          C       0.47      0.26      0.33        27
          F       0.77      0.93      0.85       250
          R       0.34      0.19      0.25        52

avg / total       0.65      0.71      0.67       352

01/23/2018 00:09:50 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:09:50 [INFO] exp_shallowmodel: 
[[  1   1  19   2]
 [  0   7  13   7]
 [  3   4 233  10]
 [  3   3  36  10]]
01/23/2018 00:09:56 [INFO] exp_shallowmodel: ******************** family - Round 13 
01/23/2018 00:09:56 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:09:56 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:09:56 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:09:56 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:09:56 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:09:56 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:09:56 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:11:12 [INFO] exp_shallowmodel: train time: 75.768s
01/23/2018 00:11:12 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:11:12 [INFO] exp_shallowmodel: accuracy:   0.744
01/23/2018 00:11:12 [INFO] exp_shallowmodel: f1_score:   0.415
01/23/2018 00:11:12 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:11:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.13      0.20        23
          C       0.46      0.22      0.30        27
          F       0.78      0.97      0.87       250
          R       0.48      0.21      0.29        52

avg / total       0.69      0.74      0.69       352

01/23/2018 00:11:12 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:11:12 [INFO] exp_shallowmodel: 
[[  3   1  17   2]
 [  0   6  15   6]
 [  2   2 242   4]
 [  2   4  35  11]]
01/23/2018 00:11:18 [INFO] exp_shallowmodel: ******************** family - Round 14 
01/23/2018 00:11:18 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:11:18 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:11:18 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:11:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:11:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:11:18 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:11:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:12:35 [INFO] exp_shallowmodel: train time: 76.494s
01/23/2018 00:12:35 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:12:35 [INFO] exp_shallowmodel: accuracy:   0.756
01/23/2018 00:12:35 [INFO] exp_shallowmodel: f1_score:   0.378
01/23/2018 00:12:35 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:12:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.43      0.22      0.29        27
          F       0.78      0.99      0.88       250
          R       0.67      0.23      0.34        52

avg / total       0.69      0.76      0.70       352

01/23/2018 00:12:35 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:12:35 [INFO] exp_shallowmodel: 
[[  0   2  18   3]
 [  1   6  17   3]
 [  1   1 248   0]
 [  2   5  33  12]]
01/23/2018 00:12:41 [INFO] exp_shallowmodel: ******************** family - Round 15 
01/23/2018 00:12:41 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:12:41 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:12:41 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:12:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:12:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:12:41 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:12:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:13:57 [INFO] exp_shallowmodel: train time: 76.006s
01/23/2018 00:13:57 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:13:57 [INFO] exp_shallowmodel: accuracy:   0.713
01/23/2018 00:13:57 [INFO] exp_shallowmodel: f1_score:   0.303
01/23/2018 00:13:57 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:13:57 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.18      0.07      0.11        27
          F       0.77      0.96      0.85       250
          R       0.38      0.19      0.26        52

avg / total       0.62      0.71      0.65       352

01/23/2018 00:13:57 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:13:57 [INFO] exp_shallowmodel: 
[[  0   0  18   5]
 [  0   2  20   5]
 [  2   3 239   6]
 [  2   6  34  10]]
01/23/2018 00:14:03 [INFO] exp_shallowmodel: ******************** family - Round 16 
01/23/2018 00:14:03 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:14:03 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:14:03 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:14:03 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:14:03 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:14:03 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:14:03 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:15:16 [INFO] exp_shallowmodel: train time: 73.189s
01/23/2018 00:15:16 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:15:16 [INFO] exp_shallowmodel: accuracy:   0.733
01/23/2018 00:15:16 [INFO] exp_shallowmodel: f1_score:   0.380
01/23/2018 00:15:16 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:15:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.55      0.22      0.32        27
          F       0.78      0.95      0.86       250
          R       0.50      0.27      0.35        52

avg / total       0.67      0.73      0.68       352

01/23/2018 00:15:16 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:15:16 [INFO] exp_shallowmodel: 
[[  0   2  15   6]
 [  1   6  19   1]
 [  3   2 238   7]
 [  3   1  34  14]]
01/23/2018 00:15:22 [INFO] exp_shallowmodel: ******************** family - Round 17 
01/23/2018 00:15:22 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:15:22 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:15:22 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:15:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:15:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:15:22 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:15:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:16:35 [INFO] exp_shallowmodel: train time: 73.201s
01/23/2018 00:16:35 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:16:35 [INFO] exp_shallowmodel: accuracy:   0.741
01/23/2018 00:16:35 [INFO] exp_shallowmodel: f1_score:   0.400
01/23/2018 00:16:35 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:16:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.54      0.26      0.35        27
          F       0.78      0.95      0.85       250
          R       0.55      0.31      0.40        52

avg / total       0.67      0.74      0.69       352

01/23/2018 00:16:35 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:16:35 [INFO] exp_shallowmodel: 
[[  0   0  22   1]
 [  2   7  13   5]
 [  1   4 238   7]
 [  0   2  34  16]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 00:16:41 [INFO] exp_shallowmodel: ******************** family - Round 18 
01/23/2018 00:16:41 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:16:41 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:16:41 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:16:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:16:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:16:41 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:16:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:17:55 [INFO] exp_shallowmodel: train time: 73.545s
01/23/2018 00:17:55 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:17:55 [INFO] exp_shallowmodel: accuracy:   0.736
01/23/2018 00:17:55 [INFO] exp_shallowmodel: f1_score:   0.405
01/23/2018 00:17:55 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:17:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.55      0.22      0.32        27
          F       0.79      0.94      0.86       250
          R       0.42      0.33      0.37        52

avg / total       0.70      0.74      0.69       352

01/23/2018 00:17:55 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:17:55 [INFO] exp_shallowmodel: 
[[  1   0  18   4]
 [  1   6  13   7]
 [  0   3 235  12]
 [  0   2  33  17]]
01/23/2018 00:18:01 [INFO] exp_shallowmodel: ******************** family - Round 19 
01/23/2018 00:18:01 [INFO] exp_shallowmodel: #(data) = 2816
01/23/2018 00:18:01 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:18:01 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:18:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:18:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:18:01 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:18:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:19:14 [INFO] exp_shallowmodel: train time: 73.860s
01/23/2018 00:19:14 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:19:14 [INFO] exp_shallowmodel: accuracy:   0.713
01/23/2018 00:19:14 [INFO] exp_shallowmodel: f1_score:   0.382
01/23/2018 00:19:14 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:19:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.12      0.19        25
          C       0.44      0.15      0.22        27
          F       0.75      0.95      0.84       251
          R       0.44      0.20      0.28        59

avg / total       0.65      0.71      0.66       362

01/23/2018 00:19:14 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:19:14 [INFO] exp_shallowmodel: 
[[  3   0  18   4]
 [  0   4  18   5]
 [  2   4 239   6]
 [  2   1  44  12]]
01/23/2018 00:19:20 [INFO] exp_shallowmodel: ******************** family - Round 20 
01/23/2018 00:19:20 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:19:20 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:19:20 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:19:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:19:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:19:20 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:19:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:20:35 [INFO] exp_shallowmodel: train time: 74.490s
01/23/2018 00:20:35 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:20:35 [INFO] exp_shallowmodel: accuracy:   0.747
01/23/2018 00:20:35 [INFO] exp_shallowmodel: f1_score:   0.387
01/23/2018 00:20:35 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:20:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.36      0.15      0.21        27
          F       0.78      0.96      0.86       250
          R       0.62      0.38      0.48        52

avg / total       0.67      0.75      0.70       352

01/23/2018 00:20:35 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:20:35 [INFO] exp_shallowmodel: 
[[  0   0  20   3]
 [  1   4  19   3]
 [  1   4 239   6]
 [  1   3  28  20]]
01/23/2018 00:20:41 [INFO] exp_shallowmodel: ******************** family - Round 21 
01/23/2018 00:20:41 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:20:41 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:20:41 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:20:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:20:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:20:41 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:20:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:21:56 [INFO] exp_shallowmodel: train time: 75.413s
01/23/2018 00:21:56 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:21:56 [INFO] exp_shallowmodel: accuracy:   0.750
01/23/2018 00:21:56 [INFO] exp_shallowmodel: f1_score:   0.411
01/23/2018 00:21:56 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:21:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.40      0.22      0.29        27
          F       0.77      0.96      0.86       250
          R       0.67      0.31      0.42        52

avg / total       0.71      0.75      0.70       352

01/23/2018 00:21:56 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:21:56 [INFO] exp_shallowmodel: 
[[  1   0  20   2]
 [  1   6  19   1]
 [  0   4 241   5]
 [  0   5  31  16]]
01/23/2018 00:22:02 [INFO] exp_shallowmodel: ******************** family - Round 22 
01/23/2018 00:22:02 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:22:02 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:22:02 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:22:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:22:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:22:02 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:22:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:23:16 [INFO] exp_shallowmodel: train time: 74.044s
01/23/2018 00:23:16 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:23:16 [INFO] exp_shallowmodel: accuracy:   0.747
01/23/2018 00:23:16 [INFO] exp_shallowmodel: f1_score:   0.408
01/23/2018 00:23:16 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:23:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.09      0.14        23
          C       0.36      0.19      0.24        27
          F       0.79      0.96      0.87       250
          R       0.58      0.29      0.38        52

avg / total       0.69      0.75      0.70       352

01/23/2018 00:23:16 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:23:16 [INFO] exp_shallowmodel: 
[[  2   0  19   2]
 [  1   5  16   5]
 [  1   4 241   4]
 [  2   5  30  15]]
01/23/2018 00:23:22 [INFO] exp_shallowmodel: ******************** family - Round 23 
01/23/2018 00:23:22 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:23:22 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:23:22 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:23:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:23:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:23:22 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:23:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:24:36 [INFO] exp_shallowmodel: train time: 73.816s
01/23/2018 00:24:36 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:24:36 [INFO] exp_shallowmodel: accuracy:   0.741
01/23/2018 00:24:36 [INFO] exp_shallowmodel: f1_score:   0.411
01/23/2018 00:24:36 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:24:36 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.64      0.33      0.44        27
          F       0.79      0.95      0.86       250
          R       0.48      0.27      0.35        52

avg / total       0.68      0.74      0.70       352

01/23/2018 00:24:36 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:24:36 [INFO] exp_shallowmodel: 
[[  0   0  20   3]
 [  0   9  13   5]
 [  1   4 238   7]
 [  5   1  32  14]]
01/23/2018 00:24:42 [INFO] exp_shallowmodel: ******************** family - Round 24 
01/23/2018 00:24:42 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:24:42 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:24:42 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:24:42 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:24:42 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:24:42 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:24:42 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:25:56 [INFO] exp_shallowmodel: train time: 73.995s
01/23/2018 00:25:56 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:25:56 [INFO] exp_shallowmodel: accuracy:   0.759
01/23/2018 00:25:56 [INFO] exp_shallowmodel: f1_score:   0.424
01/23/2018 00:25:56 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:25:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.04      0.08        23
          C       0.54      0.26      0.35        27
          F       0.79      0.97      0.87       250
          R       0.57      0.31      0.40        52

avg / total       0.71      0.76      0.71       352

01/23/2018 00:25:56 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:25:56 [INFO] exp_shallowmodel: 
[[  1   0  20   2]
 [  1   7  14   5]
 [  0   2 243   5]
 [  1   4  31  16]]
01/23/2018 00:26:02 [INFO] exp_shallowmodel: ******************** family - Round 25 
01/23/2018 00:26:02 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:26:02 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:26:02 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:26:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:26:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:26:02 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:26:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:27:15 [INFO] exp_shallowmodel: train time: 73.431s
01/23/2018 00:27:15 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:27:15 [INFO] exp_shallowmodel: accuracy:   0.736
01/23/2018 00:27:15 [INFO] exp_shallowmodel: f1_score:   0.360
01/23/2018 00:27:15 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:27:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.30      0.11      0.16        27
          F       0.77      0.97      0.86       250
          R       0.54      0.25      0.34        52

avg / total       0.68      0.74      0.68       352

01/23/2018 00:27:15 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:27:15 [INFO] exp_shallowmodel: 
[[  1   0  22   0]
 [  0   3  20   4]
 [  1   0 242   7]
 [  0   7  32  13]]
01/23/2018 00:27:21 [INFO] exp_shallowmodel: ******************** family - Round 26 
01/23/2018 00:27:21 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:27:21 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:27:21 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:27:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:27:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:27:21 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:27:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:28:33 [INFO] exp_shallowmodel: train time: 71.452s
01/23/2018 00:28:33 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:28:33 [INFO] exp_shallowmodel: accuracy:   0.730
01/23/2018 00:28:33 [INFO] exp_shallowmodel: f1_score:   0.362
01/23/2018 00:28:33 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:28:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.50      0.19      0.27        27
          F       0.77      0.96      0.86       250
          R       0.45      0.25      0.32        52

avg / total       0.65      0.73      0.68       352

01/23/2018 00:28:33 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:28:33 [INFO] exp_shallowmodel: 
[[  0   0  18   5]
 [  1   5  18   3]
 [  2   1 239   8]
 [  1   4  34  13]]
01/23/2018 00:28:39 [INFO] exp_shallowmodel: ******************** family - Round 27 
01/23/2018 00:28:39 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:28:39 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:28:39 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:28:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:28:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:28:39 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:28:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:29:52 [INFO] exp_shallowmodel: train time: 73.718s
01/23/2018 00:29:52 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:29:52 [INFO] exp_shallowmodel: accuracy:   0.739
01/23/2018 00:29:52 [INFO] exp_shallowmodel: f1_score:   0.386
01/23/2018 00:29:52 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:29:52 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.46      0.22      0.30        27
          F       0.78      0.95      0.85       250
          R       0.53      0.31      0.39        52

avg / total       0.66      0.74      0.69       352

01/23/2018 00:29:52 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:29:52 [INFO] exp_shallowmodel: 
[[  0   2  18   3]
 [  0   6  16   5]
 [  1   5 238   6]
 [  1   0  35  16]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 00:29:58 [INFO] exp_shallowmodel: ******************** family - Round 28 
01/23/2018 00:29:58 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:29:58 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:29:58 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:29:58 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:29:58 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:29:58 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:29:58 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:31:12 [INFO] exp_shallowmodel: train time: 73.664s
01/23/2018 00:31:12 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:31:12 [INFO] exp_shallowmodel: accuracy:   0.739
01/23/2018 00:31:12 [INFO] exp_shallowmodel: f1_score:   0.403
01/23/2018 00:31:12 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:31:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.04      0.08        23
          C       0.78      0.26      0.39        27
          F       0.78      0.96      0.86       250
          R       0.36      0.23      0.28        52

avg / total       0.70      0.74      0.69       352

01/23/2018 00:31:12 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:31:12 [INFO] exp_shallowmodel: 
[[  1   1  16   5]
 [  0   7  13   7]
 [  1   0 240   9]
 [  0   1  39  12]]
01/23/2018 00:31:18 [INFO] exp_shallowmodel: ******************** family - Round 29 
01/23/2018 00:31:18 [INFO] exp_shallowmodel: #(data) = 2816
01/23/2018 00:31:18 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:31:18 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:31:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:31:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:31:18 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:31:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:32:31 [INFO] exp_shallowmodel: train time: 73.082s
01/23/2018 00:32:31 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:32:31 [INFO] exp_shallowmodel: accuracy:   0.724
01/23/2018 00:32:31 [INFO] exp_shallowmodel: f1_score:   0.367
01/23/2018 00:32:31 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:32:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.08      0.13        25
          C       0.22      0.07      0.11        27
          F       0.76      0.96      0.85       251
          R       0.53      0.29      0.37        59

avg / total       0.66      0.72      0.67       362

01/23/2018 00:32:31 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:32:31 [INFO] exp_shallowmodel: 
[[  2   0  19   4]
 [  0   2  19   6]
 [  3   2 241   5]
 [  0   5  37  17]]
01/23/2018 00:32:37 [INFO] exp_shallowmodel: ******************** family - Round 30 
01/23/2018 00:32:37 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:32:37 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:32:37 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:32:37 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:32:37 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:32:37 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:32:37 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:33:51 [INFO] exp_shallowmodel: train time: 73.690s
01/23/2018 00:33:51 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:33:51 [INFO] exp_shallowmodel: accuracy:   0.744
01/23/2018 00:33:51 [INFO] exp_shallowmodel: f1_score:   0.429
01/23/2018 00:33:51 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:33:51 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.09      0.14        23
          C       0.64      0.26      0.37        27
          F       0.78      0.96      0.86       250
          R       0.50      0.27      0.35        52

avg / total       0.70      0.74      0.70       352

01/23/2018 00:33:51 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:33:51 [INFO] exp_shallowmodel: 
[[  2   0  18   3]
 [  0   7  17   3]
 [  2   1 239   8]
 [  2   3  33  14]]
01/23/2018 00:33:57 [INFO] exp_shallowmodel: ******************** family - Round 31 
01/23/2018 00:33:57 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:33:57 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:33:57 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:33:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:33:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:33:57 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:33:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:35:09 [INFO] exp_shallowmodel: train time: 72.822s
01/23/2018 00:35:09 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:35:09 [INFO] exp_shallowmodel: accuracy:   0.727
01/23/2018 00:35:09 [INFO] exp_shallowmodel: f1_score:   0.369
01/23/2018 00:35:09 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:35:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.42      0.19      0.26        27
          F       0.77      0.94      0.85       250
          R       0.47      0.31      0.37        52

avg / total       0.65      0.73      0.68       352

01/23/2018 00:35:09 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:35:09 [INFO] exp_shallowmodel: 
[[  0   1  20   2]
 [  0   5  18   4]
 [  1   2 235  12]
 [  1   4  31  16]]
01/23/2018 00:35:15 [INFO] exp_shallowmodel: ******************** family - Round 32 
01/23/2018 00:35:15 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:35:15 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:35:15 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:35:15 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:35:15 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:35:15 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:35:15 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:36:28 [INFO] exp_shallowmodel: train time: 72.223s
01/23/2018 00:36:28 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:36:28 [INFO] exp_shallowmodel: accuracy:   0.750
01/23/2018 00:36:28 [INFO] exp_shallowmodel: f1_score:   0.399
01/23/2018 00:36:28 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:36:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        23
          C       0.50      0.19      0.27        27
          F       0.78      0.97      0.86       250
          R       0.60      0.29      0.39        52

avg / total       0.69      0.75      0.70       352

01/23/2018 00:36:28 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:36:28 [INFO] exp_shallowmodel: 
[[  1   2  19   1]
 [  0   5  17   5]
 [  3   0 243   4]
 [  1   3  33  15]]
01/23/2018 00:36:34 [INFO] exp_shallowmodel: ******************** family - Round 33 
01/23/2018 00:36:34 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:36:34 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:36:34 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:36:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:36:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:36:34 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:36:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:37:46 [INFO] exp_shallowmodel: train time: 72.514s
01/23/2018 00:37:46 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:37:46 [INFO] exp_shallowmodel: accuracy:   0.716
01/23/2018 00:37:46 [INFO] exp_shallowmodel: f1_score:   0.347
01/23/2018 00:37:46 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:37:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.04      0.08        23
          C       0.38      0.19      0.25        27
          F       0.76      0.95      0.84       250
          R       0.38      0.15      0.22        52

avg / total       0.64      0.72      0.65       352

01/23/2018 00:37:46 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:37:46 [INFO] exp_shallowmodel: 
[[  1   2  19   1]
 [  0   5  17   5]
 [  0   5 238   7]
 [  2   1  41   8]]
01/23/2018 00:37:52 [INFO] exp_shallowmodel: ******************** family - Round 34 
01/23/2018 00:37:52 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:37:52 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:37:52 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:37:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:37:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:37:52 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:37:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:39:06 [INFO] exp_shallowmodel: train time: 73.719s
01/23/2018 00:39:06 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:39:06 [INFO] exp_shallowmodel: accuracy:   0.741
01/23/2018 00:39:06 [INFO] exp_shallowmodel: f1_score:   0.388
01/23/2018 00:39:06 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:39:06 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.41      0.26      0.32        27
          F       0.79      0.96      0.86       250
          R       0.52      0.29      0.37        52

avg / total       0.67      0.74      0.69       352

01/23/2018 00:39:06 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:39:06 [INFO] exp_shallowmodel: 
[[  0   0  17   6]
 [  2   7  16   2]
 [  0   5 239   6]
 [  0   5  32  15]]
01/23/2018 00:39:12 [INFO] exp_shallowmodel: ******************** family - Round 35 
01/23/2018 00:39:12 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:39:12 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:39:12 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:39:12 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:39:12 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:39:12 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:39:12 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:40:26 [INFO] exp_shallowmodel: train time: 74.195s
01/23/2018 00:40:26 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 00:40:26 [INFO] exp_shallowmodel: accuracy:   0.690
01/23/2018 00:40:26 [INFO] exp_shallowmodel: f1_score:   0.299
01/23/2018 00:40:26 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:40:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.15      0.07      0.10        27
          F       0.76      0.92      0.83       250
          R       0.35      0.21      0.27        52

avg / total       0.60      0.69      0.64       352

01/23/2018 00:40:26 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:40:26 [INFO] exp_shallowmodel: 
[[  0   1  20   2]
 [  2   2  16   7]
 [  2   7 230  11]
 [  0   3  38  11]]
01/23/2018 00:40:32 [INFO] exp_shallowmodel: ******************** family - Round 36 
01/23/2018 00:40:32 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:40:32 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:40:32 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:40:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:40:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:40:32 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:40:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:41:35 [INFO] exp_shallowmodel: train time: 63.601s
01/23/2018 00:41:36 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:41:36 [INFO] exp_shallowmodel: accuracy:   0.719
01/23/2018 00:41:36 [INFO] exp_shallowmodel: f1_score:   0.355
01/23/2018 00:41:36 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:41:36 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.07        23
          C       0.55      0.22      0.32        27
          F       0.78      0.96      0.86       250
          R       0.23      0.13      0.17        52

avg / total       0.65      0.72      0.66       352

01/23/2018 00:41:36 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:41:36 [INFO] exp_shallowmodel: 
[[  1   2  17   3]
 [  0   6  12   9]
 [  0   0 239  11]
 [  3   3  39   7]]
01/23/2018 00:41:41 [INFO] exp_shallowmodel: ******************** family - Round 37 
01/23/2018 00:41:41 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:41:41 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:41:41 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:41:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:41:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:41:41 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:41:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:42:28 [INFO] exp_shallowmodel: train time: 46.601s
01/23/2018 00:42:28 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:42:28 [INFO] exp_shallowmodel: accuracy:   0.747
01/23/2018 00:42:28 [INFO] exp_shallowmodel: f1_score:   0.400
01/23/2018 00:42:28 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:42:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       1.00      0.13      0.23        23
          C       0.33      0.11      0.17        27
          F       0.77      0.98      0.86       250
          R       0.54      0.25      0.34        52

avg / total       0.72      0.75      0.69       352

01/23/2018 00:42:28 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:42:28 [INFO] exp_shallowmodel: 
[[  3   1  17   2]
 [  0   3  19   5]
 [  0   2 244   4]
 [  0   3  36  13]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 00:42:34 [INFO] exp_shallowmodel: ******************** family - Round 38 
01/23/2018 00:42:34 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:42:34 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:42:34 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:42:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:42:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:42:34 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:42:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:43:20 [INFO] exp_shallowmodel: train time: 46.609s
01/23/2018 00:43:20 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:43:20 [INFO] exp_shallowmodel: accuracy:   0.753
01/23/2018 00:43:20 [INFO] exp_shallowmodel: f1_score:   0.421
01/23/2018 00:43:20 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:43:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.09      0.15        23
          C       0.54      0.26      0.35        27
          F       0.78      0.98      0.87       250
          R       0.50      0.23      0.32        52

avg / total       0.71      0.75      0.70       352

01/23/2018 00:43:20 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:43:20 [INFO] exp_shallowmodel: 
[[  2   0  18   3]
 [  0   7  14   6]
 [  0   3 244   3]
 [  2   3  35  12]]
01/23/2018 00:43:26 [INFO] exp_shallowmodel: ******************** family - Round 39 
01/23/2018 00:43:26 [INFO] exp_shallowmodel: #(data) = 2816
01/23/2018 00:43:26 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:43:26 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:43:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:43:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:43:26 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:43:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:44:15 [INFO] exp_shallowmodel: train time: 49.142s
01/23/2018 00:44:15 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:44:15 [INFO] exp_shallowmodel: accuracy:   0.732
01/23/2018 00:44:15 [INFO] exp_shallowmodel: f1_score:   0.407
01/23/2018 00:44:15 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:44:15 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.25      0.04      0.07        25
          C       0.45      0.19      0.26        27
          F       0.76      0.95      0.84       251
          R       0.62      0.36      0.45        59

avg / total       0.68      0.73      0.68       362

01/23/2018 00:44:15 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:44:15 [INFO] exp_shallowmodel: 
[[  1   0  23   1]
 [  1   5  17   4]
 [  1   4 238   8]
 [  1   2  35  21]]
01/23/2018 00:44:21 [INFO] exp_shallowmodel: ******************** family - Round 40 
01/23/2018 00:44:21 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:44:21 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:44:21 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:44:21 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:44:21 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:44:21 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:44:21 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:45:07 [INFO] exp_shallowmodel: train time: 46.423s
01/23/2018 00:45:07 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:45:07 [INFO] exp_shallowmodel: accuracy:   0.724
01/23/2018 00:45:07 [INFO] exp_shallowmodel: f1_score:   0.344
01/23/2018 00:45:07 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:45:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.50      0.22      0.31        27
          F       0.77      0.96      0.85       250
          R       0.36      0.15      0.22        52

avg / total       0.64      0.72      0.66       352

01/23/2018 00:45:07 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:45:07 [INFO] exp_shallowmodel: 
[[  0   0  19   4]
 [  1   6  16   4]
 [  1   2 241   6]
 [  1   4  39   8]]
01/23/2018 00:45:13 [INFO] exp_shallowmodel: ******************** family - Round 41 
01/23/2018 00:45:13 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:45:13 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:45:13 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:45:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:45:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:45:13 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:45:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:45:59 [INFO] exp_shallowmodel: train time: 46.280s
01/23/2018 00:45:59 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:45:59 [INFO] exp_shallowmodel: accuracy:   0.761
01/23/2018 00:45:59 [INFO] exp_shallowmodel: f1_score:   0.422
01/23/2018 00:45:59 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:45:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        23
          C       0.45      0.19      0.26        27
          F       0.79      0.97      0.87       250
          R       0.65      0.38      0.48        52

avg / total       0.71      0.76      0.72       352

01/23/2018 00:45:59 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:45:59 [INFO] exp_shallowmodel: 
[[  1   1  17   4]
 [  0   5  18   4]
 [  2   3 242   3]
 [  2   2  28  20]]
01/23/2018 00:46:05 [INFO] exp_shallowmodel: ******************** family - Round 42 
01/23/2018 00:46:05 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:46:05 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:46:05 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:46:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:46:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:46:05 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:46:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:46:46 [INFO] exp_shallowmodel: train time: 41.390s
01/23/2018 00:46:46 [INFO] exp_shallowmodel: test time:  0.022s
01/23/2018 00:46:46 [INFO] exp_shallowmodel: accuracy:   0.753
01/23/2018 00:46:46 [INFO] exp_shallowmodel: f1_score:   0.413
01/23/2018 00:46:46 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:46:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.04      0.08        23
          C       0.50      0.26      0.34        27
          F       0.80      0.97      0.87       250
          R       0.48      0.29      0.36        52

avg / total       0.70      0.75      0.71       352

01/23/2018 00:46:46 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:46:46 [INFO] exp_shallowmodel: 
[[  1   1  17   4]
 [  0   7  14   6]
 [  1   1 242   6]
 [  1   5  31  15]]
01/23/2018 00:46:52 [INFO] exp_shallowmodel: ******************** family - Round 43 
01/23/2018 00:46:52 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:46:52 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:46:52 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:46:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:46:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:46:52 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:46:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:47:33 [INFO] exp_shallowmodel: train time: 41.614s
01/23/2018 00:47:33 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:47:33 [INFO] exp_shallowmodel: accuracy:   0.741
01/23/2018 00:47:33 [INFO] exp_shallowmodel: f1_score:   0.402
01/23/2018 00:47:33 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:47:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.67      0.09      0.15        23
          C       0.45      0.19      0.26        27
          F       0.77      0.96      0.86       250
          R       0.50      0.25      0.33        52

avg / total       0.70      0.74      0.69       352

01/23/2018 00:47:33 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:47:33 [INFO] exp_shallowmodel: 
[[  2   1  17   3]
 [  1   5  17   4]
 [  0   3 241   6]
 [  0   2  37  13]]
01/23/2018 00:47:39 [INFO] exp_shallowmodel: ******************** family - Round 44 
01/23/2018 00:47:39 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:47:39 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:47:39 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:47:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:47:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:47:39 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:47:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:48:21 [INFO] exp_shallowmodel: train time: 42.091s
01/23/2018 00:48:21 [INFO] exp_shallowmodel: test time:  0.022s
01/23/2018 00:48:21 [INFO] exp_shallowmodel: accuracy:   0.744
01/23/2018 00:48:21 [INFO] exp_shallowmodel: f1_score:   0.407
01/23/2018 00:48:21 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:48:21 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.12      0.04      0.06        23
          C       0.46      0.22      0.30        27
          F       0.79      0.96      0.86       250
          R       0.57      0.31      0.40        52

avg / total       0.69      0.74      0.70       352

01/23/2018 00:48:21 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:48:21 [INFO] exp_shallowmodel: 
[[  1   0  19   3]
 [  0   6  15   6]
 [  5   3 239   3]
 [  2   4  30  16]]
01/23/2018 00:48:26 [INFO] exp_shallowmodel: ******************** family - Round 45 
01/23/2018 00:48:26 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:48:26 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:48:26 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:48:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:48:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:48:26 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:48:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:49:09 [INFO] exp_shallowmodel: train time: 42.280s
01/23/2018 00:49:09 [INFO] exp_shallowmodel: test time:  0.022s
01/23/2018 00:49:09 [INFO] exp_shallowmodel: accuracy:   0.736
01/23/2018 00:49:09 [INFO] exp_shallowmodel: f1_score:   0.405
01/23/2018 00:49:09 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:49:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.60      0.13      0.21        23
          C       0.38      0.19      0.25        27
          F       0.78      0.96      0.86       250
          R       0.41      0.23      0.30        52

avg / total       0.69      0.74      0.69       352

01/23/2018 00:49:09 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:49:09 [INFO] exp_shallowmodel: 
[[  3   1  17   2]
 [  0   5  13   9]
 [  2   3 239   6]
 [  0   4  36  12]]
01/23/2018 00:49:14 [INFO] exp_shallowmodel: ******************** family - Round 46 
01/23/2018 00:49:14 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:49:14 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:49:14 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:49:14 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:49:14 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:49:14 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:49:14 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:49:56 [INFO] exp_shallowmodel: train time: 41.732s
01/23/2018 00:49:56 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:49:56 [INFO] exp_shallowmodel: accuracy:   0.702
01/23/2018 00:49:56 [INFO] exp_shallowmodel: f1_score:   0.340
01/23/2018 00:49:56 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:49:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.00      0.00      0.00        23
          C       0.44      0.15      0.22        27
          F       0.75      0.92      0.83       250
          R       0.41      0.25      0.31        52

avg / total       0.63      0.70      0.65       352

01/23/2018 00:49:56 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:49:56 [INFO] exp_shallowmodel: 
[[  0   0  18   5]
 [  0   4  20   3]
 [  6   3 230  11]
 [  0   2  37  13]]
01/23/2018 00:50:02 [INFO] exp_shallowmodel: ******************** family - Round 47 
01/23/2018 00:50:02 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:50:02 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:50:02 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:50:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:50:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:50:02 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:50:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:50:43 [INFO] exp_shallowmodel: train time: 41.329s
01/23/2018 00:50:43 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:50:43 [INFO] exp_shallowmodel: accuracy:   0.730
01/23/2018 00:50:43 [INFO] exp_shallowmodel: f1_score:   0.372
01/23/2018 00:50:43 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:50:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.09      0.14        23
          C       0.27      0.11      0.16        27
          F       0.77      0.96      0.85       250
          R       0.54      0.25      0.34        52

avg / total       0.67      0.73      0.68       352

01/23/2018 00:50:43 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:50:43 [INFO] exp_shallowmodel: 
[[  2   0  20   1]
 [  1   3  20   3]
 [  1   3 239   7]
 [  2   5  32  13]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 00:50:48 [INFO] exp_shallowmodel: ******************** family - Round 48 
01/23/2018 00:50:48 [INFO] exp_shallowmodel: #(data) = 2826
01/23/2018 00:50:48 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:50:48 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:50:48 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:50:48 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:50:48 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:50:48 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:51:30 [INFO] exp_shallowmodel: train time: 41.717s
01/23/2018 00:51:30 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:51:30 [INFO] exp_shallowmodel: accuracy:   0.730
01/23/2018 00:51:30 [INFO] exp_shallowmodel: f1_score:   0.364
01/23/2018 00:51:30 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:51:30 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.17      0.04      0.07        23
          C       0.40      0.15      0.22        27
          F       0.77      0.96      0.85       250
          R       0.50      0.23      0.32        52

avg / total       0.66      0.73      0.67       352

01/23/2018 00:51:30 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:51:30 [INFO] exp_shallowmodel: 
[[  1   0  20   2]
 [  2   4  16   5]
 [  2   3 240   5]
 [  1   3  36  12]]
01/23/2018 00:51:36 [INFO] exp_shallowmodel: ******************** family - Round 49 
01/23/2018 00:51:36 [INFO] exp_shallowmodel: #(data) = 2816
01/23/2018 00:51:36 [INFO] exp_shallowmodel: #(feature) = 49505
01/23/2018 00:51:36 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:51:36 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:51:36 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:51:36 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:51:36 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:52:17 [INFO] exp_shallowmodel: train time: 41.155s
01/23/2018 00:52:17 [INFO] exp_shallowmodel: test time:  0.023s
01/23/2018 00:52:17 [INFO] exp_shallowmodel: accuracy:   0.724
01/23/2018 00:52:17 [INFO] exp_shallowmodel: f1_score:   0.393
01/23/2018 00:52:17 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:52:17 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.20      0.04      0.07        25
          C       0.67      0.22      0.33        27
          F       0.75      0.96      0.84       251
          R       0.54      0.24      0.33        59

avg / total       0.67      0.72      0.67       362

01/23/2018 00:52:17 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:52:17 [INFO] exp_shallowmodel: 
[[  1   0  22   2]
 [  1   6  16   4]
 [  2   2 241   6]
 [  1   1  43  14]]
01/23/2018 00:52:39 [INFO] exp_shallowmodel: ******************** ghome - Round 0 
01/23/2018 00:52:39 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:52:39 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:52:39 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:52:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:52:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:52:39 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:52:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:53:39 [INFO] exp_shallowmodel: train time: 59.834s
01/23/2018 00:53:39 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:53:39 [INFO] exp_shallowmodel: accuracy:   0.778
01/23/2018 00:53:39 [INFO] exp_shallowmodel: f1_score:   0.401
01/23/2018 00:53:39 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:53:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.36      0.17      0.23        59
          C       0.33      0.08      0.13        12
          F       0.81      0.96      0.88       396
          R       0.76      0.24      0.36        55

avg / total       0.74      0.78      0.73       522

01/23/2018 00:53:39 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:53:39 [INFO] exp_shallowmodel: 
[[ 10   0  49   0]
 [  2   1   8   1]
 [ 10   1 382   3]
 [  6   1  35  13]]
01/23/2018 00:53:45 [INFO] exp_shallowmodel: ******************** ghome - Round 1 
01/23/2018 00:53:45 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:53:45 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:53:45 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:53:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:53:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:53:45 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:53:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:54:47 [INFO] exp_shallowmodel: train time: 61.330s
01/23/2018 00:54:47 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:54:47 [INFO] exp_shallowmodel: accuracy:   0.776
01/23/2018 00:54:47 [INFO] exp_shallowmodel: f1_score:   0.431
01/23/2018 00:54:47 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:54:47 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.20      0.29        59
          C       0.67      0.17      0.27        12
          F       0.81      0.96      0.88       396
          R       0.43      0.22      0.29        55

avg / total       0.73      0.78      0.74       522

01/23/2018 00:54:47 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:54:47 [INFO] exp_shallowmodel: 
[[ 12   0  41   6]
 [  2   2   8   0]
 [  6   1 379  10]
 [  4   0  39  12]]
01/23/2018 00:54:53 [INFO] exp_shallowmodel: ******************** ghome - Round 2 
01/23/2018 00:54:53 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:54:53 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:54:53 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:54:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:54:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:54:53 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:54:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:55:54 [INFO] exp_shallowmodel: train time: 61.275s
01/23/2018 00:55:54 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:55:54 [INFO] exp_shallowmodel: accuracy:   0.774
01/23/2018 00:55:54 [INFO] exp_shallowmodel: f1_score:   0.383
01/23/2018 00:55:54 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:55:54 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.41      0.20      0.27        59
          C       0.00      0.00      0.00        12
          F       0.81      0.95      0.88       396
          R       0.57      0.29      0.39        55

avg / total       0.72      0.77      0.74       522

01/23/2018 00:55:54 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:55:54 [INFO] exp_shallowmodel: 
[[ 12   1  40   6]
 [  2   0  10   0]
 [ 14   0 376   6]
 [  1   1  37  16]]
01/23/2018 00:56:00 [INFO] exp_shallowmodel: ******************** ghome - Round 3 
01/23/2018 00:56:00 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:56:00 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:56:00 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:56:00 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:56:00 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:56:00 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:56:00 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:56:58 [INFO] exp_shallowmodel: train time: 57.863s
01/23/2018 00:56:58 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:56:58 [INFO] exp_shallowmodel: accuracy:   0.776
01/23/2018 00:56:58 [INFO] exp_shallowmodel: f1_score:   0.385
01/23/2018 00:56:58 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:56:58 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.45      0.24      0.31        59
          C       0.00      0.00      0.00        12
          F       0.82      0.95      0.88       396
          R       0.48      0.27      0.35        55

avg / total       0.73      0.78      0.74       522

01/23/2018 00:56:58 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:56:58 [INFO] exp_shallowmodel: 
[[ 14   0  40   5]
 [  0   0   8   4]
 [ 10   3 376   7]
 [  7   0  33  15]]
01/23/2018 00:57:05 [INFO] exp_shallowmodel: ******************** ghome - Round 4 
01/23/2018 00:57:05 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:57:05 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:57:05 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:57:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:57:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:57:05 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:57:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:58:04 [INFO] exp_shallowmodel: train time: 59.668s
01/23/2018 00:58:04 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:58:04 [INFO] exp_shallowmodel: accuracy:   0.785
01/23/2018 00:58:04 [INFO] exp_shallowmodel: f1_score:   0.449
01/23/2018 00:58:04 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:58:04 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.52      0.22      0.31        59
          C       0.50      0.17      0.25        12
          F       0.82      0.96      0.88       396
          R       0.50      0.27      0.35        55

avg / total       0.75      0.79      0.75       522

01/23/2018 00:58:04 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:58:04 [INFO] exp_shallowmodel: 
[[ 13   1  38   7]
 [  0   2   9   1]
 [  8   1 380   7]
 [  4   0  36  15]]
01/23/2018 00:58:11 [INFO] exp_shallowmodel: ******************** ghome - Round 5 
01/23/2018 00:58:11 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:58:11 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:58:11 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:58:11 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:58:11 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:58:11 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:58:11 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 00:59:12 [INFO] exp_shallowmodel: train time: 61.679s
01/23/2018 00:59:12 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 00:59:12 [INFO] exp_shallowmodel: accuracy:   0.757
01/23/2018 00:59:12 [INFO] exp_shallowmodel: f1_score:   0.344
01/23/2018 00:59:12 [INFO] exp_shallowmodel: classification report:
01/23/2018 00:59:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.42      0.17      0.24        59
          C       0.00      0.00      0.00        12
          F       0.79      0.94      0.86       396
          R       0.42      0.20      0.27        55

avg / total       0.69      0.76      0.71       522

01/23/2018 00:59:12 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 00:59:12 [INFO] exp_shallowmodel: 
[[ 10   0  46   3]
 [  1   0  10   1]
 [ 11   0 374  11]
 [  2   1  41  11]]
01/23/2018 00:59:19 [INFO] exp_shallowmodel: ******************** ghome - Round 6 
01/23/2018 00:59:19 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 00:59:19 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 00:59:19 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 00:59:19 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 00:59:19 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 00:59:19 [INFO] exp_shallowmodel: Training: 
01/23/2018 00:59:19 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:00:20 [INFO] exp_shallowmodel: train time: 61.462s
01/23/2018 01:00:20 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:00:20 [INFO] exp_shallowmodel: accuracy:   0.780
01/23/2018 01:00:20 [INFO] exp_shallowmodel: f1_score:   0.377
01/23/2018 01:00:20 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:00:20 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.62      0.22      0.33        59
          C       0.50      0.08      0.14        12
          F       0.81      0.98      0.89       396
          R       0.26      0.11      0.15        55

avg / total       0.73      0.78      0.73       522

01/23/2018 01:00:20 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:00:20 [INFO] exp_shallowmodel: 
[[ 13   1  35  10]
 [  1   1   8   2]
 [  4   0 387   5]
 [  3   0  46   6]]
01/23/2018 01:00:26 [INFO] exp_shallowmodel: ******************** ghome - Round 7 
01/23/2018 01:00:26 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:00:26 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:00:26 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:00:26 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:00:26 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:00:26 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:00:26 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:01:25 [INFO] exp_shallowmodel: train time: 58.610s
01/23/2018 01:01:25 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:01:25 [INFO] exp_shallowmodel: accuracy:   0.766
01/23/2018 01:01:25 [INFO] exp_shallowmodel: f1_score:   0.379
01/23/2018 01:01:25 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:01:25 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.31      0.14      0.19        59
          C       1.00      0.08      0.15        12
          F       0.81      0.96      0.88       396
          R       0.48      0.22      0.30        55

avg / total       0.72      0.77      0.72       522

01/23/2018 01:01:25 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:01:25 [INFO] exp_shallowmodel: 
[[  8   0  47   4]
 [  1   1   8   2]
 [ 10   0 379   7]
 [  7   0  36  12]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 01:01:31 [INFO] exp_shallowmodel: ******************** ghome - Round 8 
01/23/2018 01:01:31 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:01:31 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:01:31 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:01:31 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:01:31 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:01:31 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:01:31 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:02:32 [INFO] exp_shallowmodel: train time: 60.828s
01/23/2018 01:02:32 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:02:32 [INFO] exp_shallowmodel: accuracy:   0.762
01/23/2018 01:02:32 [INFO] exp_shallowmodel: f1_score:   0.349
01/23/2018 01:02:32 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:02:32 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.20        59
          C       0.50      0.08      0.14        12
          F       0.81      0.96      0.88       396
          R       0.27      0.13      0.17        55

avg / total       0.70      0.76      0.71       522

01/23/2018 01:02:32 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:02:32 [INFO] exp_shallowmodel: 
[[  8   0  44   7]
 [  0   1   8   3]
 [  5   0 382   9]
 [  7   1  40   7]]
01/23/2018 01:02:38 [INFO] exp_shallowmodel: ******************** ghome - Round 9 
01/23/2018 01:02:38 [INFO] exp_shallowmodel: #(data) = 4176
01/23/2018 01:02:38 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:02:38 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:02:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:02:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:02:38 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:02:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:03:39 [INFO] exp_shallowmodel: train time: 60.824s
01/23/2018 01:03:39 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:03:39 [INFO] exp_shallowmodel: accuracy:   0.766
01/23/2018 01:03:39 [INFO] exp_shallowmodel: f1_score:   0.353
01/23/2018 01:03:39 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:03:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.42      0.17      0.24        64
          C       0.00      0.00      0.00        14
          F       0.79      0.98      0.87       402
          R       0.63      0.19      0.29        63

avg / total       0.71      0.77      0.71       543

01/23/2018 01:03:39 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:03:39 [INFO] exp_shallowmodel: 
[[ 11   0  49   4]
 [  3   0  11   0]
 [  6   0 393   3]
 [  6   0  45  12]]
01/23/2018 01:03:45 [INFO] exp_shallowmodel: ******************** ghome - Round 10 
01/23/2018 01:03:45 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:03:45 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:03:45 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:03:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:03:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:03:45 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:03:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:04:43 [INFO] exp_shallowmodel: train time: 57.960s
01/23/2018 01:04:43 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:04:43 [INFO] exp_shallowmodel: accuracy:   0.766
01/23/2018 01:04:43 [INFO] exp_shallowmodel: f1_score:   0.329
01/23/2018 01:04:43 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:04:43 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.47      0.12      0.19        59
          C       0.00      0.00      0.00        12
          F       0.80      0.97      0.88       396
          R       0.38      0.18      0.25        55

avg / total       0.70      0.77      0.71       522

01/23/2018 01:04:43 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:04:43 [INFO] exp_shallowmodel: 
[[  7   0  44   8]
 [  2   0  10   0]
 [  2   3 383   8]
 [  4   2  39  10]]
01/23/2018 01:04:50 [INFO] exp_shallowmodel: ******************** ghome - Round 11 
01/23/2018 01:04:50 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:04:50 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:04:50 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:04:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:04:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:04:50 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:04:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:05:49 [INFO] exp_shallowmodel: train time: 59.460s
01/23/2018 01:05:49 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:05:49 [INFO] exp_shallowmodel: accuracy:   0.764
01/23/2018 01:05:49 [INFO] exp_shallowmodel: f1_score:   0.392
01/23/2018 01:05:49 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:05:49 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.35      0.12      0.18        59
          C       0.33      0.17      0.22        12
          F       0.80      0.95      0.87       396
          R       0.46      0.22      0.30        55

avg / total       0.71      0.76      0.72       522

01/23/2018 01:05:49 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:05:49 [INFO] exp_shallowmodel: 
[[  7   1  47   4]
 [  0   2   9   1]
 [  7   2 378   9]
 [  6   1  36  12]]
01/23/2018 01:05:55 [INFO] exp_shallowmodel: ******************** ghome - Round 12 
01/23/2018 01:05:55 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:05:55 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:05:55 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:05:55 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:05:55 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:05:55 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:05:55 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:06:56 [INFO] exp_shallowmodel: train time: 60.566s
01/23/2018 01:06:56 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:06:56 [INFO] exp_shallowmodel: accuracy:   0.761
01/23/2018 01:06:56 [INFO] exp_shallowmodel: f1_score:   0.353
01/23/2018 01:06:56 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:06:56 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.47      0.15      0.23        59
          C       0.00      0.00      0.00        12
          F       0.80      0.94      0.87       396
          R       0.41      0.25      0.31        55

avg / total       0.70      0.76      0.72       522

01/23/2018 01:06:56 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:06:56 [INFO] exp_shallowmodel: 
[[  9   1  46   3]
 [  0   0  10   2]
 [  6   1 374  15]
 [  4   0  37  14]]
01/23/2018 01:07:02 [INFO] exp_shallowmodel: ******************** ghome - Round 13 
01/23/2018 01:07:02 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:07:02 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:07:02 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:07:02 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:07:02 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:07:02 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:07:02 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:08:03 [INFO] exp_shallowmodel: train time: 61.273s
01/23/2018 01:08:03 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:08:03 [INFO] exp_shallowmodel: accuracy:   0.776
01/23/2018 01:08:03 [INFO] exp_shallowmodel: f1_score:   0.411
01/23/2018 01:08:03 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:08:03 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.20      0.29        59
          C       0.50      0.08      0.14        12
          F       0.81      0.95      0.88       396
          R       0.52      0.25      0.34        55

avg / total       0.73      0.78      0.74       522

01/23/2018 01:08:03 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:08:03 [INFO] exp_shallowmodel: 
[[ 12   1  42   4]
 [  1   1  10   0]
 [  9   0 378   9]
 [  3   0  38  14]]
01/23/2018 01:08:10 [INFO] exp_shallowmodel: ******************** ghome - Round 14 
01/23/2018 01:08:10 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:08:10 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:08:10 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:08:10 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:08:10 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:08:10 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:08:10 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:09:13 [INFO] exp_shallowmodel: train time: 63.356s
01/23/2018 01:09:13 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:09:13 [INFO] exp_shallowmodel: accuracy:   0.776
01/23/2018 01:09:13 [INFO] exp_shallowmodel: f1_score:   0.390
01/23/2018 01:09:13 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:09:13 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.45      0.22      0.30        59
          C       1.00      0.08      0.15        12
          F       0.82      0.96      0.88       396
          R       0.38      0.16      0.23        55

avg / total       0.73      0.78      0.73       522

01/23/2018 01:09:13 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:09:13 [INFO] exp_shallowmodel: 
[[ 13   0  36  10]
 [  2   1   8   1]
 [ 10   0 382   4]
 [  4   0  42   9]]
01/23/2018 01:09:19 [INFO] exp_shallowmodel: ******************** ghome - Round 15 
01/23/2018 01:09:19 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:09:19 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:09:19 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:09:19 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:09:19 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:09:19 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:09:19 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:10:22 [INFO] exp_shallowmodel: train time: 62.434s
01/23/2018 01:10:22 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:10:22 [INFO] exp_shallowmodel: accuracy:   0.782
01/23/2018 01:10:22 [INFO] exp_shallowmodel: f1_score:   0.400
01/23/2018 01:10:22 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:10:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.65      0.22      0.33        59
          C       1.00      0.08      0.15        12
          F       0.80      0.97      0.88       396
          R       0.43      0.16      0.24        55

avg / total       0.75      0.78      0.73       522

01/23/2018 01:10:22 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:10:22 [INFO] exp_shallowmodel: 
[[ 13   0  44   2]
 [  1   1   9   1]
 [  2   0 385   9]
 [  4   0  42   9]]
01/23/2018 01:10:28 [INFO] exp_shallowmodel: ******************** ghome - Round 16 
01/23/2018 01:10:28 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:10:28 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:10:28 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:10:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:10:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:10:28 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:10:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:11:27 [INFO] exp_shallowmodel: train time: 58.769s
01/23/2018 01:11:27 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:11:27 [INFO] exp_shallowmodel: accuracy:   0.770
01/23/2018 01:11:27 [INFO] exp_shallowmodel: f1_score:   0.338
01/23/2018 01:11:27 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:11:27 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.54      0.22      0.31        59
          C       0.00      0.00      0.00        12
          F       0.80      0.97      0.88       396
          R       0.32      0.11      0.16        55

avg / total       0.70      0.77      0.72       522

01/23/2018 01:11:27 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:11:27 [INFO] exp_shallowmodel: 
[[ 13   2  39   5]
 [  3   0   8   1]
 [  5   1 383   7]
 [  3   0  46   6]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 01:11:33 [INFO] exp_shallowmodel: ******************** ghome - Round 17 
01/23/2018 01:11:33 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:11:33 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:11:33 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:11:33 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:11:33 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:11:33 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:11:33 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:12:33 [INFO] exp_shallowmodel: train time: 60.073s
01/23/2018 01:12:33 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:12:33 [INFO] exp_shallowmodel: accuracy:   0.753
01/23/2018 01:12:33 [INFO] exp_shallowmodel: f1_score:   0.384
01/23/2018 01:12:33 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:12:33 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.33      0.14      0.19        59
          C       0.50      0.17      0.25        12
          F       0.79      0.94      0.86       396
          R       0.39      0.16      0.23        55

avg / total       0.69      0.75      0.71       522

01/23/2018 01:12:33 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:12:33 [INFO] exp_shallowmodel: 
[[  8   0  50   1]
 [  2   2   6   2]
 [  9   2 374  11]
 [  5   0  41   9]]
01/23/2018 01:12:39 [INFO] exp_shallowmodel: ******************** ghome - Round 18 
01/23/2018 01:12:39 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:12:39 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:12:39 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:12:39 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:12:39 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:12:39 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:12:39 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:13:39 [INFO] exp_shallowmodel: train time: 59.554s
01/23/2018 01:13:39 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:13:39 [INFO] exp_shallowmodel: accuracy:   0.757
01/23/2018 01:13:39 [INFO] exp_shallowmodel: f1_score:   0.355
01/23/2018 01:13:39 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:13:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.14      0.20        59
          C       1.00      0.08      0.15        12
          F       0.80      0.95      0.87       396
          R       0.29      0.15      0.19        55

avg / total       0.70      0.76      0.71       522

01/23/2018 01:13:39 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:13:39 [INFO] exp_shallowmodel: 
[[  8   0  43   8]
 [  0   1   9   2]
 [  8   0 378  10]
 [  4   0  43   8]]
01/23/2018 01:13:45 [INFO] exp_shallowmodel: ******************** ghome - Round 19 
01/23/2018 01:13:45 [INFO] exp_shallowmodel: #(data) = 4176
01/23/2018 01:13:45 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:13:45 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:13:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:13:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:13:45 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:13:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:14:44 [INFO] exp_shallowmodel: train time: 59.079s
01/23/2018 01:14:44 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:14:44 [INFO] exp_shallowmodel: accuracy:   0.768
01/23/2018 01:14:44 [INFO] exp_shallowmodel: f1_score:   0.441
01/23/2018 01:14:44 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:14:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.46      0.17      0.25        64
          C       0.38      0.21      0.27        14
          F       0.80      0.96      0.87       402
          R       0.59      0.27      0.37        63

avg / total       0.72      0.77      0.73       543

01/23/2018 01:14:44 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:14:44 [INFO] exp_shallowmodel: 
[[ 11   1  48   4]
 [  2   3   9   0]
 [  4   4 386   8]
 [  7   0  39  17]]
01/23/2018 01:14:50 [INFO] exp_shallowmodel: ******************** ghome - Round 20 
01/23/2018 01:14:50 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:14:50 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:14:50 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:14:50 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:14:50 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:14:50 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:14:50 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:15:50 [INFO] exp_shallowmodel: train time: 59.779s
01/23/2018 01:15:50 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:15:50 [INFO] exp_shallowmodel: accuracy:   0.778
01/23/2018 01:15:50 [INFO] exp_shallowmodel: f1_score:   0.408
01/23/2018 01:15:50 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:15:50 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.19      0.27        59
          C       1.00      0.08      0.15        12
          F       0.81      0.96      0.88       396
          R       0.47      0.25      0.33        55

avg / total       0.74      0.78      0.74       522

01/23/2018 01:15:50 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:15:50 [INFO] exp_shallowmodel: 
[[ 11   0  43   5]
 [  1   1   8   2]
 [  7   0 380   9]
 [  3   0  38  14]]
01/23/2018 01:15:57 [INFO] exp_shallowmodel: ******************** ghome - Round 21 
01/23/2018 01:15:57 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:15:57 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:15:57 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:15:57 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:15:57 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:15:57 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:15:57 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:16:55 [INFO] exp_shallowmodel: train time: 58.138s
01/23/2018 01:16:55 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:16:55 [INFO] exp_shallowmodel: accuracy:   0.764
01/23/2018 01:16:55 [INFO] exp_shallowmodel: f1_score:   0.346
01/23/2018 01:16:55 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:16:55 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.52      0.20      0.29        59
          C       0.00      0.00      0.00        12
          F       0.81      0.95      0.88       396
          R       0.32      0.16      0.22        55

avg / total       0.71      0.76      0.72       522

01/23/2018 01:16:55 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:16:55 [INFO] exp_shallowmodel: 
[[ 12   1  41   5]
 [  0   0   7   5]
 [  7   2 378   9]
 [  4   1  41   9]]
01/23/2018 01:17:01 [INFO] exp_shallowmodel: ******************** ghome - Round 22 
01/23/2018 01:17:01 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:17:01 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:17:01 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:17:01 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:17:01 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:17:01 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:17:01 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:18:01 [INFO] exp_shallowmodel: train time: 60.376s
01/23/2018 01:18:01 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:18:01 [INFO] exp_shallowmodel: accuracy:   0.770
01/23/2018 01:18:01 [INFO] exp_shallowmodel: f1_score:   0.387
01/23/2018 01:18:01 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:18:01 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.15      0.22        59
          C       0.50      0.08      0.14        12
          F       0.80      0.96      0.87       396
          R       0.55      0.22      0.31        55

avg / total       0.72      0.77      0.72       522

01/23/2018 01:18:01 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:18:01 [INFO] exp_shallowmodel: 
[[  9   0  48   2]
 [  2   1   9   0]
 [  7   1 380   8]
 [  5   0  38  12]]
01/23/2018 01:18:08 [INFO] exp_shallowmodel: ******************** ghome - Round 23 
01/23/2018 01:18:08 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:18:08 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:18:08 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:18:08 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:18:08 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:18:08 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:18:08 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:19:09 [INFO] exp_shallowmodel: train time: 61.800s
01/23/2018 01:19:09 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:19:09 [INFO] exp_shallowmodel: accuracy:   0.770
01/23/2018 01:19:09 [INFO] exp_shallowmodel: f1_score:   0.351
01/23/2018 01:19:09 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:19:09 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.10      0.15        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.52      0.29      0.37        55

avg / total       0.70      0.77      0.72       522

01/23/2018 01:19:09 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:19:09 [INFO] exp_shallowmodel: 
[[  6   1  44   8]
 [  3   0   9   0]
 [  8   1 380   7]
 [  3   0  36  16]]
01/23/2018 01:19:16 [INFO] exp_shallowmodel: ******************** ghome - Round 24 
01/23/2018 01:19:16 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:19:16 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:19:16 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:19:16 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:19:16 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:19:16 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:19:16 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:20:16 [INFO] exp_shallowmodel: train time: 60.062s
01/23/2018 01:20:16 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:20:16 [INFO] exp_shallowmodel: accuracy:   0.770
01/23/2018 01:20:16 [INFO] exp_shallowmodel: f1_score:   0.330
01/23/2018 01:20:16 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:20:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.36      0.15      0.21        59
          C       0.00      0.00      0.00        12
          F       0.80      0.97      0.88       396
          R       0.53      0.15      0.23        55

avg / total       0.70      0.77      0.71       522

01/23/2018 01:20:16 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:20:16 [INFO] exp_shallowmodel: 
[[  9   0  47   3]
 [  3   0   8   1]
 [  8   0 385   3]
 [  5   0  42   8]]
01/23/2018 01:20:22 [INFO] exp_shallowmodel: ******************** ghome - Round 25 
01/23/2018 01:20:22 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:20:22 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:20:22 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:20:22 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:20:22 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:20:22 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:20:22 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:21:23 [INFO] exp_shallowmodel: train time: 60.846s
01/23/2018 01:21:23 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:21:23 [INFO] exp_shallowmodel: accuracy:   0.768
01/23/2018 01:21:23 [INFO] exp_shallowmodel: f1_score:   0.352
01/23/2018 01:21:23 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:21:23 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.30      0.12      0.17        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.54      0.27      0.36        55

avg / total       0.70      0.77      0.72       522

01/23/2018 01:21:23 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:21:23 [INFO] exp_shallowmodel: 
[[  7   0  44   8]
 [  1   0  11   0]
 [ 11   1 379   5]
 [  4   0  36  15]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 01:21:29 [INFO] exp_shallowmodel: ******************** ghome - Round 26 
01/23/2018 01:21:29 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:21:29 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:21:29 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:21:29 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:21:29 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:21:29 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:21:29 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:22:28 [INFO] exp_shallowmodel: train time: 58.866s
01/23/2018 01:22:28 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:22:28 [INFO] exp_shallowmodel: accuracy:   0.762
01/23/2018 01:22:28 [INFO] exp_shallowmodel: f1_score:   0.369
01/23/2018 01:22:28 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:22:28 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.17      0.24        59
          C       0.50      0.08      0.14        12
          F       0.81      0.95      0.87       396
          R       0.36      0.16      0.22        55

avg / total       0.70      0.76      0.72       522

01/23/2018 01:22:28 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:22:28 [INFO] exp_shallowmodel: 
[[ 10   0  45   4]
 [  1   1   9   1]
 [  6   1 378  11]
 [  9   0  37   9]]
01/23/2018 01:22:34 [INFO] exp_shallowmodel: ******************** ghome - Round 27 
01/23/2018 01:22:34 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:22:34 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:22:34 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:22:34 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:22:34 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:22:34 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:22:34 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:23:34 [INFO] exp_shallowmodel: train time: 59.800s
01/23/2018 01:23:34 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:23:34 [INFO] exp_shallowmodel: accuracy:   0.772
01/23/2018 01:23:34 [INFO] exp_shallowmodel: f1_score:   0.354
01/23/2018 01:23:34 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:23:34 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.20      0.29        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.42      0.18      0.25        55

avg / total       0.71      0.77      0.73       522

01/23/2018 01:23:34 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:23:34 [INFO] exp_shallowmodel: 
[[ 12   1  42   4]
 [  0   0  10   2]
 [  7   0 381   8]
 [  6   1  38  10]]
01/23/2018 01:23:40 [INFO] exp_shallowmodel: ******************** ghome - Round 28 
01/23/2018 01:23:40 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:23:40 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:23:40 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:23:40 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:23:40 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:23:40 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:23:40 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:24:37 [INFO] exp_shallowmodel: train time: 56.326s
01/23/2018 01:24:37 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:24:37 [INFO] exp_shallowmodel: accuracy:   0.776
01/23/2018 01:24:37 [INFO] exp_shallowmodel: f1_score:   0.346
01/23/2018 01:24:37 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:24:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.50      0.17      0.25        59
          C       0.00      0.00      0.00        12
          F       0.81      0.97      0.88       396
          R       0.38      0.18      0.25        55

avg / total       0.71      0.78      0.73       522

01/23/2018 01:24:37 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:24:37 [INFO] exp_shallowmodel: 
[[ 10   0  43   6]
 [  0   0  10   2]
 [  2   1 385   8]
 [  8   0  37  10]]
01/23/2018 01:24:43 [INFO] exp_shallowmodel: ******************** ghome - Round 29 
01/23/2018 01:24:43 [INFO] exp_shallowmodel: #(data) = 4176
01/23/2018 01:24:43 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:24:43 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:24:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:24:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:24:43 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:24:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:25:39 [INFO] exp_shallowmodel: train time: 56.083s
01/23/2018 01:25:39 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:25:39 [INFO] exp_shallowmodel: accuracy:   0.757
01/23/2018 01:25:39 [INFO] exp_shallowmodel: f1_score:   0.388
01/23/2018 01:25:39 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:25:39 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.29      0.16      0.20        64
          C       0.25      0.07      0.11        14
          F       0.81      0.95      0.87       402
          R       0.57      0.27      0.37        63

avg / total       0.70      0.76      0.72       543

01/23/2018 01:25:39 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:25:39 [INFO] exp_shallowmodel: 
[[ 10   2  48   4]
 [  5   1   5   3]
 [ 12   1 383   6]
 [  8   0  38  17]]
01/23/2018 01:25:45 [INFO] exp_shallowmodel: ******************** ghome - Round 30 
01/23/2018 01:25:45 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:25:45 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:25:45 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:25:45 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:25:45 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:25:45 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:25:45 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:26:45 [INFO] exp_shallowmodel: train time: 60.047s
01/23/2018 01:26:45 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:26:45 [INFO] exp_shallowmodel: accuracy:   0.753
01/23/2018 01:26:45 [INFO] exp_shallowmodel: f1_score:   0.325
01/23/2018 01:26:45 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:26:45 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.41      0.19      0.26        59
          C       0.00      0.00      0.00        12
          F       0.80      0.95      0.87       396
          R       0.29      0.13      0.18        55

avg / total       0.68      0.75      0.70       522

01/23/2018 01:26:45 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:26:45 [INFO] exp_shallowmodel: 
[[ 11   0  43   5]
 [  2   0  10   0]
 [  8   1 375  12]
 [  6   0  42   7]]
01/23/2018 01:26:52 [INFO] exp_shallowmodel: ******************** ghome - Round 31 
01/23/2018 01:26:52 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:26:52 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:26:52 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:26:52 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:26:52 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:26:52 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:26:52 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:27:53 [INFO] exp_shallowmodel: train time: 61.284s
01/23/2018 01:27:53 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:27:53 [INFO] exp_shallowmodel: accuracy:   0.768
01/23/2018 01:27:53 [INFO] exp_shallowmodel: f1_score:   0.343
01/23/2018 01:27:53 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:27:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.17      0.25        59
          C       0.00      0.00      0.00        12
          F       0.79      0.96      0.87       396
          R       0.53      0.16      0.25        55

avg / total       0.71      0.77      0.72       522

01/23/2018 01:27:53 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:27:53 [INFO] exp_shallowmodel: 
[[ 10   1  44   4]
 [  0   0  12   0]
 [  8   2 382   4]
 [  3   0  43   9]]
01/23/2018 01:27:59 [INFO] exp_shallowmodel: ******************** ghome - Round 32 
01/23/2018 01:27:59 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:27:59 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:27:59 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:27:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:27:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:27:59 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:27:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:28:59 [INFO] exp_shallowmodel: train time: 59.937s
01/23/2018 01:28:59 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:28:59 [INFO] exp_shallowmodel: accuracy:   0.774
01/23/2018 01:28:59 [INFO] exp_shallowmodel: f1_score:   0.397
01/23/2018 01:28:59 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:28:59 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.55      0.19      0.28        59
          C       1.00      0.08      0.15        12
          F       0.80      0.96      0.87       396
          R       0.48      0.20      0.28        55

avg / total       0.74      0.77      0.73       522

01/23/2018 01:28:59 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:28:59 [INFO] exp_shallowmodel: 
[[ 11   0  46   2]
 [  0   1  10   1]
 [  6   0 381   9]
 [  3   0  41  11]]
01/23/2018 01:29:05 [INFO] exp_shallowmodel: ******************** ghome - Round 33 
01/23/2018 01:29:05 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:29:05 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:29:05 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:29:05 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:29:05 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:29:05 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:29:05 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:30:07 [INFO] exp_shallowmodel: train time: 61.400s
01/23/2018 01:30:07 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:30:07 [INFO] exp_shallowmodel: accuracy:   0.778
01/23/2018 01:30:07 [INFO] exp_shallowmodel: f1_score:   0.390
01/23/2018 01:30:07 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:30:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.15      0.23        59
          C       0.50      0.08      0.14        12
          F       0.82      0.97      0.89       396
          R       0.43      0.24      0.31        55

avg / total       0.73      0.78      0.73       522

01/23/2018 01:30:07 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:30:07 [INFO] exp_shallowmodel: 
[[  9   0  43   7]
 [  1   1   8   2]
 [  4   1 383   8]
 [  7   0  35  13]]
01/23/2018 01:30:13 [INFO] exp_shallowmodel: ******************** ghome - Round 34 
01/23/2018 01:30:13 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:30:13 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:30:13 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:30:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:30:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:30:13 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:30:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:31:14 [INFO] exp_shallowmodel: train time: 60.670s
01/23/2018 01:31:14 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:31:14 [INFO] exp_shallowmodel: accuracy:   0.747
01/23/2018 01:31:14 [INFO] exp_shallowmodel: f1_score:   0.320
01/23/2018 01:31:14 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:31:14 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.32      0.12      0.17        59
          C       0.33      0.08      0.13        12
          F       0.79      0.95      0.87       396
          R       0.20      0.07      0.11        55

avg / total       0.67      0.75      0.69       522

01/23/2018 01:31:14 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:31:14 [INFO] exp_shallowmodel: 
[[  7   0  47   5]
 [  1   1   9   1]
 [  7   1 378  10]
 [  7   1  43   4]]
01/23/2018 01:31:20 [INFO] exp_shallowmodel: ******************** ghome - Round 35 
01/23/2018 01:31:20 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:31:20 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:31:20 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:31:20 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:31:20 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:31:20 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:31:20 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:32:21 [INFO] exp_shallowmodel: train time: 61.590s
01/23/2018 01:32:22 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:32:22 [INFO] exp_shallowmodel: accuracy:   0.785
01/23/2018 01:32:22 [INFO] exp_shallowmodel: f1_score:   0.391
01/23/2018 01:32:22 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:32:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.46      0.19      0.27        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.63      0.31      0.41        55

avg / total       0.74      0.79      0.74       522

01/23/2018 01:32:22 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:32:22 [INFO] exp_shallowmodel: 
[[ 11   0  45   3]
 [  1   0   9   2]
 [  8   1 382   5]
 [  4   1  33  17]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 01:32:28 [INFO] exp_shallowmodel: ******************** ghome - Round 36 
01/23/2018 01:32:28 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:32:28 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:32:28 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:32:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:32:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:32:28 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:32:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:33:26 [INFO] exp_shallowmodel: train time: 58.286s
01/23/2018 01:33:26 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:33:26 [INFO] exp_shallowmodel: accuracy:   0.770
01/23/2018 01:33:26 [INFO] exp_shallowmodel: f1_score:   0.468
01/23/2018 01:33:26 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:33:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.38      0.15      0.22        59
          C       1.00      0.33      0.50        12
          F       0.82      0.95      0.88       396
          R       0.38      0.22      0.28        55

avg / total       0.72      0.77      0.73       522

01/23/2018 01:33:26 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:33:26 [INFO] exp_shallowmodel: 
[[  9   0  43   7]
 [  0   4   6   2]
 [  8   0 377  11]
 [  7   0  36  12]]
01/23/2018 01:33:32 [INFO] exp_shallowmodel: ******************** ghome - Round 37 
01/23/2018 01:33:32 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:33:32 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:33:32 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:33:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:33:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:33:32 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:33:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:34:31 [INFO] exp_shallowmodel: train time: 59.101s
01/23/2018 01:34:31 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:34:31 [INFO] exp_shallowmodel: accuracy:   0.753
01/23/2018 01:34:31 [INFO] exp_shallowmodel: f1_score:   0.364
01/23/2018 01:34:31 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:34:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.43      0.15      0.23        59
          C       1.00      0.08      0.15        12
          F       0.80      0.94      0.86       396
          R       0.30      0.16      0.21        55

avg / total       0.71      0.75      0.71       522

01/23/2018 01:34:31 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:34:31 [INFO] exp_shallowmodel: 
[[  9   0  43   7]
 [  1   1   9   1]
 [  9   0 374  13]
 [  2   0  44   9]]
01/23/2018 01:34:38 [INFO] exp_shallowmodel: ******************** ghome - Round 38 
01/23/2018 01:34:38 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:34:38 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:34:38 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:34:38 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:34:38 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:34:38 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:34:38 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:35:35 [INFO] exp_shallowmodel: train time: 57.275s
01/23/2018 01:35:35 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:35:35 [INFO] exp_shallowmodel: accuracy:   0.770
01/23/2018 01:35:35 [INFO] exp_shallowmodel: f1_score:   0.347
01/23/2018 01:35:35 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:35:35 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.60      0.20      0.30        59
          C       0.00      0.00      0.00        12
          F       0.80      0.96      0.87       396
          R       0.40      0.15      0.21        55

avg / total       0.71      0.77      0.72       522

01/23/2018 01:35:35 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:35:35 [INFO] exp_shallowmodel: 
[[ 12   0  43   4]
 [  2   0  10   0]
 [  4   2 382   8]
 [  2   0  45   8]]
01/23/2018 01:35:41 [INFO] exp_shallowmodel: ******************** ghome - Round 39 
01/23/2018 01:35:41 [INFO] exp_shallowmodel: #(data) = 4176
01/23/2018 01:35:41 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:35:41 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:35:41 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:35:41 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:35:41 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:35:41 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:36:37 [INFO] exp_shallowmodel: train time: 55.828s
01/23/2018 01:36:37 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:36:37 [INFO] exp_shallowmodel: accuracy:   0.753
01/23/2018 01:36:37 [INFO] exp_shallowmodel: f1_score:   0.362
01/23/2018 01:36:37 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:36:37 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.14      0.21        64
          C       1.00      0.07      0.13        14
          F       0.79      0.97      0.87       402
          R       0.38      0.17      0.24        63

avg / total       0.70      0.75      0.70       543

01/23/2018 01:36:37 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:36:37 [INFO] exp_shallowmodel: 
[[  9   0  46   9]
 [  3   1   9   1]
 [  6   0 388   8]
 [  5   0  47  11]]
01/23/2018 01:36:43 [INFO] exp_shallowmodel: ******************** ghome - Round 40 
01/23/2018 01:36:43 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:36:43 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:36:43 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:36:43 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:36:43 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:36:43 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:36:43 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:37:44 [INFO] exp_shallowmodel: train time: 60.913s
01/23/2018 01:37:44 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:37:44 [INFO] exp_shallowmodel: accuracy:   0.774
01/23/2018 01:37:44 [INFO] exp_shallowmodel: f1_score:   0.435
01/23/2018 01:37:44 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:37:44 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.52      0.22      0.31        59
          C       1.00      0.17      0.29        12
          F       0.81      0.95      0.88       396
          R       0.41      0.20      0.27        55

avg / total       0.74      0.77      0.73       522

01/23/2018 01:37:44 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:37:44 [INFO] exp_shallowmodel: 
[[ 13   0  42   4]
 [  1   2   8   1]
 [  7   0 378  11]
 [  4   0  40  11]]
01/23/2018 01:37:51 [INFO] exp_shallowmodel: ******************** ghome - Round 41 
01/23/2018 01:37:51 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:37:51 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:37:51 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:37:51 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:37:51 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:37:51 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:37:51 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:38:46 [INFO] exp_shallowmodel: train time: 55.750s
01/23/2018 01:38:46 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:38:46 [INFO] exp_shallowmodel: accuracy:   0.776
01/23/2018 01:38:46 [INFO] exp_shallowmodel: f1_score:   0.395
01/23/2018 01:38:46 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:38:46 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.47      0.15      0.23        59
          C       1.00      0.08      0.15        12
          F       0.80      0.96      0.88       396
          R       0.50      0.24      0.32        55

avg / total       0.74      0.78      0.73       522

01/23/2018 01:38:46 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:38:46 [INFO] exp_shallowmodel: 
[[  9   0  46   4]
 [  0   1   9   2]
 [  7   0 382   7]
 [  3   0  39  13]]
01/23/2018 01:38:53 [INFO] exp_shallowmodel: ******************** ghome - Round 42 
01/23/2018 01:38:53 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:38:53 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:38:53 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:38:53 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:38:53 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:38:53 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:38:53 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:39:53 [INFO] exp_shallowmodel: train time: 60.157s
01/23/2018 01:39:53 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:39:53 [INFO] exp_shallowmodel: accuracy:   0.780
01/23/2018 01:39:53 [INFO] exp_shallowmodel: f1_score:   0.411
01/23/2018 01:39:53 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:39:53 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.17      0.25        59
          C       0.33      0.08      0.13        12
          F       0.82      0.96      0.88       396
          R       0.49      0.31      0.38        55

avg / total       0.73      0.78      0.74       522

01/23/2018 01:39:53 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:39:53 [INFO] exp_shallowmodel: 
[[ 10   1  40   8]
 [  2   1   8   1]
 [  7   1 379   9]
 [  2   0  36  17]]
01/23/2018 01:39:59 [INFO] exp_shallowmodel: ******************** ghome - Round 43 
01/23/2018 01:39:59 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:39:59 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:39:59 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:39:59 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:39:59 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:39:59 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:39:59 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:41:02 [INFO] exp_shallowmodel: train time: 62.702s
01/23/2018 01:41:02 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:41:02 [INFO] exp_shallowmodel: accuracy:   0.774
01/23/2018 01:41:02 [INFO] exp_shallowmodel: f1_score:   0.351
01/23/2018 01:41:02 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:41:02 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.48      0.19      0.27        59
          C       0.00      0.00      0.00        12
          F       0.81      0.97      0.88       396
          R       0.42      0.18      0.25        55

avg / total       0.71      0.77      0.73       522

01/23/2018 01:41:02 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:41:02 [INFO] exp_shallowmodel: 
[[ 11   1  43   4]
 [  2   0   8   2]
 [  4   1 383   8]
 [  6   0  39  10]]
01/23/2018 01:41:08 [INFO] exp_shallowmodel: ******************** ghome - Round 44 
01/23/2018 01:41:08 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:41:08 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:41:08 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:41:08 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:41:08 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:41:08 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:41:08 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:42:07 [INFO] exp_shallowmodel: train time: 58.536s
01/23/2018 01:42:07 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:42:07 [INFO] exp_shallowmodel: accuracy:   0.768
01/23/2018 01:42:07 [INFO] exp_shallowmodel: f1_score:   0.350
01/23/2018 01:42:07 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:42:07 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.53      0.17      0.26        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.39      0.20      0.27        55

avg / total       0.71      0.77      0.72       522

01/23/2018 01:42:07 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:42:07 [INFO] exp_shallowmodel: 
[[ 10   0  42   7]
 [  1   0  10   1]
 [  3   4 380   9]
 [  5   0  39  11]]
01/23/2018 01:42:13 [INFO] exp_shallowmodel: ******************** ghome - Round 45 
01/23/2018 01:42:13 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:42:13 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:42:13 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:42:13 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:42:13 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:42:13 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:42:13 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:43:12 [INFO] exp_shallowmodel: train time: 58.770s
01/23/2018 01:43:12 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:43:12 [INFO] exp_shallowmodel: accuracy:   0.757
01/23/2018 01:43:12 [INFO] exp_shallowmodel: f1_score:   0.336
01/23/2018 01:43:12 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:43:12 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.27      0.10      0.15        59
          C       0.50      0.08      0.14        12
          F       0.81      0.96      0.88       396
          R       0.28      0.13      0.17        55

avg / total       0.68      0.76      0.70       522

01/23/2018 01:43:12 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:43:12 [INFO] exp_shallowmodel: 
[[  6   0  45   8]
 [  3   1   7   1]
 [  5   1 381   9]
 [  8   0  40   7]]
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
01/23/2018 01:43:18 [INFO] exp_shallowmodel: ******************** ghome - Round 46 
01/23/2018 01:43:18 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:43:18 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:43:18 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:43:18 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:43:18 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:43:18 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:43:18 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:44:16 [INFO] exp_shallowmodel: train time: 58.626s
01/23/2018 01:44:16 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:44:16 [INFO] exp_shallowmodel: accuracy:   0.762
01/23/2018 01:44:16 [INFO] exp_shallowmodel: f1_score:   0.355
01/23/2018 01:44:16 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:44:16 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.40      0.17      0.24        59
          C       0.50      0.08      0.14        12
          F       0.80      0.96      0.87       396
          R       0.38      0.11      0.17        55

avg / total       0.70      0.76      0.71       522

01/23/2018 01:44:16 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:44:16 [INFO] exp_shallowmodel: 
[[ 10   0  45   4]
 [  2   1   9   0]
 [  8   1 381   6]
 [  5   0  44   6]]
01/23/2018 01:44:23 [INFO] exp_shallowmodel: ******************** ghome - Round 47 
01/23/2018 01:44:23 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:44:23 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:44:23 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:44:23 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:44:23 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:44:23 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:44:23 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:45:22 [INFO] exp_shallowmodel: train time: 58.921s
01/23/2018 01:45:22 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:45:22 [INFO] exp_shallowmodel: accuracy:   0.768
01/23/2018 01:45:22 [INFO] exp_shallowmodel: f1_score:   0.342
01/23/2018 01:45:22 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:45:22 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.41      0.19      0.26        59
          C       0.00      0.00      0.00        12
          F       0.81      0.96      0.88       396
          R       0.41      0.16      0.23        55

avg / total       0.70      0.77      0.72       522

01/23/2018 01:45:22 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:45:22 [INFO] exp_shallowmodel: 
[[ 11   0  42   6]
 [  1   0  10   1]
 [  9   0 381   6]
 [  6   0  40   9]]
01/23/2018 01:45:28 [INFO] exp_shallowmodel: ******************** ghome - Round 48 
01/23/2018 01:45:28 [INFO] exp_shallowmodel: #(data) = 4197
01/23/2018 01:45:28 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:45:28 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:45:28 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:45:28 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:45:28 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:45:28 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:46:26 [INFO] exp_shallowmodel: train time: 58.121s
01/23/2018 01:46:26 [INFO] exp_shallowmodel: test time:  0.024s
01/23/2018 01:46:26 [INFO] exp_shallowmodel: accuracy:   0.759
01/23/2018 01:46:26 [INFO] exp_shallowmodel: f1_score:   0.336
01/23/2018 01:46:26 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:46:26 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.53      0.15      0.24        59
          C       0.00      0.00      0.00        12
          F       0.80      0.95      0.87       396
          R       0.36      0.18      0.24        55

avg / total       0.70      0.76      0.71       522

01/23/2018 01:46:26 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:46:26 [INFO] exp_shallowmodel: 
[[  9   0  44   6]
 [  2   0  10   0]
 [  4   3 377  12]
 [  2   1  42  10]]
01/23/2018 01:46:32 [INFO] exp_shallowmodel: ******************** ghome - Round 49 
01/23/2018 01:46:32 [INFO] exp_shallowmodel: #(data) = 4176
01/23/2018 01:46:32 [INFO] exp_shallowmodel: #(feature) = 37842
01/23/2018 01:46:32 [INFO] exp_shallowmodel: ================================================================================
01/23/2018 01:46:32 [INFO] exp_shallowmodel: LR.pen=l1.C=1.000000
01/23/2018 01:46:32 [INFO] exp_shallowmodel: ________________________________________________________________________________
01/23/2018 01:46:32 [INFO] exp_shallowmodel: Training: 
01/23/2018 01:46:32 [INFO] exp_shallowmodel: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
01/23/2018 01:47:31 [INFO] exp_shallowmodel: train time: 59.084s
01/23/2018 01:47:31 [INFO] exp_shallowmodel: test time:  0.025s
01/23/2018 01:47:31 [INFO] exp_shallowmodel: accuracy:   0.738
01/23/2018 01:47:31 [INFO] exp_shallowmodel: f1_score:   0.329
01/23/2018 01:47:31 [INFO] exp_shallowmodel: classification report:
01/23/2018 01:47:31 [INFO] exp_shallowmodel:              precision    recall  f1-score   support

          A       0.39      0.17      0.24        64
          C       0.00      0.00      0.00        14
          F       0.78      0.95      0.86       402
          R       0.36      0.16      0.22        63

avg / total       0.67      0.74      0.69       543

01/23/2018 01:47:31 [INFO] exp_shallowmodel: confusion matrix:
01/23/2018 01:47:31 [INFO] exp_shallowmodel: 
[[ 11   0  51   2]
 [  4   0   8   2]
 [  8   0 380  14]
 [  5   1  47  10]]
Done: 20180123-014741
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:169: RuntimeWarning: invalid value encountered in subtract
  Xr -= mean_1
/ihome/pbrusilosky/rum20/packages/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
